---
title: 第11回 2025 年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div align="center">
<font size="+2" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 04/Jul./2025<br/>
Appache 2.0 license<br/>
</div>

# 第 11 回

* [課題提出フォルダ <img src="/2025assets/Google_Drive_icon_2020.svg" style="width:02%">](https://drive.google.com/drive/u/3/folders/1HwzDH2M1tM6Dg9m-QXXm7NFt0xiOcTOw){:target="_blank"}


# キーワード

* 言語モデル, 再帰型ニューラルネットワーク, Transformer, エンコーダ・デコーダモデル

### デモンストレーション

- [リカレントニューラルネットワークによる文字ベース言語モデルデモ](https://komazawa-deep-learning.github.io/character_demo.html){:target="_blank"}
- [リカレントニューラルネットワークによる文字ベース言語モデルデモ みんなの日本語](https://komazawa-deep-learning.github.io/minnichi_char_rnn.html)

### 実習ファイル

* [word2vec 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb){:target="_blank"}
* [tSNE を用いた TLPA 200語の word2vec 視覚化](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2020cnps_tSNE_for_word2vec.ipynb){:target="_blank"}
<!-- * [2020年2月24日資料1 tlpa 画像](https://ShinAsakawa.github.io/2020making_tlpa.html){:target="_blank"} -->

* [百人一首の上の句とエンコーダによって符号化し，下の句をデコーダで生成する自作 Transformer モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}

* [三夕の歌 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0925RNN_3twilight_poetries.ipynb)
* [足し算モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb)
- [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}
- [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb){:target="_blank"}

- [Bahdanau and Loung attentions <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb)
* [Attention is all you need <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022The_Annotated_%22Attention_is_All_You_Need%22.ipynb)

<!--
* [注意つき翻訳モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)
* [バニラ風味 注意なし翻訳モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1003vanilla_seq2seq2.ipynb) -->

<!-- * [GPT-3 を使って，自発話のシミュレーション <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0719japanese_gpt_1b.ipynb)
* [T5 による，文章穴埋め問題  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0918T5_demo_filling_blank_question.ipynb)
* [Seq2seq モデル による翻訳デモ 注意付きリカレントニューラルネットワーク <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)
* [BERT の微調整実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0623BERT_SNOW_training.ipynb) -->

<!-- - [足し算のデモ keras 版 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb)-->



<!-- * [GPT-3 を使って，自発話のシミュレーション <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0719japanese_gpt_1b.ipynb)
* [T5 による，文章穴埋め問題  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0918T5_demo_filling_blank_question.ipynb)
* [Seq2seq モデル による翻訳デモ 注意付きリカレントニューラルネットワーク <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)
* [BERT の微調整実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0623BERT_SNOW_training.ipynb) -->

<!-- - [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}
* [CNN 畳み込み層の可視化 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1024CNN_layer_visualization.ipynb){:target="_blank"}
* [各画像の画面表示時に日本語キャプションを付与する準備 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/ccap/blob/master/notebooks/2020importing_ccap_from_GitHub.ipynb){:target="_blank"}
* [EfficientNet のパラメータ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/drive/1QpKBHsBR5yvEOz2M-pKCUpliDh1XXplS){:target="_blank"}
* [データ拡張 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617plot_transforms_demo.ipynb) -->

<!-- - [各画像の画面表示時に日本語キャプションを付与する準備 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/ccap/blob/master/notebooks/2020importing_ccap_from_GitHub.ipynb){:target="_blank"}-->
<!-- - [EfficientNet のパラメータ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/drive/1QpKBHsBR5yvEOz2M-pKCUpliDh1XXplS) -->


<!-- - [BERT の超簡単な使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.c
om/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0903BERT_demo.ipynb){:target="_blank"} -->
<!-- - [最小限の MeCab](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2022_0916mecab_test.ipynb) -->

<!-- - [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb)
- [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb) - [足し算のデモ keras 版 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb)-->


# 再帰型ニューラルネットワーク

<!-- ## 2.1. NETtalk
系列情報処理を扱った初期のニューラルネットワーク例として NETTalk が挙げられます。
NETTalk[^NETTalk] は文字を音読するネットワークです。下図のような構成になっています。
下図のようにアルファベット 7 文字を入力して，空白はアンダーラインで表現されています，中央の文字の発音を学習する 3 層のニューラルネットワークです。NETTalk は 7 文字幅の窓を移動させながら
逐次中央の文字の発音を学習しました。たとえば /I ate the apple/ という文章では
"the" を "ザ" ではなく "ジ" と発音することになります。

印刷単語の読字過程のニューラルネットワークモデルである SM89[^SM89], PMSP96[^PMSP96] で用いられた発音表現は <a target="_blank" href="https://en.wikipedia.org/wiki/ARPABET">ARPABET</a> の亜種です。Python では `nltk` ライブラリを使うと ARPABET の発音を得ることができます(<a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_arpabet_test.ipynb">ARPABET のデモ<img src="/assets/colab_icon.svg"></a>)。

[^NETTalk]: Sejnowski, T.J. and Rosenberg, C. R. (1987) Parallel Networks that Learn to Pronounce English Text, Complex Systems 1, 145-168.
[^SM89]: Seidenberg, M. S. & McClelland, J. L. (1989). A distributed, developmetal model of word recognition and naming. Psychological Review, 96(4), 523–568.
[^PMSP96]: Plaut, D. C., McClelland, J. L., Seidenberg, M. S. & Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103, 56–115.

<center>
<img src="/assets/1986Sejnowski_NETtalkFig2.svg" style="width:47%"><br/>
Sejnowski (1986) Fig. 2
</center> -->

## 単純再帰型ニューラルネットワーク

<!-- NETTalk を先がけとして **単純再帰型ニューラルネットワーク** Simple Recurrent Neural networks (SRN) が提案された。 -->
発案者の名前で **Jordan ネット**，**Elman ネット** と呼ばれる。
<!-- [JordanNet]: Joradn, M.I. (1986) Serial Order: A Parallel Distributed Processing Approach, UCSD tech report.
[ElmanNet]: Elman, J. L. (1990)Finding structure in time, Cognitive Science, 14, 179-211. -->
Jordan ネットも Elman ネットも上位層からの **帰還信号** を持つ。
これを **フィードバック結合** と呼び，位置時刻前の状態が次の時刻に使われる。
Jordan ネットでは一時刻前の出力層の情報が用いられる (下図)。
一方，Elman ネットでは一時刻前の中間層の状態がフィードバック信号として用いられる。

<center>
<img src="/assets/SRN_J.svg" style="width:47%"><br/>
<div style="width:74%" align="center">
図：マイケル・ジョーダン発案ジョーダンネット [@1986Jordan]
</div>
</center>

<center>
<img src="/assets/SRN_E.svg" style="width:47%"><br/>
<div style="align:center; width:47%">
図：ジェフ・エルマン発案のエルマンネット[@lman1990],[@Elman1993]
</div>
</center>

どちらも一時刻前の状態を短期記憶として保持して利用するのですが，実際の学習では一時刻前の状態をコピーして保存しておくだけで，実際の学習では通常の **誤差逆伝播法** すなわちバックプロパゲーション法が用いられる。
上 2 つの図に示したとおり U と W とは共に中間層への結合係数であり，V は中間層から出力層への結合係数である。
Z=I と書き点線で描かれている矢印はコピーするだけですので学習は起こりません。このように考えれば SRN は 3 層のニューラルネットワークであることが分かる。

SRN はこのような単純な構造にも関わらず **チューリング完全** であろうと言われてきた。
すなわちコンピュータで計算可能な問題はすべて計算できるくらい強力な計算機だという意味である。

- Jordan ネットは出力層の情報を用いるため **運動制御** に
- Elan ネットは内部状態を利用するため **言語処理** に

それぞれ用いられる。
従って **失行** aparxia (no matter what kind of apraxia such as 'ideomotor' or 'conceptual')，**行為障害** のモデルを考える場合 Jordan ネットは考慮すべき選択肢の候補の一つとなるだろう。

## リカレントニューラルネットワークの時間展開

一時刻前の状態を保持して利用する SRN は下図左のように描くことができる。
同時に時間発展を考慮すれば下図右のように描くことも可能である。

<center>
<img src="/assets/RNN_fold.svg" style="width:49%"><br/>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてみよ。
あるいは同義だ上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してほしい。
このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かる。

### エルマンネットによる言語モデル

下図に <a target="_blank" href="/assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示した。
図中の数字はニューロンの数を表す。
入力層と出力層のニューロン数 26 とは，用いた語彙数が 26 であったことを表す。

<center>
<img src="/assets/1991Elman_starting_small_Fig1.svg" style="width:47%"><br/>

from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示した。
Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示された (Elman, 1990, 1991, 1993)。

- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | lives

これらの規則にはさらに 2 つの制約がある。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもとらなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というものである。

下図に訓練後の中間層の状態を主成分分析にかけた結果を示した。
"boy chases boy", "boy sees boy", および "boy walks" という文を逐次入力した場合の遷移を示している。
同じ文型の文章は同じような状態遷移を辿ることが分かる。

<center>
<img src="/assets/1991Elman_Fig3.jpg" style="width:49%"><br/>
<!-- <p align="left" style="width:47%">
Trajectories through state space for sentences boy chases boy, boy sees boy, boy walks.
Principal component 1 is plotted along the abscissa; principal component 3 is plotted along the ordinate.
These two PC’s together encode differences in verb-argument expectations.
</p>-->
</center>

<center>
<img src="/assets/1991Elman_Fig4a.jpg" style="width:48%"><br/>
</center>

下図は文 "boy chases boy who chases boy" を入力した場合の遷移図である。
この文章には単語 "boy" が 3 度出てくる。
それぞれが異なるけれど，他の単語とは異なる位置に附置されていることがわかる。
同様に "chases" が 2 度出てくるが，やはり同じような位置で，かつ，別の単語とは異なる位置に附置されている。<br/>

<center>
<img src="/assets/1991Elman_Fig4b.jpg" style="width:49%"><br/>
</center>

同様にして "boy who chases boy chases boy" (男の子を追いかける男の子が男の子を追いかける) の状態遷移図を下図に示した。<br/>
<center>

<img src="/assets/1991Elman_Fig4c.jpg" style="width:48%"><br/>
</center>

さらに複雑な文章例 "boy chases boy who chases boy who chases boy" の状態遷移図を下図に示す。</br>
<center>

<img src="/assets/1991Elman_Fig4d.jpg" style="width:48%"><br/>
</center>

Elman ネットが構文，文法処理ができるということは上図のような中間層での状態遷移で同じ単語が異なる文位置で異なる文法的役割を担っている場合に，微妙に異なる表象を，図に即してで言えば， 同じ単語では，同じような場所を占めるが，その文法的役割によって異なる位置を占めることが示唆される。
このことから中間層の状態は異なる文章の表現を異なる位置として表現していることが考えられ，後述する **単語の意味** や **自動翻訳** などに使われることに繋がります(浅川の主観半分以上)

<p align="left" style="width:74%">

関係節を含む文の状態で空間内の動き。
主成分 I は横軸に沿って表示され、主成分 II は縦軸に沿って表示される。
これら 2 つの主成分は、関係節の埋め込みの深さをエンコードしている。
<!-- Movement through state space for sentences with relative clauses. 
Principal component 1 is displayed along the abscissa; principal component 11 is displayed along the ordinate. 
These two PC’s encode depth of embedding in relative clauses. -->
</p>

<!-- ## 2.5. Seq2sep 翻訳モデル

上記の中間層の状態を素直に応用すると **機械翻訳** や **対話** のモデルになります。
下図は初期の翻訳モデルである "seq2seq" の概念図を示しました。
"`<eos>`" は文末 end of sentence を表します。中央の "`<eos>`" の前がソース言語であり，中央の "`<eos>`" の後はターゲット言語の言語モデルである SRN の中間層への入力として用います。

注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がないことです。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことです。
この点を改善することを目指すことが 2014 年以降盛んに行われてきました。
顕著な例が後述する **双方向 RNN**， **LSTM** を採用したり，**注意** 機構を導入することでした。 -->

<!--
<center>
<img src="/assets/RNN_fold.svg" style="width:94%"></br>
Time unfoldings of recurrent neural networks
</center>
-->

<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" style="width:88%"><br/>
From [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$
\mbox{argmax}_{\theta}
\left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)
$$ -->

## 多様な RNN とその万能性
双方向 RNN や LSTM を紹介する前に，カルパシーのブログから下図に引用する。
下の 2 つ図ではピンク色が入力層，緑が中間層，青が出力層を示している。

<!-- [^karpathy]: 去年までスタンフォード大学の大学院生。
現在はステラ自動車，イーロン・マスクが社長，の AI 部長さんです。図は彼のブログから引用です。蛇足ですがブログのタイトルが unreasonable effectiveness of RNN です。
過去の偉大な論文 Wiegner (1960), Hamming (1967), Halevy (2009) からの <del>パクリ</del> **敬意を表したオマージュ**です。
"unreasonable effectiveness of [science|mathematics|data]" $\ldots$ www -->

<center>
<img src="/assets/diags.jpeg" style="width:77%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

- 上図最左は通常の多層ニューラルネットワークで画像認識，分類，識別問題に用いられます。
- 上図左から 2 つ目は，画像からの文章生成
- 上図中央，左から 3 つ目は，極性分析，文章のレビュー，星の数推定
- 上図右から 2 つ目は翻訳や文章生成
- 上図最右はビデオ分析，ビデオ脚注付け

などに用いられます。これまで理解を促進する目的で中間層をただ一層として描いてきた。
だが中間層は多層化されていることの方が多いこと，中間層各層のニューロン数は 1024 程度まで用いられていることには注意。

数は各層のニューロン数が 4 つである場合の数値例を示しています。入力層では **ワンホット** 表現が用いられている。

ワンホット表現: ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。
一つだけが "熱い" あるいは "辛い" ベクトルと呼ぶ。
以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていたが ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派)と呼ばれることが多い。
ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じる。
典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新する。
従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになる。
そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行った。
上のエルマンネットによる文法学習において,ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当する。
単語埋め込み層を用いることで学習効率が改善し，後に示す word2vec などの **分散ベクトルモデル** へと発展する。

<center>
<img src="/assets/charseq.jpeg" style="width:66%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

<!-- 
- [@1991Siegelmann_RNN_universal] said Turing completeness of RNN.
- [用語集](/2023/2023lect03_glossary){:target="_blank"}
-->

<!--## 実習-->

<!--- [実習 画像認識 Keras 版<img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kera_CNN_demo_with_wordnet_ja.ipynb) -->
<!-- - [実習 Keras CNN](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb){:target="_blank"}
- [実習 Kermack McKendric model<img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"} -->
<!-- - [実習 PyTorch CNN <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0416PyTorch_CNN_demo.ipynb){:target="_blank"}
<!--- [実習](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb#scrollTo=oD497lby40Fp)-->

<!-- - [実習 画像認識 Keras 版](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kera_CNN_demo_with_wordnet_ja.ipynb) -->
<!-- - [実習 Keras CNN](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb){:target="_blank"} -->
<!-- - [実習 PyTorch CNN](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0416PyTorch_CNN_demo.ipynb){:target="_blank"} -->
<!-- - [実習 Kermack McKendric model](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}-->
<!--- [実習](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb#scrollTo=oD497lby40Fp)-->


<!-- - CNN: 畳み込みニューラルネットワーク
- RNN: リカレントニューラルネットワーク
- RL: 強化学習-->

<!-- <center> -->
<!--  <img src="https://komazawa-deep-learning.github.io/assets/2008Fuster_Prefrontal_Cortex_fig8_4.svg" width="39%"> -->
<!--  <img src="https://komazawa-deep-learning.github.io/assets/2015Ronneberger_U-Net_Fig1_ja.svg" width="48%"> -->
<!-- </center> -->

<!-- - [2018Kriegeskorte](2018Kriegeskorte){:target="_blank"}
- [1970Newell](1970Newell){:target="_blank"}
- [2019Glaser](2019Glaser){:target="_blank"}
- [2020Lindsay](2020Lindsay){:target="_blank"}
- [G 検定](https://www.seshop.com/product/detail/23864?utm_source=seid_it_spot_20210412&utm_medium=email&utm_campaign=coupon){:target="_blank"}

### 2021年02月23日分
- [2020-0215](2020-0215abstract){:target="_blank"}
- [どうぶつの森モデル，動物の名前連想モデル](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021_0223word_associtaion.ipynb){:target="_blank"}
- [導入講義用 CCP ウィルス感染者予測モデルを題材に](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}
- [CNN の簡単なデモ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Keras_CNN_demo_with_wordnet_ja.ipynb){:target="_blank"}

# 統計学と機械学習の関係

母集団における差異の有無を問題にする心理統計学と機械学習との間には，決定的な差があります。

- [1970Newell](1970Newell){:target="_blank"}
- [2019Glaser](2019Glaser){:target="_blank"}
- [2020Lindsay](2020Lindsay){:target="_blank"}


<a href="https://guides.github.com/features/pages/">Read this page to write this page.</a>-->



<!-- ## 注意

* 認知心理学，生理学，などではトップダウンとボトムアップの 2 種類の注意が区別されてきた。計算論的には Crick (1984) のサーチライト仮説などにより提唱された **勝者占有回路 winner-take-all 回路** である。
* 自然言語処理 (Vaswani+2017)，画像処理 (Ramachandran2019) や眼球運動の DeepGaze (Kummerer2019) などがある。

### 文献

- [⼼理学，神経科学，機械学習における注意, Lindsay, 2019](https://project-ccap.github.io/2020Lindsay_Attention_inPsychology_Neuroscience_and_Machine_Learning.pdf){:target="_blank"}


### 心理学的注意

<img src="/assets/1988Treisman_Fig1.svg" width="24%">
<img src="/assets/1994Wolfe_GS2Fig2.jpg" width="39%">
<img src="/assets/1998IttiKoch_fig1.jpg" width="29%"><br/>

<p style="text-align: left;width:88%;background-color: cornsilk;">
左: Treisman1988 Fig.1，特徴統合理論の概念図。ボトムアップ注意
中: Wolfe1994 Fig.2 ガイド付き探索 バージョン 2 モデル。トップダウン注意
右: Itti and Koch (1998) Fig. 1 計算モデルの実装例
</p>

### ニューラルネットワーク的注意

#### CAM

<center>
<img src="/assets/2016Zhou_CAM_fig2ja.svg" width="66%"><br/>
CAM の概念図 Zhou (2016) Fig. 2 を改変
</center>

<br/><br/><br/>
<center>
<img src="/assets/2016Grad-CAM_boxer_tigercat.png" style="width:20%">
<img src="/assets/2016Grad-CAM_boxer.png" style="width:20%">
<img src="/assets/2016Grad-CAM_tigercat.png" style="width:20%"><br/>
Grad-CAM の結果。Selvaraj (2016) Fig. 5 より。左: 元画像。央: ボクサー犬と判断した場合のヒートマップ。右:トラ猫と判断した場合のヒートマップ
</center>



1. 交差エントロピー: $t\log(p) + (1-t)\log(1-p)$
損失関数として，最小自乗誤差を用いるよりも，分類課題での離散変数については，交差エントロピーを用いる方が収束が早く便利である。

# 注意とソフトマックス関数，CAM

前回は，自然画像分類のために訓練した畳み込みニューラルネットワークの結合係数を視覚化すると，特徴地図が現れることを示した。

今回は，この特徴地図を顕著性地図と考えて，ボトムアップによる注意を考える。
 -->

# 言語モデル

$$
P(x_{t+1}) = P(x|x_{i<t})
$$

1. RNN
2. LSTM, GRU
3. Transformer

<center>

<img src="/assets/SRN_J.svg" style="width:23%">
<img src="/assets/SRN_E.svg" style="width:23%">
<img src="/assets/2015Greff_LSTM_ja.svg" width="29%"><br/>
<img src="/assets/RNN_fold.svg" style="width:49%"><br>
Time unfoldings of recurrent neural networks

左：マイケル・ジョーダン発案ジョーダンネット [@1986Jordan]
中：ジェフ・エルマン発案エルマンネット
右: LSTM
</center>

の LSTM は一つのニューロンに該当します。このニューロンには 3 つのゲート(gate, 門) が付いています。
3 つのゲートは以下の名前で呼ばれます。

1. 入力ゲート input gate
2. 出力ゲート output gate
3. 忘却ゲート forget gate

各ゲートの位置を上図で確認してください。入力ゲートと出力ゲートが閉じていれば，セルの内容(これまでは中間層の状
態と呼んできました)が保持されることになります。
出力ゲートが開いている場合には，セル内容が出力されます。一方出力ゲートが閉じていればそのセル内容は出力されませ
ん。このように入力ゲートと出力ゲートはセル内容の入出力に関与します。
忘却ゲートはセル内容の保持に関与します。忘却ゲートが開いていれば一時刻前のセル内容が保持されることを意味します
。反対に忘却ゲートが閉じていれば一時刻前のセル内容は破棄されます。全セルの忘却ゲートが全閉ならば通常の多層ニュ
ーラルネットワークであることと同義です。すなわち記憶内容を保持しないことを意味します。SRN でフィードバック信号
が存在しない場合に相当します。セルへの入力は，

1. 下層からの信号，
2. 上層からの信号, すなわち Jordan ネットの帰還信号
3. 自分自身の内容，すなわち Elman ネットの帰還信号

が用いられます。これら入力信号が

1. 入力信号そのもの
2. 入力ゲートの開閉制御用信号
3. 出力ゲートの開閉制御用信号
4. 忘却ゲートの開閉制御用信号

という 4 種類に用いられます。従って LSTM のパラメータ数は SRN に比べて 4 倍になります。

LSTM に限らず一般のニューラルネットワークの出力には非線形関数が用いられます。代表的な非線形出力関数としては，
以下のような関数が挙げられます。

1. シグモイド関数: $f(x)=\left[1+e^{-x}\right]^{-1}$
2. ハイパーボリックタンジェント関数:  $f(x)=\left(e^{x}-e^{-x}\right)/\left(e^{x}+e^{-x}\right)$
3. 整流線形ユニット関数: $f(x)=\max\left(0,x\right)$

この中で，セルの出力関数として 2. のハイパーボリックタンジェント関数が，ゲートの出力関数にはシグモイド関数が使われます。
その理由はハイパーボリックタンジェント関数の方が収束が早いこと，シグモイド関数は値域が $[0,1]$ であるためゲートの開閉に直接対応しているからです。

- Le Cun, Y. Bottou, L., Orr, G. B, Muller K-R. (1988) Efficient BackProp, in Orr, G. and Muller, K. (Eds.) Neural Networks: tricks and trade, Springer.

<!--
The LSTM (left figure) can be described as the input signals $\mathbf{x}_t$ at
time $t$, the output signals $\mathbf{o}_t$, the forget gate $\mathbf{f}_t$, and
the output signal $\mathbf{y}_t$, the memory cell $\mathbf{c}_t$, then we can get
the following:
$i_{t}=\sigma\left(W_{xi}x_{t}+W_{hi}y_{t-1}+b_{i}\right)$, <br>
$f_{t}=\sigma\left(W_{xf}x_{t}+W_{hf}y_{t-1}+b_{f}\right)$, <br>
$o_{t}=\sigma\left(W_{xo}x_{t}+W_{ho}y_{t-1}+b_{o}\right)$, <br>
$g_{t}=\phi\left(W_{xc}x_{t}+W_{hc}y_{t-1}+b_{c}\right)$,<br>
$c_{t}=f_{t}\odot c_{t-1} + i_{t}\odot g_{t}$,<br>
$h_{t}=o_{t}\odot\phi\left(c_{t}\right)$<br>\label{eq:LSTM}
where
$\sigma\left(x\right)=\displaystyle\frac{1}{1+\mbox{exp}\left(-x\right)}$ (logistic function)
%% =1/2\left(\phi\Brc{x}+1\right)$,
$\phi\left(x\right)=\displaystyle\frac{\mbox{exp}\left(x\right)-\mbox{exp}\left(-x\right)}{\mbox{exp}\left(x\r
ight)+\mbox{exp}\left(-x\right)}$ (hyper tangent)
%% $=2\sigma\left(x\right)-1$
and $\odot$ menas Hadamard (element--wise) product.
-->

## LSTM におけるゲートの生理学的対応物 <!--Physiological correlates of gates in LSTM-->

以下の画像は <http://kybele.psych.cornell.edu/~edelman/Psych-2140/week-2-2.html> よりの引用。
ウミウシのエラ引っ込め反応時に，ニューロンへの入力信号ではなく，入力信号を修飾する結合が存在する。下図参照。

<center>
<img src="/assets/2016McComas_presynaptic_inhibition.jpg" style="width:24%">
<img src="/assets/C87-fig2_24.jpg" width="17%">
<img src="/assets/C87-fig2_25.jpg" width="33%"><br>
アメフラシ (Aplysia) のエラ引っ込め反応(a.k.a. 防御反応)の模式図[^seaslang]
<!-- <img src="/assets/shunting-inhibition.jpg" width="29%"> -->
</center>

* [注意機構の補足説明 大門他 (2023) <img src="https://www.adobe.com/content/dam/cc/en/legal/images/badges/PDF_32.png">](/2023/2023cnps注意機構の補足説明.pdf){:target="_blank"}



# ソフトマックス関数

## ゲートとして

<div class="figure figcenter">
<img src="/2023assets/2006Oreilly_wm_fig2.svg" width="49%">
<div class="figcaption">

#### 図 ゲートの概念図
ゲートが開いているときは，感覚入力は作業記憶を更新できるが，閉じているときはそれができない。
このため，妨害刺激など無関連情報が以前に記憶した情報の維持を妨げるのを防ぐことができる。
O'Reilly (2006) より
</div></div>


<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" style="width:88%"><br/>
From [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$
\mbox{argmax}_{\theta}
\left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)
$$ -->

# 意味論

ここでは意味論の研究史を心理学関連領域に絞ってまとめることを試みます。
<!-- 神経心理学症状との関連については
付録 <a target="_blank" href="https://github.com/ShinAsakawa/wbai_aphasia/blob/master/2019Primer_AphasiaDyslexia.pdf">失語，失読に関する神経心理学モデルの基礎</a> をご覧ください。
-->

意味についての言及は言語学者 Firth さらに遡れば Witgenstein まで辿ることが可能です。
ですがここでは直接関連する研究として以下をとりげます

- 第 1 世代 意味微分法 Osgood
- 第 2 世代 潜在意味解析 Ladauer
- 第 3 世代 潜在ディレクリ配置，トピックモデル
- 第 4 世代 分散埋め込みモデル word2vec とその後継モデル
- 最近の展開


## 意味微分法 Semantics Differential (SD) 1952 年 
チャールズ・オズグッドによって提案された意味微分法は，被験者に対象を評価させる際に形容詞対を用います。
形容詞対は 5 件法あるいはその他の変種によって評価されます。
得られた結果を 評価対象 X 形容詞対の行列にします。
すなわち評価対象者の平均を求めて得た行列を **固有値分解**，正確には因子分析 FA を行います。
最大固有値から順に満足の行くまで求めます。
固有値行列への射影行列を因子負荷量と呼びます。得られた結果を下図に示しました。

<center>
<img src="/assets/1957Osgood_Tab1.svg" style="width:48%"><br/>
From  Osgood (1952) Tab. 1
</center>
<!-- <a target="_blank" href="/assets/1957Osgood_Tab1.svg">Osgood (1952) Tab. 1</a>-->

上図では，50 対の形容詞対によって対象を評価した値が描かれています。

<a target="_blank" href="https://ja.wikipedia.org/wiki/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90">因子分析(FA)</a> 形容詞対による多段階評定

<center>

<img src="/assets/1957Osgood_Fig2.svg" style="width:46%"><br/>
From  Osgood (1952) Fig.2
<!--- <a target="_blank" href="/assets/1957Osgood_Fig2.svg">Osgood (1952) Fig. 2</a>-->
</center>

意味微分法においては，研究者の用意した形容詞対の関係に依存して対象となる概念やモノ，コトが決まります。
従って研究者の想定していない概念空間については言及できないという点が問題点として指摘できます。

このことは評価対象がよくわかっている問題であれば精度良く測定できるという長所の裏返しです。

一般的な意味，対象者が持っている意味空間全体を考えるためには，50 個の形容詞対では捉えきれないことも意味します。従って以下のような分野に適用する場合には問題が発生する可能性があると言えます

- 神経心理学的な症状である **意味痴呆** semantic dimentia を扱う場合
- 入試問題などの一般知識を評価したい場合
- 一般言語モデルを作成する場合


## 潜在意味分析 Latent Semantic Analysis (LSA, LSI) 1997 年 
- **潜在意味分析**: <a target="_blank" href="https://ja.wikipedia.org/wiki/%E7%89%B9%E7%95%B0%E5%80%A4%E5%88%86%E8%A7%A3">特異値分解(SVD)</a> は，当時増大しつつあったコンピュータ計算資源を背景に一般意味論に踏み込む先鞭をつけたと考えることができます。

すなわち先代の意味微分法が持つ問題点である，評価方法が 50 対の形容詞であること，
50 をいくら増やしても，結局は研究者の恣意性が排除できないこと，評価者が人間であるため大量の評価対象を評価させることは，
心理実験参加者の拘束時間を長くするため現実的には不可能であることを解消するために，辞書そのものをコンピュータで解析するという手法を採用しました。

1. 辞書の項目とその項目の記述内容とを考えます
2. 特定の辞書項項目にはどの単語が使われているいるのかという共起行列 内容 $\times$ 単語 を
考え，この行列について **特異値分解** を行います。

Osgood の意味微分法で用いられた行列のサイズと比較すると，単語数が数万，項目数は数万から数十万に増加しています。
数の増加は網羅する範囲の拡大を意味します。
下図は持ちられたデータセット例を示したものです。

<center>

<img src="/assets/1997Landauer_Dumais_FigA2.svg" style="width:48%"><br/>
From Landauer and Duman (1997) Fig. A2
</center>

LSA (LSI) の問題点としては以下図を見てください

<center>

<img src="/assets/1997Landauer_Fig3.svg" style="width:46%"><br/>
From Landauer and Dumas (1997) Fig.3
</center>

上図は，得られた結果を元に類義語テストを問いた場合に特異値分解で得られる次元数を横軸に，正解率を縦軸にプロットした図です。
次元を上げると成績の向上が認められます。
ですが，ある程度 300 以上の次元を抽出しても返って成績が低下することが示されています。

次元数を増やすことで本来の類義語検査に必要な知識以外の情報が含まれてしまうため推察されます。


<!--- <a target="_blank" href="/assets/1997Landauer_Fig3.svg">Landauer (1997) Fig. 3</a>-->

## 潜在ディレクリ配置 Latent Direchlet Allocation (LDA) 2003 年 

潜在ディレクリ配置 Latent Direchlet Allocation: LDA[^LDA] は LSA (LSI) を確率的に拡張したモデルであると考えることができます。すなわち LDA では単語と項目との関係に確率的な生成モデルを仮定します。

[^LDA]: 伝統的な統計学においては Fischer の線形判別分析を LDA と表記します。ですがデータサイエンス，すなわち統計学の一分野では近年の潜在ディレクリ配置の成功により LDA と未定義で表記された場合には潜在ディクレクリ配置を指すことが多くなっています。

その理由としては，対象となる項目，しばしば **トピック** と言い表すと，項目の説明に用いられる単語との間に，決定論的な関係を仮定しないと考えることによります。確率的な関係を仮定することにより柔軟な関係をモデル化が可能であるからです。

例えば，ある概念，話題(トピック) "神経" を説明する場合を考えます。
"神経" を説明するには多様な表現や説明が可能です。
"神経" を説明する文章を数多く集めてると，単語 "脳" は高頻度で出現すると予想できます。
同様にして "細胞" や "脳" も高頻度で観察できるでしょう。ところが単語 "犬" は低頻度でしょう。
単語 "アメフラシ" や "イカ" は場合によりけりでしょう。どちらも神経生理学の発展に貢献した実験動物ですから単語 "アメフラシ" や "イカ" が出現する文章もあれば，単語 "脳梗塞" や単語 "失語" と同時に出現する確率もありえます。
このように考えると確率的に考えた方が良い場合があることが分かります。

### ディレクリ分布

もう一点，ノンパラメトリックモデルについて説明します。
parametric model はパラーメータを用いたモデルほどの意味です。
心理統計学の古典的な教科書では，ノンパラメトリック検定とは母集団分布のパラメータに依存 **しない** 統計的検定という意味で用いられます。
一方 LDA の場合には推定すべき分布のパラメータ(の数)を **事前に定めない** という意味で **ノンパラメトリック** なモデルであると言います。
すなわちある話題 (トピック) とそれを説明する単語の出現確率について，取り扱う現象の複雑さに応じてモデルを記述するパラメータ数を適応的に増やして行くことを考えます。

数学的既述は省略しますが，[ベータ分布]("https://en.wikipedia.org/wiki/Beta_distribution"){:target="_blank"} を用いると区間 $[a,b]$ の間をとる分布でパラメータにより分布が柔軟に記述できます。
ベータ分布の多次元拡張を [ディククリ分布](https://en.wikipedia.org/wiki/Dirichlet_distribution){:target="_blank"} と言います。

確率空間に対して一定の成約を付した表現をシンプレックスと言ったりします。<!--例えばコインの裏表は
2 値ですからベータ分布を用いても表す事ができます。
たとえばじゃんけんで対戦相手が，グー，チョキ，パー のいずれかを出す確率は，2 つが分かれば 3 つ目の手は自ずと分かってきます。
このような関係は 3 つの手の確率分布でディククリ分布として扱うことが可能です。
下図はウィキペディアから持ってきました。この図はそのようなじゃんけんの手の出現確率をディレクリ分布として表現した例だと思ってください。

<!--## ディククリ分布 (多次元ベルヌーイ分布)-->
<!-- 多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定-->

<div class="figcenter">

<img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Dirichlet-3d-panel.png" style="width:46%"><br/>
<!--<img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Dirichlet_distributions.png" style="width:49%"></br>-->
<div class="figcaption">
多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定<br/>
図は <https://en.wikipedia.org/wiki/Dirichlet_distribution> より
</div></div>

トピック毎の単語の出現確率も上図と同じ枠組みで記述することが可能です。
かつ，上図ではとりうる値が 3 つの場合ですが，話題が複雑になれば適応的に選択肢の数，すなわちディレクリ分布の次元数が増加することになります。

### プレート表記

あらかじめ定められた数のパラメータを用いて分布を記述するのではなく，
解くべき問題の複雑さに応じて適応的にパラメータ数を定めることに対応して，
LDA あるいはトピックモデルの図示方法として **プレート表記** plate notation があります。
下図にプレート表記の例を示しました。

<div class="figcenter">
<img src="/assets/2009Blei_Topic_Models_02.svg" style="width:47%"><br/>
<div class="figcaption">
プレート表記: ノンパラメトリックモデルの表現に用いられる
</div></div>

- 丸は確率変数
- 矢印は確率的依存関係を表現
- 観測変数は影付き(文献によっては二重丸)
- プレートは繰り返しを表す

Y からパラメータ X が生成される場合，矢印を使ってその依存関係を表現します。
ノンパラメトリックモデルの場合，矢印の数を予め定めません。そのため矢印を多数描くのが煩雑なので，一つの箱代用して表現します。
これがプレート表記になります。

観測可能な変数をグレー，または二重丸で表し，観測不能な，類推すべきパラメータを白丸で表記します。
実際には観測不可能な潜在パラメータを観測データから類推することになります。

大まかなルールとして，潜在変数をギリシャアルファベット表記，観測される変数はローマアルファベット表記の場合が多いですが，一般則ですので例外もあります。

下図に潜在ディレクリ配置  LDA のプレート表記を示しました。
<!--### 潜在ディレクリ配置のプレート表記-->

<div class="figcenter">
<img src="/assets/2009Blei_Topic_Models_03.svg" style="width:47%"><br/>
<!--<img src="/assets/2009Blei_Topic_Models_04.svg" style="width:94%"></br>-->
</div>

### トピックと単語の関係

トピックモデルの要点をまとめた下図はこれまでの説明をすべて含んでいます。


<div class="figcenter">
<img src="/assets/2009Blei_Topic_Models_01.svg" style="width:47%"><br/>
<div class="figcaption">
 出典: ブライのスライド(2009)より，文章は話題(トピック)の混合<br/>
 各文章はその話題から文章が生成されたと考える
</div></div>

興味深い応用例として Mochihashi ら(2009) の示した教師なし学習による日本語分かち書き例を示します。
下図は源氏物語をトピックモデルにより分かち書きさせた例です。どこに空白を挿入すると文字間の隣接関係を表現できるかをトピックモデルで解くことを考えた場合，空白の挿入位置が確率的に定まると仮定して居ます。

<div class="figcenter">
<img src="/assets/2009Mochihashi_Fig10.svg" style="width:49%"><br/>
</div>

Mochihashi らは，ルイス・キャロルの小説 "不思議の国のアリス" 原文から空白を取り除き，文字間の隣接関係から文字の区切り，すなわち空白を推定することを試みました。結果を下図に示しました。

<div class="figcenter">
<img src="/assets/2009Mochihashi_Fig12.svg" style="width:49%"><br/>
</div>

### 原著論文

- [Blei&Jordan(2003) Latent Dirichlet Allocation 原著論文](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf){:target="_blank"}
- [中華レストラン過程 CRP 発展モデル:](http://arxiv.org/abs/0710.0845){:target="_blank"}
- [インド食堂過程 IBP](http://www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf){:target="_blank"}
<!-- - パチンコ過程 -->

### R による実装

- https://cran.r-project.org/web/packages/lda/index.html
- https://cran.r-project.org/web/packages/topicmodels/index.html


## word2vec, 単語埋め込み, ベクトル埋め込みモデル 2013 年 

<div class="figcenter">
<img src="/assets/Mikolov_portrait.jpg" style="width:19%">
<img src="/assets/2015Mikolov_NIPSportrait.png" style="width:24%"><br/>
ミコロフ
</div>

- ミコロフは **word2vec** によりニューラルネットワークによる意味実装を示しました。
ワードツーベックと発音します。
Word2vec は実装に 2 種類あリます。それぞれ **CBOW** と **skip-gram** と命名されています。
"シーボウ" または "シーバウ" と日本人は言ったりすることが多いようです。

有名な "king" - "man" + "woman" = "queen" のアナロジーを解くことができると喧伝されました。

下図左は意味的なアナロジーがベクトルの向きとして表現されていることに注目してください。
ベクトルは方向と大きさを持っている矢印で表現されます。矢印の原点を移動することを考えます。
たとえば "MAN" から "WOMAN" へ向かう矢印を平行移動して "KING" まで持ってくると，その矢印は "QUEEN" を重なることが予想できます。
これがアナロジー問題の解放の直感的説明になります。

<center>
<img src="/assets/2013Mikolov_KingQueenFig.svg" style="width:49%">
</center>

上図右は同じ word2vec でできた空間に対して，統語関係 syntax を解かせた場合を示しています。
"KING" から "KINGS" へ向かう矢印を "QUEEN" まで持ってくると "QUEENS" に重なることが見て取れます。

このことから上図右の赤矢印で示されたベクトルは **複数形** への変換という統語情報，文法情報を表現しているとみなすことが可能です。

伝統的な言語学の知識では，統語構造と意味構造は別個に取り組む課題であると考えられてきました。
ところが word2vec が示す意味空間はそのような区別を考える必要があるのか否かについて問題を提起しているように思われます。

逆に一つのモジュールで処理することができるのであれば，分割して扱う意味があるのかどうかを考える切っ掛けになると考えます。

もう一つ面白い結果を下図に示します。下図は word2vec によって世界の国とその首都との関係を主成分分析 PCA で 2 次元に描画した図です。

<center>
<img src="/assets/2013Mikolov_FigCountries.svg" style="width:49%"><br/>
</center>

横軸は国と首都との関係を表現しているとみなすことができます。縦軸は下から上に向かっておおまかにユーラシア大陸を西から東へ横断しているように配置されています。
意味を表現するということは，解釈によって，この場合 PCA によって 2 次元に図示してみると大まかに我々の知識を表現できることを示唆していると考えます。

word2vec の実装には 2 種類あります。
どちらを使っても同じような結果を得ることができます。

- CBOW: Continous Bog of Words 連続単語袋
- skip-gram: スキップグラム

両者は反対の関係になります。下図を参照してください。

<center>
<img src="/assets/2013Mikolov_Fig1.svg" style="width:49%"><br/>
From Mikolov (2013) Fig. 1
</center>

CBOW も skip-gram も 3 層にニューラルネットワークです。その中間層に現れた表現を **ベクトル埋め込みモデル** あるいは **単語埋め込みモデル** と言ったりします。

- CBOW モデルは周辺の単語の単語袋詰め表現から中央の単語を予測するモデルです。
- skip-gram は中心の単語から周辺の単語袋詰表現を予測するモデルです。

たとえば，次の文章を考えます。

```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

表記を簡潔にするため各単語に ID をふることにします。

```python
{"彼":0, "は":1, "意味論":2, "を":3, "論じ":4, "た":5}
```

すると上記例文は

```python
[0, 1, 2, 3, 4, 5]
```

と表現されます。
ウィンドウ幅がプラスマイナス 2 である CBOW モデルでは 3 層の多層パーセプトロン
の入出力関係は，入力が 4 次元ベクトル，出力も 4 次元ベクトルとなります。
文の境界を無視すれば，以下のような入出力関係とみなせます。


```bash
[0,1,1,0,0,0] -> [1,0,0,0,0,0] # In:"は","意味論" Out:"彼"
[1,0,1,1,0,0] -> [0,1,0,0,0,0] # In:"彼","意味論","を" Out:"は"
[1,1,0,1,1,0] -> [0,0,1,0,0,0] # In:"彼","は","を","論じ" Out:"意味論"
[0,1,1,0,1,1] -> [0,0,0,1,0,0] # In:"は","意味論","論じ","た" Out:"を"
[0,0,1,1,0,1] -> [0,0,0,0,1,0] # In:"意味論","を","た" Out:"論じ"
[0,0,0,1,1,0] -> [0,0,0,0,0,1] # In:"を","論じ" 出力:"た"
```

を学習することとなります。

- CBOW にせよ skip-gram にせよ大規模コーパス，例えばウィキペディア全文を用いて訓練を行います。周辺の単語をどの程度取るかは勝手に決めます。
- Mikolov が類推に用いたデータ例を下図に示しました。国名と対応する首都名，国名とその通貨名，などは意味的関係です。一方罫線下方は文法関係です。
形容詞から副詞形を類推したり，反意語を類推したり，比較級，過去分詞，国名と国民，過去形，複数形，動詞の 3 人称単数現在形などです。

<div class="figcenter">
<img src="/assets/2013Mikolov_Tab1.svg" style="width:49%"><br/>
From Milolov (2013) Tab. 1
</div>

- しばしば，神経心理学や認知心理学では，それぞれの品詞別の処理を仮定したり，意味的な脱落を考えたりする場合に，異なるモジュールを想定することが行われます。
- それらの仮定したモジュールが脳内に対応関係が存在するのであれば神経心理学的には説明として十分でしょう。
- ところが word2vec で示した表現では一つの意味と統語との表現を与える中間層に味方を変える (PCA など)で描画してみれば，異なる複数の言語知識を一つの表象で表現できることが示唆されます。
- word2vec による表現が脳内に分散していると考えるとカテゴリー特異性の問題や基本概念優位性の問題をどう捉えれば良いのかについて示唆に富むと考えます。

<!--<img src="/assets/2013Mikolov_skip-gram_cbow.svg" style="width:74%">
<img src="/assets/skip-gram.svg" style="width:74%">
<img src="/assets/skip-gram_cbow.svg" style="width:74%">
</center>-->

日本語のウィキペディアを用いた word2vec と NTT 日本語の語彙特性との関連に関心のある方は
[日本語 Wikipedia の word2vec 表現と語彙特性との関係, 近藤・浅川(2017)](/2017jpa_word2vec_NTTdict.pdf){:target="_blank"} をご覧ください

## さらなる蘊蓄 負例サンプリング

Word2vec を使って大規模コーパスを学習させる際に，学習させるデータ以外に全く関係のない組み合わせをペナルティーとして与えることで精度が向上します。


## 発展 文章埋め込みモデルへ

単語の word2vec による表現は 3 層パーセプトロンの中間層の活性値として表現されます。

単語より大きなまとまりの意味表現，たとえば，文，段落，などの表現をどのように得るのかが問題になります。
ここで詳細には触れませんが，文表現ベクトルは各単語表現の総和であると考えるのがもっとも簡単な表現になります。
すなわち次文:


```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

の文表現を得るためには，各単語の word2vec 表現を足し合わせることが行われます。
ただし，単純に足し合わせたのでは BOW 単語袋表現と同じことですので，単語の順序情報が失われていることになります。
この辺りをどう改善すれば良いのかが議論されてきました。

### 文献

* <a target="_blank" href="https://papers.nips.cc/paper/5021-distributed-representations-of--words-and-phrases-and-their-compositionality.pdf">word2vec オリジナル論文</a> 2013年 Mikolov
* <a target="_blank" href="https://fasttext.cc/">fastText</a> 高速文埋め込みモデル
* その発展 <a target="_blank" href="../2018jsai.pdf">浅川, 岡, 楠見 (2018)</a>
<!-- - <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論概説</a> -->
<!-- [リカレントニューラルネットワーク](./lect08_RNN.pdf)-->
<!--- [word2vec のやや詳しい解説](/2016word_embbed_slides_tmp.pdf)-->

<!--
<center>
<img src="https://www.tensorflow.org/images/linear-relationships.png" style="width:84%"><br>
<p algin="left" style="width:74%">
Source: <https://www.tensorflow.org/tutorials/representation/word2vec>
</p>
</center>

- <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論の蘊蓄</a>
-->

#### 埋め込みベクトルの応用例

<div class="figcenter">
<img src="/assets/2008Mitchell_fig1ja.svg" style="width:49%"><br/>
<div class="figcaption" style="text-align: left;width: 66%; background-color: cornsilk;">
Mitchell2008 図 1. 任意の名詞刺激に対するfMRI活性化を予測するモデルの形式。
fMRI の活性化は、2段階 プロセスで予測される。
第 1 段階では，入力刺激語の意味を，典型的な単語使用を示す大規模なテキストコーパスから値を抽出した中間的な意味的特徴の観点から符号化する。
第 2 段階では，これらの中間的な意味的特徴のそれぞれに関連する fMRIシグネチャ の線形結合として，fMRI 画像を予測する。
<!--Form of the model for predicting fMRI activation for arbitrary noun stimuli.
fMRI activation is predicted in a two-step process.
The first step encodes the meaning of the input stimulus word in terms of intermediate semantic features whose values are extracted from a large corpus of text exhibiting typical word use.
The second step predicts the fMRI image as a linear combination of the fMRI signatures associated with each of these intermediate semantic features. -->
</div></div>

# 1. 単語埋め込みモデル word2vec

* ミコロフは **word2vec** によりニューラルネットワークによる意味実装を示した (Mikolov+2013)。
* Word2vec は実装に 2 種類ある。それぞれ **CBOW** と **skip-gram** と命名されている。
* "king" - "man" + "woman" = "queen" のアナロジーを解くことができると喧伝された。
* 下図左は意味的なアナロジーがベクトルの向きとして表現されていることに注目。
ベクトルは方向と大きさを持っている矢印で表現される。
このとき，矢印の原点を移動することを考える。
たとえば "MAN" から "WOMAN" へ向かう矢印を平行移動して "KING" まで持ってくると，その矢印は "QUEEN" を重なることが予想できる。
これがアナロジー問題の解放の直感的説明である。
* word2vec の実装には 2 種類あります。どちらを使っても同じような結果を得ることができます。
    1. CBOW: Continous Bog of Words 連続単語袋
    2. skip-gram: スキップグラム

両者は反対の関係になrる。下図を参照。

<center>
<img src="/assets/2013Mikolov_Fig1.svg" style="width:49%"><br>
From Mikolov (2013) Fig. 1
</center>

CBOW も skip-gram も 3 層にニューラルネットワークです。その中間層に現れた表現を **ベクトル埋め込みモデル** あ
るいは **単語埋め込みモデル** と言ったりします。

- CBOW モデルは周辺の単語の単語袋詰め表現から中央の単語を予測するモデルです。
- skip-gram は中心の単語から周辺の単語袋詰表現を予測するモデルです。

たとえば，次の文章を考えます。

```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

表記を簡潔にするため各単語に ID をふることにします。

```python
{"彼":0, "は":1, "意味論":2, "を":3, "論じ":4, "た":5}
```

```python
[0, 1, 2, 3, 4, 5]
```

と表現されます。
ウィンドウ幅がプラスマイナス 2 である CBOW モデルでは 3 層の多層パーセプトロン
の入出力関係は，入力が 4 次元ベクトル，出力も 4 次元ベクトルとなります。
文の境界を無視すれば，以下のような入出力関係とみなせます。


```bash
[0,1,1,0,0,0] -> [1,0,0,0,0,0] # In:"は","意味論" Out:"彼"
[1,0,1,1,0,0] -> [0,1,0,0,0,0] # In:"彼","意味論","を" Out:"は"
[1,1,0,1,1,0] -> [0,0,1,0,0,0] # In:"彼","は","を","論じ" Out:"意味論"
[0,1,1,0,1,1] -> [0,0,0,1,0,0] # In:"は","意味論","論じ","た" Out:"を"
[0,0,1,1,0,1] -> [0,0,0,0,1,0] # In:"意味論","を","た" Out:"論じ"
[0,0,0,1,1,0] -> [0,0,0,0,0,1] # In:"を","論じ" 出力:"た"
```

を学習することとなる。

- CBOW にせよ skip-gram にせよ大規模コーパス，例えばウィキペディア全文を用いて訓練を行います。周辺の単語をどの程度取るかは勝手に決める。
- Mikolov が類推に用いたデータ例を下図に示した。
国名と対応する首都名，国名とその通貨名，などは意味的関係である。
一方罫線下方は文法関係である。形容詞から副詞形を類推したり，反意語を類推したり，比較級，過去分詞，国名と国民，過去形，複数形，動詞の 3 人称単数現在形などである。


### 文献

* [word2vec オリジナル論文, Mikolov2013](https://papers.nips.cc/paper/5021-distributed-representations-of--words-and-phrases-and-their-compositionality.pdf)
* [fasttext高速版](https://fasttext.cc/)
* [その発展 浅川, 岡, 楠見 (2018)](../2018jsai.pdf)
* [日本語 Wikipedia の word2vec 表現と語彙特性の関係 近藤，浅川 (2017)](../2017jpa_word2vec_NTTdict.pdf)

<!-- - <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論概説</a> -->
<!-- [リカレントニューラルネットワーク](./lect08_RNN.pdf)-->
<!--- [word2vec のやや詳しい解説](/2016word_embbed_slides_tmp.pdf)-->

## 負例サンプリング

Word2vec を使って大規模コーパスを学習させる際に，学習させるデータ以外に全く関係のない組み合わせをペナルティーとして与えることで精度が向上します。

<center>
<img src="/assets/2013Mikolov_Tab1.svg" style="width:49%"><br>
From Milolov (2013) Tab. 1
</center>

- しばしば，神経心理学や認知心理学では，それぞれの品詞別の処理を仮定したり，意味的な脱落を考えたりする場合に，異なるモジュールを想定することが行われます。
- それらの仮定したモジュールが脳内に対応関係が存在するのであれば神経心理学的には説明として十分でしょう。
- ところが word2vec で示した表現では一つの意味と統語との表現を与える中間層に味方を変える (PCA など)で描画してみれば，異なる複数の言語知識を一つの表象で表現できることが示唆されます。
- word2vec による表現が脳内に分散していると考えるとカテゴリー特異性の問題や基本概念優位性の問題をどう捉えれば良いのかについて示唆に富むと考えます。

<center>
<img src="/assets/2013Mikolov_FigCountries.svg" style="width:49%">
</center>

横軸は国と首都との関係を表現しているとみなすことができます。縦軸は下から上に向かっておおまかにユーラシア大陸を西から東へ横断しているように配置されています。
意味を表現するということは，解釈によって，この場合 PCA によって 2 次元に図示してみると大まかに我々の知識を表現できることを示唆していると考えます。
