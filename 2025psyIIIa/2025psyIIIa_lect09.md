---
title: 第09回 2025 年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div align="center">
<font size="+2" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 13/Jun/2025<br/>
Appache 2.0 license<br/>
</div>

# 第 9 回

### 実習ファイル

* [転移学習による Stroop 効果のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1123Stroop_model.ipynb){:target="_blank"}
* [1990 年代の Stroop 効果のシミュレーション<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1110Stroop_1990Cohen_model.ipynb){:target="_blank"}
<!-- * [Stroop 効果シミュレーションの準備 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1123Stroop_model.ipynb){:target="_blank"} -->


<!-- * [三夕の歌  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0925RNN_3twilight_poetries.ipynb)
* [足し算モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb)
 -->
<!-- * [word2vec 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb) -->

<!-- - [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb)
- [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb) - [足し算のデモ keras 版 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb)-->

<!-- - [Bahdanau and Loung attentions <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb)
* [Attention is all you need <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022The_Annotated_%22Attention_is_All_You_Need%22.ipynb) -->

<!--
* [注意つき翻訳モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)
* [バニラ風味 注意なし翻訳モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1003vanilla_seq2seq2.ipynb) -->

<!-- * [GPT-3 を使って，自発話のシミュレーション <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0719japanese_gpt_1b.ipynb)
* [T5 による，文章穴埋め問題  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0918T5_demo_filling_blank_question.ipynb)
* [Seq2seq モデル による翻訳デモ 注意付きリカレントニューラルネットワーク <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)
* [BERT の微調整実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0623BERT_SNOW_training.ipynb) -->

<!-- - [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}
* [CNN 畳み込み層の可視化 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1024CNN_layer_visualization.ipynb){:target="_blank"}
* [各画像の画面表示時に日本語キャプションを付与する準備 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/ccap/blob/master/notebooks/2020importing_ccap_from_GitHub.ipynb){:target="_blank"}
* [EfficientNet のパラメータ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/drive/1QpKBHsBR5yvEOz2M-pKCUpliDh1XXplS){:target="_blank"}
* [データ拡張 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617plot_transforms_demo.ipynb) -->

<!-- - [TLPA 絵画命名検査 <img src='/assets/colab_icon.svg'>](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020_0724transfer_learning_tlpa_mnasnet.ipynb){:target="_blank"} -->
<!-- - [TLPA & SALA 絵画命名検査 <img src='/assets/colab_icon.svg'>](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618pnt_transfer_learning.ipynb){:target="_blank"} -->

<!-- - [フィラデルフィア絵画命名検査課題 PNT を転移学習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2020_0720tlpa_sala_resnet18.ipynb){:target="_blank"}
- [データ拡張 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617plot_transforms_demo.ipynb){:target="_blank"} -->

<!-- - [CAM 実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb){:target="_blank"} -->

<!-- - [各画像の画面表示時に日本語キャプションを付与する準備 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/ccap/blob/master/notebooks/2020importing_ccap_from_GitHub.ipynb){:target="_blank"}-->
<!-- - [EfficientNet のパラメータ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/drive/1QpKBHsBR5yvEOz2M-pKCUpliDh1XXplS) -->

<!-- - [絵画命名課題 TLPA+SALA <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/2025notebooks/2024_0913cnps2024_v2s.ipynb){:target="_blank"} -->
<!-- * [フィラデルフィア絵画命名検査課題 PNT を転移学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618pnt_transfer_learning.ipynb){:target="_blank"} -->


<!-- * [CAM 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb){:target="_blank"}
- [gradCAM <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1020gradCAM.ipynb){:target="_blank"}
* [フィラデルフィア絵画命名検査課題 PNT を転移学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618pnt_transfer_learning.ipynb){:target="_blank"} -->
* [百人一首の上の句とエンコーダによって符号化し，下の句をデコーダで生成する自作 Transformer モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}


* [三夕の歌 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0925RNN_3twilight_poetries.ipynb)
* [足し算モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb)

- [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb){:target="_blank"}
- [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}
<!-- - [百人一首データ取得](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0917get_hyakunin_isshu.ipynb){:target="_blank"} -->

<!-- - [BERT の超簡単な使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.c
om/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0903BERT_demo.ipynb){:target="_blank"} -->
<!-- - [最小限の MeCab](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2022_0916mecab_test.ipynb) -->
<!-- * [word2vec 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb) -->

<!-- - [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb)
- [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb) - [足し算のデモ keras 版 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb)-->

- [Bahdanau and Loung attentions <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb)
* [Attention is all you need <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022The_Annotated_%22Attention_is_All_You_Need%22.ipynb)

<!--
* [注意つき翻訳モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)
* [バニラ風味 注意なし翻訳モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1003vanilla_seq2seq2.ipynb) -->

<!-- * [GPT-3 を使って，自発話のシミュレーション <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0719japanese_gpt_1b.ipynb)
* [T5 による，文章穴埋め問題  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0918T5_demo_filling_blank_question.ipynb)
* [Seq2seq モデル による翻訳デモ 注意付きリカレントニューラルネットワーク <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)
* [BERT の微調整実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0623BERT_SNOW_training.ipynb) -->

<!-- ## キーワード -->

<!-- * 誤差逆伝播法 back propagation
* 勾配降下法 gradient descent method
* 学習率 -->
<!-- * モーメンタム -->
<!-- * [ソフトマックス関数 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0512softmax.ipynb){:target="_blank"} -->
<!-- * 勾配消失問題 gradient vanishing problem，勾配爆発問題 gradient explosion problem, -->
<!-- * ワンホットベクトル -->
<!-- * 単語埋め込み -->
<!-- * 勝者占有回路 -->
<!-- One hot vector,
word embeddings,
winners-take-all (winner takes it all) circuit.
ちはやふる -->

<!-- ## クイズ

* ML Machiine Learning, Mailing list, Maximum Likelihood
* SGD
* learning ratio
* pdf: probalility density function, portable document format

One hot vector,
word embeddings,
winners-take-all (winner takes it all) circuit.
ちはやふる -->




## Stroop 効果

<div class="figcenter">
<img src="/2023assets/1990Cohen_McClelland_stroop_fig3.svg">
<img src="/2023assets/2023_1110task_demand_conflict_ja.svg" width="49%">
</div>
<div class="figcaption" style="width:94%">

左: 図 3. 単語読解と色名学習後の接続強度を示すネットワーク図。 (強度は接続の横に示され，中間ユニット
のバイアスはユニットの内側に示されている。
課題要求ユニットから中間ユニットへの注意強度は固定され，中間ユニットのバイアスも固定された。
課題要求ユニットがオンのとき，対応する経路のユニットの基本入力が 0.0 になり，もう一方の経路のユニッ
トの基本入力が，実験によって -4.0 から -4.9 の範囲になるように選ばれた)。
<!-- Figure 3. Diagram of the network showing the connection strengths after training on the word-reading and color-naming tasks.
(Strengths are shown next to connections; biases on the intermediate units are shown inside the units.
Attention strengths-from task demand units to intermediate units-were fixed, as were biases for the intermediate units.
The values were chosen so that when the task demand unit was on, the base input for units in the corresponding pathway was 0.0, whereas the base input to units in the other pathway was in the range of
 -4.0 to -4.9, depending on the experiment.) -->

出典: Cohen, Dunbar, and McClelland (1990) __On the Control of Automatic Processes: A Parallel Distributed Processing Account of the Stroop Effect__, Psychological Review, Vol. 97, No. 3, 332-361.

右: 転移学習，微調整を用いた Stroop 課題の枠組み
</div>


# 再帰型ニューラルネットワーク

<!-- ## 2.1. NETtalk
系列情報処理を扱った初期のニューラルネットワーク例として NETTalk が挙げられます。
NETTalk[^NETTalk] は文字を音読するネットワークです。下図のような構成になっています。
下図のようにアルファベット 7 文字を入力して，空白はアンダーラインで表現されています，中央の文字の発音を学習する 3 層のニューラルネットワークです。NETTalk は 7 文字幅の窓を移動させながら
逐次中央の文字の発音を学習しました。たとえば /I ate the apple/ という文章では
"the" を "ザ" ではなく "ジ" と発音することになります。

印刷単語の読字過程のニューラルネットワークモデルである SM89[^SM89], PMSP96[^PMSP96] で用いられた発音表現は <a target="_blank" href="https://en.wikipedia.org/wiki/ARPABET">ARPABET</a> の亜種です。Python では `nltk` ライブラリを使うと ARPABET の発音を得ることができます(<a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_arpabet_test.ipynb">ARPABET のデモ<img src="/assets/colab_icon.svg"></a>)。

[^NETTalk]: Sejnowski, T.J. and Rosenberg, C. R. (1987) Parallel Networks that Learn to Pronounce English Text, Complex Systems 1, 145-168.
[^SM89]: Seidenberg, M. S. & McClelland, J. L. (1989). A distributed, developmetal model of word recognition and naming. Psychological Review, 96(4), 523–568.
[^PMSP96]: Plaut, D. C., McClelland, J. L., Seidenberg, M. S. & Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103, 56–115.

<center>
<img src="/assets/1986Sejnowski_NETtalkFig2.svg" style="width:47%"><br/>
Sejnowski (1986) Fig. 2
</center> -->

## 単純再帰型ニューラルネットワーク

<!-- NETTalk を先がけとして **単純再帰型ニューラルネットワーク** Simple Recurrent Neural networks (SRN) が提案された。 -->
発案者の名前で **Jordan ネット**，**Elman ネット** と呼ばれる。
<!-- [JordanNet]: Joradn, M.I. (1986) Serial Order: A Parallel Distributed Processing Approach, UCSD tech report.
[ElmanNet]: Elman, J. L. (1990)Finding structure in time, Cognitive Science, 14, 179-211. -->
Jordan ネットも Elman ネットも上位層からの **帰還信号** を持つ。
これを **フィードバック結合** と呼び，位置時刻前の状態が次の時刻に使われる。
Jordan ネットでは一時刻前の出力層の情報が用いられる (下図)。
一方，Elman ネットでは一時刻前の中間層の状態がフィードバック信号として用いられる。

<center>
<img src="/assets/SRN_J.svg" style="width:47%"><br/>
<div style="width:74%" align="center">
図：マイケル・ジョーダン発案ジョーダンネット [@1986Jordan]
</div>
</center>

<!-- - 駄菓子菓子 <a target="_blank" href="/assets/MJ_air.jpg">彼（マイケル・ジェフェリー(エアー)・ジョーダン）</a> ではない :)
- <a target="_blank" href="/assets/c3-s4-jordan.jpg">マイケル・アーウィン・ジョーダン。ミスター機械学習[^jordan_ai_revolution_not_yet]</a> -->
<!-- [^jordan_ai_revolution_not_yet]: 彼は(も？)神様です。多くの機械学習アルゴリズムを提案し続けている影響力のある人です。長らく機械学習の国際雑誌の編集長でした。2018年 <a target="_blank" href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">AI 革命は未だ起こっていない</a> と言い出して議論を呼びました。
-->

<center>
<img src="/assets/SRN_E.svg" style="width:47%"><br/>
<div style="align:center; width:47%">
図：ジェフ・エルマン発案のエルマンネット[@lman1990],[@Elman1993]
</div>
</center>

どちらも一時刻前の状態を短期記憶として保持して利用するのですが，実際の学習では一時刻前の状態をコピーして保存しておくだけで，実際の学習では通常の **誤差逆伝播法** すなわちバックプロパゲーション法が用いられる。
上 2 つの図に示したとおり U と W とは共に中間層への結合係数であり，V は中間層から出力層への結合係数である。
Z=I と書き点線で描かれている矢印はコピーするだけですので学習は起こりません。このように考えれば SRN は 3 層のニューラルネットワークであることが分かる。

SRN はこのような単純な構造にも関わらず **チューリング完全** であろうと言われてきた。
すなわちコンピュータで計算可能な問題はすべて計算できるくらい強力な計算機だという意味である。

- Jordan ネットは出力層の情報を用いるため **運動制御** に
- Elan ネットは内部状態を利用するため **言語処理** に

それぞれ用いられる。
従って **失行** aparxia (no matter what kind of apraxia such as 'ideomotor' or 'conceptual')，**行為障害** のモデルを考える場合 Jordan ネットは考慮すべき選択肢の候補の一つとなるだろう。

## リカレントニューラルネットワークの時間展開

一時刻前の状態を保持して利用する SRN は下図左のように描くことができる。
同時に時間発展を考慮すれば下図右のように描くことも可能である。

<center>
<img src="/assets/RNN_fold.svg" style="width:49%"><br/>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてみよ。
あるいは同義だ上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してほしい。
このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かる。

### エルマンネットによる言語モデル

下図に <a target="_blank" href="/assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示した。
図中の数字はニューロンの数を表す。
入力層と出力層のニューロン数 26 とは，用いた語彙数が 26 であったことを表す。

<center>
<img src="/assets/1991Elman_starting_small_Fig1.svg" style="width:47%"><br/>

from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示した。
Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示された (Elman, 1990, 1991, 1993)。

- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | lives

これらの規則にはさらに 2 つの制約がある。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもとらなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というものである。

下図に訓練後の中間層の状態を主成分分析にかけた結果を示した。
"boy chases boy", "boy sees boy", および "boy walks" という文を逐次入力した場合の遷移を示している。
同じ文型の文章は同じような状態遷移を辿ることが分かる。

<center>
<img src="/assets/1991Elman_Fig3.jpg" style="width:49%"><br/>
<!-- <p align="left" style="width:47%">
Trajectories through state space for sentences boy chases boy, boy sees boy, boy walks.
Principal component 1 is plotted along the abscissa; principal component 3 is plotted along the ordinate.
These two PC’s together encode differences in verb-argument expectations.
</p>-->
</center>

<center>
<img src="/assets/1991Elman_Fig4a.jpg" style="width:48%"><br/>
</center>

下図は文 "boy chases boy who chases boy" を入力した場合の遷移図である。
この文章には単語 "boy" が 3 度出てくる。
それぞれが異なるけれど，他の単語とは異なる位置に附置されていることがわかる。
同様に 'chases" が 2 度出てくるが，やはり同じような位置で，かつ，別の単語とは異なる位置に附置されている。<br/>

<center>
<img src="/assets/1991Elman_Fig4b.jpg" style="width:49%"><br/>
</center>

同様にして "boy who chases boy chases boy" (男の子を追いかける男の子が男の子を追いかける) の状態遷移図を下図に示した。<br/>
<center>

<img src="/assets/1991Elman_Fig4c.jpg" style="width:48%"><br/>
</center>

さらに複雑な文章例 "boy chases boy who chases boy who chases boy" の状態遷移図を下図に示す。</br>
<center>

<img src="/assets/1991Elman_Fig4d.jpg" style="width:48%"><br/>
</center>

Elman ネットが構文，文法処理ができるということは上図のような中間層での状態遷移で同じ単語が異なる文位置で異なる文法的役割を担っている場合に，微妙に異なる表象を，図に即してで言えば， 同じ単語では，同じような場所を占めるが，その文法的役割によって異なる位置を占めることが示唆される。
このことから中間層の状態は異なる文章の表現を異なる位置として表現していることが考えられ，後述する **単語の意味** や **自動翻訳** などに使われることに繋がります(浅川の主観半分以上)

<p align="left" style="width:74%">

関係節を含む文の状態で空間内の動き。
主成分 I は横軸に沿って表示され、主成分 II は縦軸に沿って表示される。
これら 2 つの主成分は、関係節の埋め込みの深さをエンコードしている。
<!-- Movement through state space for sentences with relative clauses. 
Principal component 1 is displayed along the abscissa; principal component 11 is displayed along the ordinate. 
These two PC’s encode depth of embedding in relative clauses. -->
</p>
</center>


<!-- ## 2.5. Seq2sep 翻訳モデル

上記の中間層の状態を素直に応用すると **機械翻訳** や **対話** のモデルになります。
下図は初期の翻訳モデルである "seq2seq" の概念図を示しました。
"`<eos>`" は文末 end of sentence を表します。中央の "`<eos>`" の前がソース言語であり，中央の "`<eos>`" の後はターゲット言語の言語モデルである SRN の中間層への入力として用います。

注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がないことです。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことです。
この点を改善することを目指すことが 2014 年以降盛んに行われてきました。
顕著な例が後述する **双方向 RNN**， **LSTM** を採用したり，**注意** 機構を導入することでした。 -->

<!--
<center>
<img src="/assets/RNN_fold.svg" style="width:94%"></br>
Time unfoldings of recurrent neural networks
</center>
-->

<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" style="width:88%"><br/>
From [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$
\mbox{argmax}_{\theta}
\left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)
$$ -->

## 多様な RNN とその万能性
双方向 RNN や LSTM を紹介する前に，カルパシーのブログから下図に引用する。
下の 2 つ図ではピンク色が入力層，緑が中間層，青が出力層を示している。

<!-- [^karpathy]: 去年までスタンフォード大学の大学院生。
現在はステラ自動車，イーロン・マスクが社長，の AI 部長さんです。図は彼のブログから引用です。蛇足ですがブログのタイトルが unreasonable effectiveness of RNN です。
過去の偉大な論文 Wiegner (1960), Hamming (1967), Halevy (2009) からの <del>パクリ</del> **敬意を表したオマージュ**です。
"unreasonable effectiveness of [science|mathematics|data]" $\ldots$ www -->

<center>
<img src="/assets/diags.jpeg" style="width:77%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

- 上図最左は通常の多層ニューラルネットワークで画像認識，分類，識別問題に用いられます。
- 上図左から 2 つ目は，画像からの文章生成
- 上図中央，左から 3 つ目は，極性分析，文章のレビュー，星の数推定
- 上図右から 2 つ目は翻訳や文章生成
- 上図最右はビデオ分析，ビデオ脚注付け

などに用いられます。これまで理解を促進する目的で中間層をただ一層として描いてきた。
だが中間層は多層化されていることの方が多いこと，中間層各層のニューロン数は 1024 程度まで用いられていることには注意。

数は各層のニューロン数が 4 つである場合の数値例を示しています。入力層では **ワンホット** 表現が用いられている。

ワンホット表現: ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。
一つだけが "熱い" あるいは "辛い" ベクトルと呼ぶ。
以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていたが ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派)と呼ばれることが多い。
ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じる。
典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新する。
従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになる。
そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行った。
上のエルマンネットによる文法学習において,ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当する。
単語埋め込み層を用いることで学習効率が改善し，後に示す word2vec などの **分散ベクトルモデル** へと発展する。

<center>
<img src="/assets/charseq.jpeg" style="width:66%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

<!-- 
- [@1991Siegelmann_RNN_universal] said Turing completeness of RNN.
- [用語集](/2023/2023lect03_glossary){:target="_blank"}
-->

<!--## 実習-->

<!--- [実習 画像認識 Keras 版<img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kera_CNN_demo_with_wordnet_ja.ipynb) -->
<!-- - [実習 Keras CNN](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb){:target="_blank"}
- [実習 Kermack McKendric model<img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"} -->
<!-- - [実習 PyTorch CNN <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0416PyTorch_CNN_demo.ipynb){:target="_blank"}
<!--- [実習](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb#scrollTo=oD497lby40Fp)-->

<!-- - [実習 画像認識 Keras 版](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kera_CNN_demo_with_wordnet_ja.ipynb) -->
<!-- - [実習 Keras CNN](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb){:target="_blank"} -->
<!-- - [実習 PyTorch CNN](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0416PyTorch_CNN_demo.ipynb){:target="_blank"} -->
<!-- - [実習 Kermack McKendric model](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}-->
<!--- [実習](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb#scrollTo=oD497lby40Fp)-->


<!-- - CNN: 畳み込みニューラルネットワーク
- RNN: リカレントニューラルネットワーク
- RL: 強化学習-->

<!-- <center> -->
<!--  <img src="https://komazawa-deep-learning.github.io/assets/2008Fuster_Prefrontal_Cortex_fig8_4.svg" width="39%"> -->
<!--  <img src="https://komazawa-deep-learning.github.io/assets/2015Ronneberger_U-Net_Fig1_ja.svg" width="48%"> -->
<!-- </center> -->

<!-- - [2018Kriegeskorte](2018Kriegeskorte){:target="_blank"}
- [1970Newell](1970Newell){:target="_blank"}
- [2019Glaser](2019Glaser){:target="_blank"}
- [2020Lindsay](2020Lindsay){:target="_blank"}
- [G 検定](https://www.seshop.com/product/detail/23864?utm_source=seid_it_spot_20210412&utm_medium=email&utm_campaign=coupon){:target="_blank"}

### 2021年02月23日分
- [2020-0215](2020-0215abstract){:target="_blank"}
- [どうぶつの森モデル，動物の名前連想モデル](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021_0223word_associtaion.ipynb){:target="_blank"}
- [導入講義用 CCP ウィルス感染者予測モデルを題材に](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}
- [CNN の簡単なデモ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Keras_CNN_demo_with_wordnet_ja.ipynb){:target="_blank"}

# 統計学と機械学習の関係

母集団における差異の有無を問題にする心理統計学と機械学習との間には，決定的な差があります。

- [1970Newell](1970Newell){:target="_blank"}
- [2019Glaser](2019Glaser){:target="_blank"}
- [2020Lindsay](2020Lindsay){:target="_blank"}

1. [tSNE を用いた TLPA 200語の word2vec 視覚化](https://ShinAsakawa.github.io/2020cnps_tSNE_for_word2vec.ipynb)
2. [2020年2月24日資料1 tlpa 画像](https://ShinAsakawa.github.io/2020making_tlpa.html)


<a href="https://guides.github.com/features/pages/">Read this page to write this page.</a>-->


### デモンストレーション

- [リカレントニューラルネットワークによる文字ベース言語モデルデモ](https://komazawa-deep-learning.github.io/character_demo.html){:target="_blank"}
- [リカレントニューラルネットワークによる文字ベース言語モデルデモ みんなの日本語](https://komazawa-deep-learning.github.io/minnichi_char_rnn.html)

<!-- ## 注意

* 認知心理学，生理学，などではトップダウンとボトムアップの 2 種類の注意が区別されてきた。計算論的には Crick (1984) のサーチライト仮説などにより提唱された **勝者占有回路 winner-take-all 回路** である。
* 自然言語処理 (Vaswani+2017)，画像処理 (Ramachandran2019) や眼球運動の DeepGaze (Kummerer2019) などがある。

### 文献

- [⼼理学，神経科学，機械学習における注意, Lindsay, 2019](https://project-ccap.github.io/2020Lindsay_Attention_inPsychology_Neuroscience_and_Machine_Learning.pdf){:target="_blank"}


### 心理学的注意

<img src="/assets/1988Treisman_Fig1.svg" width="24%">
<img src="/assets/1994Wolfe_GS2Fig2.jpg" width="39%">
<img src="/assets/1998IttiKoch_fig1.jpg" width="29%"><br/>

<p style="text-align: left;width:88%;background-color: cornsilk;">
左: Treisman1988 Fig.1，特徴統合理論の概念図。ボトムアップ注意
中: Wolfe1994 Fig.2 ガイド付き探索 バージョン 2 モデル。トップダウン注意
右: Itti and Koch (1998) Fig. 1 計算モデルの実装例
</p>

### ニューラルネットワーク的注意

#### CAM

<center>
<img src="/assets/2016Zhou_CAM_fig2ja.svg" width="66%"><br/>
CAM の概念図 Zhou (2016) Fig. 2 を改変
</center>

<br/><br/><br/>
<center>
<img src="/assets/2016Grad-CAM_boxer_tigercat.png" style="width:20%">
<img src="/assets/2016Grad-CAM_boxer.png" style="width:20%">
<img src="/assets/2016Grad-CAM_tigercat.png" style="width:20%"><br/>
Grad-CAM の結果。Selvaraj (2016) Fig. 5 より。左: 元画像。央: ボクサー犬と判断した場合のヒートマップ。右:トラ猫と判断した場合のヒートマップ
</center>



1. 交差エントロピー: $t\log(p) + (1-t)\log(1-p)$
損失関数として，最小自乗誤差を用いるよりも，分類課題での離散変数については，交差エントロピーを用いる方が収束が早く便利である。

# 注意とソフトマックス関数，CAM

前回は，自然画像分類のために訓練した畳み込みニューラルネットワークの結合係数を視覚化すると，特徴地図が現れることを示した。

今回は，この特徴地図を顕著性地図と考えて，ボトムアップによる注意を考える。
 -->

# 言語モデル

$$
P(x_{t+1}) = P(x|x_{i<t})
$$

1. RNN
2. LSTM, GRU
3. Transformer

<center>

<img src="/assets/SRN_J.svg" style="width:23%">
<img src="/assets/SRN_E.svg" style="width:23%">
<img src="/assets/2015Greff_LSTM_ja.svg" width="29%"><br/>
<img src="/assets/RNN_fold.svg" style="width:49%"><br>
Time unfoldings of recurrent neural networks

左：マイケル・ジョーダン発案ジョーダンネット [@1986Jordan]
中：ジェフ・エルマン発案エルマンネット
右: LSTM
</center>

の LSTM は一つのニューロンに該当します。このニューロンには 3 つのゲート(gate, 門) が付いています。
3 つのゲートは以下の名前で呼ばれます。

1. 入力ゲート input gate
2. 出力ゲート output gate
3. 忘却ゲート forget gate

各ゲートの位置を上図で確認してください。入力ゲートと出力ゲートが閉じていれば，セルの内容(これまでは中間層の状
態と呼んできました)が保持されることになります。
出力ゲートが開いている場合には，セル内容が出力されます。一方出力ゲートが閉じていればそのセル内容は出力されませ
ん。このように入力ゲートと出力ゲートはセル内容の入出力に関与します。
忘却ゲートはセル内容の保持に関与します。忘却ゲートが開いていれば一時刻前のセル内容が保持されることを意味します
。反対に忘却ゲートが閉じていれば一時刻前のセル内容は破棄されます。全セルの忘却ゲートが全閉ならば通常の多層ニュ
ーラルネットワークであることと同義です。すなわち記憶内容を保持しないことを意味します。SRN でフィードバック信号
が存在しない場合に相当します。セルへの入力は，

1. 下層からの信号，
2. 上層からの信号, すなわち Jordan ネットの帰還信号
3. 自分自身の内容，すなわち Elman ネットの帰還信号

が用いられます。これら入力信号が

1. 入力信号そのもの
2. 入力ゲートの開閉制御用信号
3. 出力ゲートの開閉制御用信号
4. 忘却ゲートの開閉制御用信号

という 4 種類に用いられます。従って LSTM のパラメータ数は SRN に比べて 4 倍になります。

LSTM に限らず一般のニューラルネットワークの出力には非線形関数が用いられます。代表的な非線形出力関数としては，
以下のような関数が挙げられます。

1. シグモイド関数: $f(x)=\left[1+e^{-x}\right]^{-1}$
2. ハイパーボリックタンジェント関数:  $f(x)=\left(e^{x}-e^{-x}\right)/\left(e^{x}+e^{-x}\right)$
3. 整流線形ユニット関数: $f(x)=\max\left(0,x\right)$

この中で，セルの出力関数として 2. のハイパーボリックタンジェント関数が，ゲートの出力関数にはシグモイド関数が使われます。
その理由はハイパーボリックタンジェント関数の方が収束が早いこと，シグモイド関数は値域が $[0,1]$ であるためゲートの開閉に直接対応しているからです。

- Le Cun, Y. Bottou, L., Orr, G. B, Muller K-R. (1988) Efficient BackProp, in Orr, G. and Muller, K. (Eds.) Neural Networks: tricks and trade, Springer.

<!--
The LSTM (left figure) can be described as the input signals $\mathbf{x}_t$ at
time $t$, the output signals $\mathbf{o}_t$, the forget gate $\mathbf{f}_t$, and
the output signal $\mathbf{y}_t$, the memory cell $\mathbf{c}_t$, then we can get
the following:
$i_{t}=\sigma\left(W_{xi}x_{t}+W_{hi}y_{t-1}+b_{i}\right)$, <br>
$f_{t}=\sigma\left(W_{xf}x_{t}+W_{hf}y_{t-1}+b_{f}\right)$, <br>
$o_{t}=\sigma\left(W_{xo}x_{t}+W_{ho}y_{t-1}+b_{o}\right)$, <br>
$g_{t}=\phi\left(W_{xc}x_{t}+W_{hc}y_{t-1}+b_{c}\right)$,<br>
$c_{t}=f_{t}\odot c_{t-1} + i_{t}\odot g_{t}$,<br>
$h_{t}=o_{t}\odot\phi\left(c_{t}\right)$<br>\label{eq:LSTM}
where
$\sigma\left(x\right)=\displaystyle\frac{1}{1+\mbox{exp}\left(-x\right)}$ (logistic function)
%% =1/2\left(\phi\Brc{x}+1\right)$,
$\phi\left(x\right)=\displaystyle\frac{\mbox{exp}\left(x\right)-\mbox{exp}\left(-x\right)}{\mbox{exp}\left(x\r
ight)+\mbox{exp}\left(-x\right)}$ (hyper tangent)
%% $=2\sigma\left(x\right)-1$
and $\odot$ menas Hadamard (element--wise) product.
-->

## LSTM におけるゲートの生理学的対応物 <!--Physiological correlates of gates in LSTM-->

以下の画像は <http://kybele.psych.cornell.edu/~edelman/Psych-2140/week-2-2.html> よりの引用。
ウミウシのエラ引っ込め反応時に，ニューロンへの入力信号ではなく，入力信号を修飾する結合が存在する。下図参照。

<center>
<img src="/assets/2016McComas_presynaptic_inhibition.jpg" style="width:24%">
<img src="/assets/C87-fig2_24.jpg" width="17%">
<img src="/assets/C87-fig2_25.jpg" width="33%"><br>
アメフラシ (Aplysia) のエラ引っ込め反応(a.k.a. 防御反応)の模式図[^seaslang]
<!-- <img src="/assets/shunting-inhibition.jpg" width="29%"> -->
</center>

* [注意機構の補足説明 大門他 (2023) <img src="https://www.adobe.com/content/dam/cc/en/legal/images/badges/PDF_32.png">](/2023/2023cnps注意機構の補足説明.pdf){:target="_blank"}



# ソフトマックス関数

## ゲートとして

<div class="figure figcenter">
<img src="/2023assets/2006Oreilly_wm_fig2.svg" width="49%">
<div class="figcaption">

#### 図 ゲートの概念図
ゲートが開いているときは，感覚入力は作業記憶を更新できるが，閉じているときはそれができない。
このため，妨害刺激など無関連情報が以前に記憶した情報の維持を妨げるのを防ぐことができる。
O'Reilly (2006) より
</div></div>


<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" style="width:88%"><br/>
From [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$
\mbox{argmax}_{\theta}
\left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)
$$ -->
