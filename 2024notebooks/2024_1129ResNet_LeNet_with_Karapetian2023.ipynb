{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_1129ResNet_LeNet_with_Karapetian2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22cbe53-8b42-4fcc-947b-6e490e8b2b5a",
      "metadata": {
        "id": "f22cbe53-8b42-4fcc-947b-6e490e8b2b5a"
      },
      "source": [
        "# ResNet + LeNet 実習 データは Karapetian+(2023) DOI:10.1162/jocn_a_02043\n",
        "\n",
        "<center>\n",
        "<img src='https://komazawa-deep-learning.github.io/assets/ResNet_Fig2.svg' width=\"33%\"><br/>\n",
        "<img src='https://komazawa-deep-learning.github.io/assets/2015ResNet30.svg' width=\"94%\"><br/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a898596e-f2ec-47d0-927a-e5069863f25d",
      "metadata": {
        "id": "a898596e-f2ec-47d0-927a-e5069863f25d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f'device:{device}')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    basedir = '/content/drive/Shareddrives/#2024認知心理学研究(1)b/浅川先生/2023Karapetian+OSF/Stimuli'\n",
        "    fnames = list(sorted(glob.glob(os.path.join(basedir,'*.jpg'))))\n",
        "else:\n",
        "    HOME = os.environ['HOME']\n",
        "    basedir = os.path.join(HOME, 'study/2024Agnessa14_Perceptual-decision-making.git/Stimuli')\n",
        "    fnames = list(sorted(glob.glob(os.path.join(basedir,'*.jpg'))))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0868a2e4-53db-40ee-8bc0-c3d8618db620",
      "metadata": {
        "id": "0868a2e4-53db-40ee-8bc0-c3d8618db620"
      },
      "source": [
        "# 刺激画像の表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f114055f-6334-4760-b5f0-2598e4c810c4",
      "metadata": {
        "id": "f114055f-6334-4760-b5f0-2598e4c810c4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 刺激画像の表示\n",
        "nrows, ncols = 6, 10\n",
        "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12,9))\n",
        "#fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14,10))\n",
        "\n",
        "i=0\n",
        "for row in range(nrows):\n",
        "    for col in range(ncols):\n",
        "        #img = PIL.Image.open(os.path.join(basedir, 'Stimuli/'+str(i+1)+'.jpg')).convert('RGB')\n",
        "        img = PIL.Image.open(os.path.join(basedir, str(i+1)+'.jpg')).convert('RGB')\n",
        "        ax[row][col].imshow(img)\n",
        "        ax[row][col].axis('off')\n",
        "        ax[row][col].set_title(f'{i+1}')\n",
        "        i += 1\n",
        "\n",
        "# 1-10: アパート\n",
        "#11-20: ベッド\n",
        "#21-30: 高速道路\n",
        "#31-40: 海岸\n",
        "#41-50: 峡谷\n",
        "#51-60: 森林\n",
        "# 1-30 は，人工物情景であり，31-60 は，自然情景"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae839b59-422b-4804-933b-58a5329cd5cc",
      "metadata": {
        "id": "ae839b59-422b-4804-933b-58a5329cd5cc"
      },
      "source": [
        "# 課題とバッチサイズの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03858db3-4b0b-4718-b3eb-bee3136a8517",
      "metadata": {
        "id": "03858db3-4b0b-4718-b3eb-bee3136a8517"
      },
      "outputs": [],
      "source": [
        "task = 'desc'  # 識別課題\n",
        "task = 'cat'   # カテゴリー弁別課題\n",
        "\n",
        "# 'cat': カテゴリー化課題 (人工物か自然物か)，\n",
        "# 'desc': 判別課題 (1-10:アパート，11-20:ベッド，21-30:高速道路, 31-40:海岸, 41-50:峡谷, 51-60:森林\n",
        "\n",
        "batch_size = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b9c206-5e7f-4e08-abfd-4b1a4b7f7e4b",
      "metadata": {
        "id": "52b9c206-5e7f-4e08-abfd-4b1a4b7f7e4b"
      },
      "source": [
        "# データセットの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cfdb4a-40bb-4e5d-93bd-6e4af2384057",
      "metadata": {
        "id": "12cfdb4a-40bb-4e5d-93bd-6e4af2384057"
      },
      "outputs": [],
      "source": [
        "# データセットの作成\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "# 乱数シード固定（再現性の担保）\n",
        "def fix_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "\n",
        "# データローダーのサブプロセスの乱数の seed 固定\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "\n",
        "for i in range(8):\n",
        "    worker_init_fn(i)\n",
        "\n",
        "class Agressa2023_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 task:str='cat',\n",
        "# 'cat': カテゴリー化課題 (人工物か自然物か)，\n",
        "# 'desc': 判別課題 (1-10:アパート，11-20:ベッド，21-30:高速道路, 31-40:海岸, 41-50:峡谷, 51-60:森林\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        if task == 'desc':\n",
        "            self.task = 'desc'\n",
        "        else:\n",
        "            self.task = 'cat'\n",
        "\n",
        "        if isColab:\n",
        "            self.basedir = '/content/drive/Shareddrives/#2024認知心理学研究(1)b/浅川先生/2023Karapetian+OSF/Stimuli'\n",
        "            self.fnames = list(sorted(glob.glob(os.path.join(basedir,'*.jpg'))))\n",
        "        else:\n",
        "            HOME = os.environ['HOME']\n",
        "            self.basedir = os.path.join(HOME, 'study/2024Agnessa14_Perceptual-decision-making.git/Stimuli')\n",
        "            self.fnames = list(sorted(glob.glob(os.path.join(basedir,'*.jpg'))))\n",
        "        self.fname = fnames\n",
        "\n",
        "        self.Img_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize(224),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fname)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_fname = self.fname[idx]\n",
        "        #img = PIL.Image.open(os.path.join(basedir, 'Stimuli/'+str(idx+1)+'.jpg')).convert('RGB')\n",
        "        img = PIL.Image.open(os.path.join(self.basedir, str(idx+1)+'.jpg')).convert('RGB')\n",
        "        #X = torchvision.transforms.functional.pil_to_tensor(img)\n",
        "        X = self.Img_transform(img)\n",
        "\n",
        "        if self.task == 'cat':\n",
        "            # カテゴリ弁別課題であれば，idx < 30 なら 0 (人工物), idx >= 30 なら 1 (自然物) という二値弁別課題\n",
        "            label = idx // 30\n",
        "        else:\n",
        "            # 識別課題であれば，idx < 10:アパート，10<=idx<20:ベッド，20<=idx<30:高速道路\n",
        "            #              30<=idx<40:海岸,   40<=idx<50:峡谷, 50<=idx<60:森林\n",
        "            label = idx // 10\n",
        "\n",
        "        return X, label\n",
        "\n",
        "ds = Agressa2023_dataset(task=task)\n",
        "train_dl = torch.utils.data.DataLoader(ds,\n",
        "                                       batch_size=batch_size,    # バッチサイズ\n",
        "                                       shuffle=True,     # データシャッフル\n",
        "                                       num_workers=0,    # 高速化\n",
        "                                       pin_memory=True,  # 高速化\n",
        "                                       worker_init_fn=worker_init_fn\n",
        "                                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e35951f0-20ea-44a3-8178-8c48c95affbd",
      "metadata": {
        "id": "e35951f0-20ea-44a3-8178-8c48c95affbd"
      },
      "source": [
        "# モデルの定義"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca3f571-57e0-4132-9fc8-2c56b17a1e35",
      "metadata": {
        "id": "8ca3f571-57e0-4132-9fc8-2c56b17a1e35"
      },
      "source": [
        "## `torch.nn.Conv2d` の引数\n",
        "\n",
        "* in_channels: int, 入力特徴数，(チャンネル数)\n",
        "* out_channels: int, 出力特徴数 (チャンネル数)\n",
        "* kernel_size: Union[int, Tuple[int, int]], カーネルサイズ，数字を 1 つだけ与えると 縦横とも同じサイズのカーネル幅になる\n",
        "* stride: Union[int, Tuple[int, int]] = 1,  ストライド，カーネルをスライドさせる幅，数字を 1 だけ与えると縦横とも同サイズの幅となる\n",
        "* padding: Union[str, int, Tuple[int, int]] = 0, 4 角に加える幅，数字を 1 つだけ与えると上下左右とも同サイズの幅にまる。\n",
        "W x H の画像に対して，横幅は，W_pad + W + W_pad となり，縦長は H_pad + H + H_pad となるので，H x W の入力データが (W + 2 W_pad) * (H + 2 H_pad) のサイズとなる\n",
        "* dilation: Union[int, Tuple[int, int]] = 1\n",
        "ダイレーションの幅，畳み込みカーネルの間隙を指定する\n",
        "* groups: int = 1,\n",
        "* bias: bool = True,\n",
        "バイアス項 `X @ w + b` にするときの `b` のこと\n",
        "* padding_mode: str = 'zeros', device=None, dtype=None) -> None        \n",
        "`padding` で指定した 4 角の拡張領域をどのような数値で埋めるかを指定する。デフォルトでは `zeros` すなわち 0 パディングとなる。\n",
        "そのほかに取りうる値は，`reflect`, `replicate`, `circular` である。\n",
        "\n",
        "$$\n",
        "H_{\\text{out}} = \\frac{H_{in} + 2 H_{\\text{pad}} - H_{\\text{dilation}}\\times \\left(H_{\\text{kernel}} -1\\right)+1}{H_{\\text{stride}}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_{\\text{out}} = \\frac{W_{in} + 2 W_{\\text{pad}} - W_{\\text{dilation}}\\times \\left(W_{\\text{kernel}} -1\\right)+1}{W_{\\text{stride}}}\n",
        "$$\n",
        "\n",
        "## `torch.nn.MacPool2d` の引数\n",
        "\n",
        "* kernel_size: Union[int, Tuple[int, ...]],\n",
        "* stride: Union[int, Tuple[int, ...], NoneType] = None,\n",
        "* padding: Union[int, Tuple[int, ...]] = 0,\n",
        "* dilation: Union[int, Tuple[int, ...]] = 1,\n",
        "* return_indices: bool = False,\n",
        "* ceil_mode: bool = False) -> None\n",
        "\n",
        "$$\n",
        "out(N_i,C_j,h,w)= \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1}\n",
        "\\text{input}\\left(N_i, C_j, \\text{stride[0]} \\times h + m, \\text{stride[1]} \\times w + n\\right)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c74c6ed-5286-4fd3-be20-855a211449ab",
      "metadata": {
        "id": "9c74c6ed-5286-4fd3-be20-855a211449ab"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "def conv3x3(\n",
        "    in_features:int,\n",
        "    out_features:int,\n",
        "    stride:int=1,\n",
        "    groups:int=1,\n",
        "    dilation:int=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_channels=in_features,\n",
        "        out_channels=out_features,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(\n",
        "    in_features:int,\n",
        "    out_features:int,\n",
        "    stride:int=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_channels=in_features,\n",
        "        out_channels=out_features,\n",
        "        kernel_size=1,\n",
        "        stride=stride,\n",
        "        bias=False)\n",
        "\n",
        "\n",
        "class ResNet_BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_features:int,\n",
        "                 out_features:int,\n",
        "                 stride:int=1,\n",
        "                 downsample:bool=None,\n",
        "                 groups:int=1,\n",
        "                 base_width:int=64,\n",
        "                 dilation:int=1,\n",
        "                 norm_layer:bool=None):\n",
        "\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self.conv1 = conv3x3(\n",
        "            in_features=in_features,\n",
        "            out_features=out_features,\n",
        "            stride=stride)\n",
        "        self.bn1 = norm_layer(out_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_features, out_features)\n",
        "        self.bn2 = norm_layer(out_features)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eefe4fe-822c-4627-8250-35a581d11779",
      "metadata": {
        "id": "2eefe4fe-822c-4627-8250-35a581d11779"
      },
      "outputs": [],
      "source": [
        "class ResNet_Bottleneck(nn.Module):\n",
        "    # ボトルネックは、ダウンサンプリングのストライドを 3x3 convolution(self.conv2) に置くのに対し、\n",
        "    # オリジナルの実装では最初の 1x1 convolution(self.conv1) にしています。\n",
        "    # [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385) によると、\n",
        "    # ダウンサンプリングのストライドを 3x3 convolution(self.conv2) にしています。\n",
        "    # このバージョンは ResNet V1.5 としても知られており、精度が向上しています。\n",
        "    # (https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch)\n",
        "\n",
        "    #expansion: int = 4\n",
        "\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 stride=1,\n",
        "                 downsample=None,\n",
        "                 groups=1,\n",
        "                 base_width=64,\n",
        "                 dilation=1,\n",
        "                 norm_layer= None):\n",
        "\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(out_featuress * (base_width / 64.0)) * groups\n",
        "\n",
        "        self.expansion = 4\n",
        "\n",
        "        self.conv1 = conv1x1(in_features, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, otu_features * self.expansion)\n",
        "        self.bn3 = norm_layer(out_features * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, inp):\n",
        "        identity = inp\n",
        "\n",
        "        out = self.conv1(inp)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 block,\n",
        "                 layers,\n",
        "                 in_channels = 3,\n",
        "                 num_classes = 40, # 1000,\n",
        "                 zero_init_residual=False,\n",
        "                 groups=1,\n",
        "                 width_per_group=64,\n",
        "                 #width_per_group=8,\n",
        "                 norm_layer=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.in_features = 64\n",
        "        self.dilation = 1\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(in_channels, self.in_features, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.in_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # 各残差ブランチの最後のバッチ正規化をゼロ初期化し，残差ブランチがゼロで始まり，各残差ブロックが恒等写像のように振る舞うようにします。\n",
        "        # https://arxiv.org/abs/1706.02677 によると，これによりモデルが 0.2~0.3 %改善されます。\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self,\n",
        "                    block,\n",
        "                    out_features,\n",
        "                    blocks,\n",
        "                    stride=1):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if stride != 1 or self.in_features != out_features * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.in_features, out_features * block.expansion, stride),\n",
        "                norm_layer(out_features * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.in_features, out_features, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.in_features = out_features * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.in_features,\n",
        "                    out_features,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    \"\"\"ResNet-18 model from [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "    \"\"\"\n",
        "    return _resnet(\"resnet18\", ResNet_BasicBlock, [1, 1, 1, 1], **kwargs)\n",
        "    #return _resnet(\"resnet18\", ResNet_BasicBlock, [2, 2, 2, 2], **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81bacbf6-9287-433d-8cc9-dfaaa6c637d0",
      "metadata": {
        "id": "81bacbf6-9287-433d-8cc9-dfaaa6c637d0"
      },
      "outputs": [],
      "source": [
        "#from torchsummary import summary\n",
        "\n",
        "in_channels=3\n",
        "a_model = resnet18(in_channels=in_channels, num_classes=6).to(device)\n",
        "#print(tmp_resnet_model)\n",
        "#summary(model, input_size=(in_channels,64,64))\n",
        "a_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58256458-e3ee-4d6e-a789-774fd9fac5fe",
      "metadata": {
        "id": "58256458-e3ee-4d6e-a789-774fd9fac5fe"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "#lr = 0.1\n",
        "#lr = 0.001\n",
        "# 最適化手法の設定\n",
        "# #optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(params=a_model.parameters(), lr=lr)\n",
        "#type(optimizer)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#criterion = torch.nn.MSELoss()\n",
        "\n",
        "verbose = False\n",
        "n_epochs = 20\n",
        "a_model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.\n",
        "    n_corrects = 0\n",
        "    for X, y in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        out = a_model(X.to(device))\n",
        "        if verbose:\n",
        "            print(f'エポック:{epoch+1}',\n",
        "                  f'教師:{y.detach().numpy()}',\n",
        "                  f'出力:{out.argmax(dim=1).numpy()}',\n",
        "                  f'正誤:{(y.numpy() == out.argmax(dim=1).numpy()) * 1}'\n",
        "                 )\n",
        "        loss = criterion(out.cpu(), y.cpu())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        n_corrects += np.sum((y.cpu().numpy() == out.cpu().argmax(dim=1).numpy()) * 1)\n",
        "\n",
        "    print(f'エポック:{epoch+1:03d}',\n",
        "          f'損失:{epoch_loss:.3f}',\n",
        "          f'正解率:{n_corrects}/{ds.__len__()}'\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d308514b-ea4a-4d84-b793-472b7c83e2f4",
      "metadata": {
        "id": "d308514b-ea4a-4d84-b793-472b7c83e2f4"
      },
      "source": [
        "# 学習済モデルを用いる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b70d08c-a443-43ee-8c85-a91165b2ea36",
      "metadata": {
        "id": "0b70d08c-a443-43ee-8c85-a91165b2ea36"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "\n",
        "# ここでは，AlexNet か ResNet18, ResNet50 を仮定している\n",
        "#a_model = models.alexnet(weights='DEFAULT', progress=True)\n",
        "#a_model = models.alexnet(weights=None, progress=True)\n",
        "#a_model_name = 'AlexNet'\n",
        "\n",
        "#a_model = models.resnet50(weights='DEFAULT', progress=True)\n",
        "#a_model = models.resnet50(weights=None, progress=True)\n",
        "#a_model_name = 'ResNet50'\n",
        "\n",
        "a_model = models.resnet18(weights='DEFAULT', progress=True)\n",
        "a_model = models.resnet18(weights=None, progress=True)\n",
        "a_model_name = 'ResNet18'\n",
        "\n",
        "print(a_model.eval())\n",
        "print(f'a_model_name:{a_model_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f136c6-af88-40a2-9fd1-dd99c68748cf",
      "metadata": {
        "id": "14f136c6-af88-40a2-9fd1-dd99c68748cf"
      },
      "outputs": [],
      "source": [
        "a_parameters = {name:param for name, param in a_model.named_parameters()}\n",
        "a_modules = {name:param for name, param in a_model.named_modules()}\n",
        "\n",
        "print(f'パラメータ名:{a_parameters.keys()}')\n",
        "print(f'モジュール名:{a_modules.keys()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f8ea49-3c19-492f-bb2f-8492e105227d",
      "metadata": {
        "id": "77f8ea49-3c19-492f-bb2f-8492e105227d"
      },
      "outputs": [],
      "source": [
        "# 解くべき課題に応じて，最終層の素子数を変更する\n",
        "if task == 'desc':  # 弁別課題の場合\n",
        "    out_features = 6\n",
        "elif task == 'cat': # カテゴリー弁別課題の場合\n",
        "    out_features = 2\n",
        "\n",
        "if a_model_name == 'AlexNet':\n",
        "    in_features = a_model.classifier[6].in_features\n",
        "    a_model.classifier[6] = torch.nn.Linear(in_features=in_features, out_features=out_features)\n",
        "elif a_model_name == 'ResNet50':\n",
        "    in_features = a_model.fc.in_features\n",
        "    a_model.fc = torch.nn.Linear(in_features=in_features, out_features=out_features)\n",
        "elif a_model_name == 'ResNet18':\n",
        "    in_features = a_model.fc.in_features\n",
        "    a_model.fc = torch.nn.Linear(in_features=in_features, out_features=out_features)\n",
        "\n",
        "print(f'in_features:{in_features}',\n",
        "      f'out_feaures:{out_features}'\n",
        "     )\n",
        "a_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e96fbc1-f224-4635-b122-c05b7d982a93",
      "metadata": {
        "id": "2e96fbc1-f224-4635-b122-c05b7d982a93"
      },
      "outputs": [],
      "source": [
        "if a_model_name == 'AlexNet':\n",
        "    update_param_names = ['classifier.6.weight', 'classifier.6.bias']\n",
        "elif 'ResNet' in a_model_name:\n",
        "    update_param_names = ['fc.weight', 'fc.bias']\n",
        "params_to_update = []\n",
        "for name, param in a_model.named_parameters():\n",
        "    if name in update_param_names:\n",
        "        param.requires_grad = True\n",
        "        params_to_update.append(param)\n",
        "        print(name)\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "#print(f'parmas_to_update:{params_to_update}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5916974-fa89-4daf-80a0-2616ac18e45b",
      "metadata": {
        "id": "e5916974-fa89-4daf-80a0-2616ac18e45b"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "lr = 0.001\n",
        "# 最適化手法の設定\n",
        "# #optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(params=params_to_update, lr=lr)\n",
        "#type(optimizer)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#criterion = torch.nn.MSELoss()\n",
        "\n",
        "verbose = False\n",
        "n_epochs = 20\n",
        "a_model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.\n",
        "    n_corrects = 0\n",
        "    for X, y in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        out = a_model(X)\n",
        "        if verbose:\n",
        "            print(f'エポック:{epoch+1}',\n",
        "                  f'教師:{y.detach().numpy()}',\n",
        "                  f'出力:{out.argmax(dim=1).numpy()}',\n",
        "                  f'正誤:{(y.numpy() == out.argmax(dim=1).numpy()) * 1}'\n",
        "                 )\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        n_corrects += np.sum((y.numpy() == out.argmax(dim=1).numpy()) * 1)\n",
        "\n",
        "    print(f'エポック:{epoch+1:03d}',\n",
        "          f'損失:{epoch_loss:.3f}',\n",
        "          f'正解率:{n_corrects}/{ds.__len__()}'\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c3c227-2489-46d0-8ec6-61d2ce5c4868",
      "metadata": {
        "id": "94c3c227-2489-46d0-8ec6-61d2ce5c4868"
      },
      "outputs": [],
      "source": [
        "# ここでは，AlexNet か ResNet18, ResNet50 を仮定している\n",
        "a_model = models.alexnet(weights='DEFAULT', progress=True).to(device)\n",
        "#a_model = models.alexnet(weights=None, progress=True).to(device)\n",
        "a_model_name = 'AlexNet'\n",
        "a_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "049a7d3d-6998-4dee-9ab3-8a7fdf9730ae",
      "metadata": {
        "id": "049a7d3d-6998-4dee-9ab3-8a7fdf9730ae"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "optimizer = torch.optim.Adam(params=a_model.parameters(), lr=lr)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "a_model.train()\n",
        "n_epochs = 50\n",
        "verbose = False\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.\n",
        "    n_corrects = 0\n",
        "    for X, y in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        out = a_model(X.to(device))\n",
        "        if verbose:\n",
        "            print(f'エポック:{epoch+1}',\n",
        "                  f'教師:{y.detach().numpy()}',\n",
        "                  f'出力:{out.cpu().argmax(dim=1).numpy()}',\n",
        "                  f'正誤:{(y.numpy() == out.cpu().argmax(dim=1).numpy()) * 1}'\n",
        "                 )\n",
        "        loss = criterion(out.cpu(), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        n_corrects += np.sum((y.numpy() == out.cpu().argmax(dim=1).numpy()) * 1)\n",
        "\n",
        "    print(f'エポック:{epoch+1:03d}',\n",
        "          f'損失:{epoch_loss:.3f}',\n",
        "          f'正解率:{n_corrects}/{ds.__len__()}'\n",
        "         )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}