{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2025_0110stable_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-vX3cavOCt"
      },
      "source": [
        "# diffuser ã‚’ç”¨ã„ãŸ **[Stable Diffusion](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=2rR2Udg5IYjn)**\n",
        "\n",
        "Stable Diffusion ã¯ [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), [LAION](https://laion.ai/) ã®ç ”ç©¶è€…ã¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã«ã‚ˆã£ã¦ä½œæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸å¤‰æ›ã™ã‚‹æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚\n",
        "ã“ã‚Œã¯ [LAION-5B](https://laion.ai/blog/laion-5b/) ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‹ã‚‰ 512x512 ç”»åƒã§å­¦ç¿’ã•ã‚Œã‚‹ã€‚\n",
        "ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ï¼Œå‡çµã•ã‚ŒãŸ CLIP ViT-L/14 ãƒ†ã‚­ã‚¹ãƒˆç¬¦å·åŒ–å™¨ã‚’ä½¿ã„ï¼Œãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¡ä»¶ã¨ã—ã¦ã„ã‚‹ã€‚\n",
        "860M ã® U-Net ã¨ 123M ã®ãƒ†ã‚­ã‚¹ãƒˆç¬¦å·åŒ–å™¨ã‚’æŒã¤ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ï¼Œæ¯”è¼ƒçš„è»½é‡ã§ï¼Œå¤šãã®æ°‘ç”Ÿç”¨ GPU ã§å®Ÿè¡Œå¯èƒ½ã§ã‚ã‚‹ã€‚\n",
        "è©³ã—ãã¯ [ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰](https://huggingface.co/CompVis/stable-diffusion) ã‚’å‚ç…§ã€‚\n",
        "<!-- Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/).\n",
        "It's trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database.\n",
        "This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts.\n",
        "With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on many consumer GPUs.\n",
        "See the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information. -->\n",
        "\n",
        "\n",
        "æœ¬ Colab ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ Hugging Face [Diffusers ãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://github.com/huggingface/diffusers) ã‚’ä½¿ã£ãŸ Stable Diffusion ã®ä½¿ã„æ–¹ã‚’ç´¹ä»‹ã™ã‚‹ã€‚\n",
        "<!-- This Colab notebook shows how to use Stable Diffusion with the ğŸ¤— Hugging Face [ğŸ§¨ Diffusers library](https://github.com/huggingface/diffusers).\n",
        "\n",
        "ã•ã£ããå§‹ã‚ã¾ã—ã‚‡ã†ï¼<!--Let's get started! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xMJ6LaET6dT"
      },
      "source": [
        "## 1. `StableDiffusionPipline` ã®ä½¿ã„æ–¹\n",
        "<!-- ## 1. How to use `StableDiffusionPipeline` -->\n",
        "\n",
        "Stable Diffusion ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ç†è«–çš„ãªå´é¢ã«é£›ã³è¾¼ã‚€å‰ã«ï¼Œå°‘ã—è©¦ã—ã¦ã¿ã‚ˆã†ã€‚\n",
        "<!-- Before diving into the theoretical aspects of how Stable Diffusion functions, let's try it out a bit ğŸ¤—.-->\n",
        "\n",
        "æœ¬ç¯€ã§ã¯ï¼Œã‚ãšã‹æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã™ã‚‹ï¼\n",
        "<!--In this section, we show how you can run text to image inference in just a few lines of code! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOlvQ1nQL7c"
      },
      "source": [
        "### ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "<!-- ### Setup-->\n",
        "\n",
        "æœ€åˆã«ã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã« GPU ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã›ã‚ˆã€‚\n",
        "ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ãŒå¤±æ•—ã™ã‚‹å ´åˆã¯ï¼Œä¸Šã®  `Runtime`  ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ `Change runtime type`  ã‚’é¸æŠã›ã‚ˆã€‚\n",
        "<!--First, please make sure you are using a GPU runtime to run this notebook, so inference is much faster.\n",
        "If the following command fails, use the `Runtime` menu above and select `Change runtime type`. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zHkHsdtnry57"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lErpT8-EAgrN",
        "outputId": "daf1e35f-e866-4036-e32c-2b969716ab0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul 18 00:51:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    # Shows the nVidia GPUs, if this system has any\n",
        "    !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJt_cx5QgVz"
      },
      "source": [
        "æ¬¡ã« `diffusers`  ã¨ `scipy`, `ftfy`, `transformers` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚\n",
        "`accelerate` ã¯èª­ã¿è¾¼ã¿ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ã€‚\n",
        "<!-- Next, you should install `diffusers` as well `scipy`, `ftfy` and `transformers`. `accelerate` is used to achieve much faster loading. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.11.1\n",
        "!pip install transformers scipy ftfy accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NnPOMAqAABv"
      },
      "source": [
        "### Stable Diffusion ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "<!-- ### Stable Diffusion Pipeline-->\n",
        "\n",
        "`StableDiffusionPipeline` ã¯ï¼Œã‚ãšã‹æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã‚ã‚‹ã€‚\n",
        "<!-- `StableDiffusionPipeline` is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code. -->\n",
        "\n",
        "ã¾ãšï¼Œãƒ¢ãƒ‡ãƒ«ã®å…¨æˆåˆ†ã®äº‹å‰å­¦ç¿’ã•ã‚ŒãŸé‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚\n",
        "æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ Stable Diffusion version 1.4 ([CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)) ã‚’ä½¿ã†ãŒï¼Œä»–ã«ã‚‚è©¦ã—ã¦ã¿ãŸã„ã‚‚ã®ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ï¼š\n",
        "<!--First, we load the pre-trained weights of all components of the model. In this notebook we use Stable Diffusion version 1.4 ([CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)), but there are other variants that you may want to try: -->\n",
        "* [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
        "* [stabilityai/stable-diffusion-2-1-base](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)\n",
        "* [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1).\n",
        "ã“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ 768x768 ã®è§£åƒåº¦ã®ç”»åƒã‚’ä½œæˆã§ãã‚‹ãŒã€ä»–ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ 512x512 ã§å‹•ä½œã™ã‚‹ã€‚\n",
        "<!-- This version can produce images with a resolution of 768x768, while the others work at 512x512. -->\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ« ID [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) ã«åŠ ãˆã¦ã€ç‰¹å®šã® `revision` ã¨ `torch_dtype` ã‚’ `from_pretrained` ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã—ã¦ã„ã‚‹ã€‚\n",
        "<!-- In addition to the model id [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4), we're also passing a specific `revision` and `torch_dtype` to the `from_pretrained` method.-->\n",
        "\n",
        "\n",
        "ãƒ•ãƒªãƒ¼ã® Google Colab ãŒã™ã¹ã¦ Stable Diffusion ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã„ã®ã§ã€åŠç²¾åº¦ãƒ–ãƒ©ãƒ³ãƒ[`fp16`](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16) ã‹ã‚‰é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€`torch_dtype=torch.float16` ã‚’æ¸¡ã™ã“ã¨ã§ `diffusers` ã« float16 ã®ç²¾åº¦ã§é‡ã¿ã‚’æœŸå¾…ã™ã‚‹ã‚ˆã†ã«ä¼ãˆã¦ã„ã‚‹ã€‚\n",
        "<!-- We want to ensure that every free Google Colab can run Stable Diffusion, hence we're loading the weights from the half-precision branch [`fp16`](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16) and also tell `diffusers` to expect the weights in float16 precision by passing `torch_dtype=torch.float16`. -->\n",
        "\n",
        "å¯èƒ½ãªé™ã‚Šé«˜ã„ç²¾åº¦ã‚’ç¢ºä¿ã—ãŸã„å ´åˆã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—ãˆã‚‹ã“ã¨ã‚’è¦šæ‚Ÿã§ `torch_dtype=torch.float16` ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- If you want to ensure the highest possible precision, please make sure to remove `torch_dtype=torch.float16` at the cost of a higher memory usage. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSrhSdsGAgrO"
      },
      "outputs": [],
      "source": [
        "# This is added to get around some issues of Torch not loading models correctly (test on Mac OS X and Kubuntu Linux)\n",
        "!pip install --upgrade huggingface-hub==0.26.2 transformers==4.46.1 tokenizers==0.20.1 diffusers==0.31.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MgNzTxwbASv"
      },
      "source": [
        "ã‚ˆã‚Šé«˜é€Ÿãªæ¨è«–ã‚’è¡Œã†ãŸã‚ï¼Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ GPU ã«ç§»ã™ã€‚\n",
        "<!-- Next, let's move the pipeline to GPU to have faster inference. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LA9myHTxbDhm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device=torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device=torch.device(\"mps\")\n",
        "\n",
        "pipe = pipe.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSTsT6McuzWW"
      },
      "source": [
        "ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æº–å‚™ãŒæ•´ã£ãŸï¼š\n",
        "<!-- And we are ready to generate images: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEErJFjlrSWS"
      },
      "outputs": [],
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "image = pipe(prompt).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "# Now to display an image you can either save it such as:\n",
        "image.save(f\"astronaut_rides_horse.png\")\n",
        "\n",
        "# or if you're in a google colab you can directly display it with\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZCwCecVJI3"
      },
      "source": [
        "ä¸Šè¨˜ã®ã‚»ãƒ«ã‚’è¤‡æ•°å›å®Ÿè¡Œã™ã‚‹ã¨ï¼Œæ¯å›ç•°ãªã‚‹ç”»åƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚\n",
        "ã‚‚ã—æ±ºå®šè«–çš„ãªå‡ºåŠ›ã‚’æœ›ã‚€ãªã‚‰ï¼Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãƒ©ãƒ³ãƒ€ãƒ ãªç¨®ã‚’æ¸¡ã™ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "åŒã˜ç¨®ã‚’ä½¿ã†ãŸã³ã«ï¼ŒåŒã˜ç”»åƒçµæœãŒå¾—ã‚‰ã‚Œã‚‹ã€‚\n",
        "<!-- Running the above cell multiple times will give you a different image every time.\n",
        "If you want deterministic output you can pass a random seed to the pipeline.\n",
        "Every time you use the same seed you'll have the same image result. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaAW4sSdV7vZ"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "generator = torch.Generator(device).manual_seed(seed)\n",
        "image = pipe(prompt, generator=generator).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RrbYQVQXK6I"
      },
      "source": [
        "`num_inference_steps` å¼•æ•°ã§æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å¤‰æ›´ã§ãã‚‹ã€‚\n",
        "ä¸€èˆ¬çš„ã«ï¼Œã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒå¤šã„ã»ã©è‰¯ã„çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã€‚\n",
        "æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã® 1 ã¤ã§ã‚ã‚‹ Stable Diffusion ã¯ï¼Œæ¯”è¼ƒçš„å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ã†ã¾ãæ©Ÿèƒ½ã™ã‚‹ã®ã§ï¼Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®`50` ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã™ã‚‹ã€‚\n",
        "ã‚ˆã‚Šé€Ÿã„çµæœã‚’å¾—ãŸã„ã®ã§ã‚ã‚Œã°ï¼Œã‚ˆã‚Šå°ã•ã„æ•°ã‚’ä½¿ã†ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "<!-- You can change the number of inference steps using the `num_inference_steps` argument.\n",
        "In general, results are better the more steps you use.\n",
        "Stable Diffusion, being one of the latest models, works great with a relatively small number of steps, so we recommend to use the default of `50`.\n",
        "If you want faster results you can use a smaller number.-->\n",
        "\n",
        "\n",
        "æ¬¡ã®ã‚»ãƒ«ã¯å…ˆã»ã©ã¨åŒã˜ç¨®ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒï¼Œã‚¹ãƒ†ãƒƒãƒ—æ•°ã¯å°‘ãªããªã£ã¦ã„ã‚‹ã€‚\n",
        "é¦¬ã®é ­ã‚„ãƒ˜ãƒ«ãƒ¡ãƒƒãƒˆã®ã‚ˆã†ãªç´°éƒ¨ã¯ï¼Œå‰ã®ç”»åƒã‚ˆã‚Šã‚‚ãƒªã‚¢ãƒ«ã§ã¯ã£ãã‚Šã—ã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ï¼š\n",
        "<!--The following cell uses the same seed as before, but with fewer steps.\n",
        "Note how some details, such as the horse's head or the helmet, are less defin realistic and less defined than in the previous image: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKiK67iTXQkt"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "\n",
        "steps=50\n",
        "steps=15\n",
        "generator = torch.Generator(device).manual_seed(seed)\n",
        "image = pipe(prompt, num_inference_steps=steps, generator=generator).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8wxFjba5zRc"
      },
      "source": [
        "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ«ã®ã‚‚ã†1ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ `guidance_scale` ã§ã‚ã‚‹ã€‚\n",
        "ã“ã‚Œã¯ã€æ¡ä»¶ä¿¡å·ï¼ˆã“ã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã€ãŠã‚ˆã³å…¨ä½“çš„ãªã‚µãƒ³ãƒ—ãƒ«ã®å“è³ªã¸ã®æº–æ‹ ã‚’é«˜ã‚ã‚‹æ–¹æ³•ã§ã‚ã‚‹ã€‚\n",
        "ç°¡å˜ã«è¨€ã†ã¨ã€åˆ†é¡å™¨ã‚’ä½¿ã‚ãªã„ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¯ã€ã‚ˆã‚Šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒãƒƒãƒã™ã‚‹ã‚ˆã†ã«ç”Ÿæˆã‚’å¼·åˆ¶ã™ã‚‹ã€‚\n",
        "`7` ã‚„ `8.5` ã®ã‚ˆã†ãªæ•°å­—ã¯è‰¯ã„çµæœã‚’ã‚‚ãŸã‚‰ã™ãŒï¼Œéå¸¸ã«å¤§ããªæ•°å­—ã‚’ä½¿ç”¨ã—ãŸå ´åˆï¼Œç”»åƒã¯è‰¯ãè¦‹ãˆã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŒï¼Œå¤šæ§˜æ€§ã«æ¬ ã‘ã‚‹ã€‚\n",
        "<!-- The other parameter in the pipeline call is `guidance_scale`.\n",
        "It is a way to increase the adherence to the conditional signal which in this case is text as well as overall sample quality.\n",
        "In simple terms classifier free guidance forces the generation to better match with the prompt.\n",
        "Numbers like `7` or `8.5` give good results, if you use a very large number the images might look good, but will be less diverse. -->\n",
        "\n",
        "\n",
        "ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŠ€è¡“çš„ãªè©³ç´°ã«ã¤ã„ã¦ã¯ï¼Œã“ã®ãƒãƒ¼ãƒˆ ã®[æœ€çµ‚ç¯€](https://colab.research.google.com/drive/1ALXuCM5iNnJDNW5vqBm5lCtUQtZJHN2f?authuser=1#scrollTo=UZp-ynZLrS-S) ã‚’å‚ç…§ã€‚\n",
        "<!-- You can learn about the technical details of this parameter in [the last section](https://colab.research.google.com/drive/1ALXuCM5iNnJDNW5vqBm5lCtUQtZJHN2f?authuser=1#scrollTo=UZp-ynZLrS-S) of this notebook. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUbR3IszB1CD"
      },
      "source": [
        "åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦è¤‡æ•°ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡æ•°å›ç¹°ã‚Šè¿”ã—ãŸãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚\n",
        "å…ˆã»ã©ä½¿ã£ãŸæ–‡å­—åˆ—ã®ä»£ã‚ã‚Šã«ï¼Œã“ã®ãƒªã‚¹ãƒˆã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«é€ã‚‹ã€‚\n",
        "<!-- To generate multiple images for the same prompt, we simply use a list with the same prompt repeated several times.\n",
        "We'll send the list to the pipeline instead of the string we used before. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZcgsflpBoEM"
      },
      "source": [
        "ã¾ãšã¯ç”»åƒã®ã‚°ãƒªãƒƒãƒ‰ã‚’è¡¨ç¤ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’æ›¸ã„ã¦ã¿ã‚ˆã†ã€‚\n",
        "ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã€`image_grid`é–¢æ•°ãŒä½œæˆã•ã‚Œã‚‹ã€‚\n",
        "<!-- Let's first write a helper function to display a grid of images.\n",
        "Just run the following cell to create the `image_grid` function, or disclose the code if you are interested in how it's done. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "REF_yuHprSa1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcHccTDWbQRU"
      },
      "source": [
        "ã“ã‚Œã§ã€3 ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã™ã‚Œã°ã€ã‚°ãƒªãƒƒãƒ‰ç”»åƒã‚’ç”Ÿæˆã§ãã‚‹ã€‚\n",
        "<!-- Now, we can generate a grid image once having run the pipeline with a list of 3 prompts. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YAFLvWWrSdM"
      },
      "outputs": [],
      "source": [
        "num_images = 3\n",
        "prompt = [\"a photograph of an astronaut riding a horse\"] * num_images\n",
        "\n",
        "images = pipe(prompt).images\n",
        "\n",
        "grid = image_grid(images, rows=1, cols=3)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj-3lCCWYtMn"
      },
      "source": [
        "æ¬¡ã« `n Ã— m` ã®ç”»åƒã®ã‚°ãƒªãƒƒãƒ‰ã‚’ç”Ÿæˆã™ã‚‹æ–¹æ³•\n",
        "<!-- And here's how to generate a grid of `n Ã— m` images. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylscg48YYxfF"
      },
      "outputs": [],
      "source": [
        "num_cols = 3\n",
        "num_rows = 4\n",
        "\n",
        "prompt = [\"a photograph of an astronaut riding a horse\"] * num_cols\n",
        "\n",
        "all_images = []\n",
        "for i in range(num_rows):\n",
        "  images = pipe(prompt).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf9pbS3kCsUf"
      },
      "source": [
        "### æ­£æ–¹å½¢ä»¥å¤–ã®ç”»åƒã‚’ç”Ÿæˆ\n",
        "<!-- ### Generate non-square images-->\n",
        "\n",
        "Stable Diffusion ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ `512 Ã— 512` ç”»ç´ ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€‚\n",
        "ã—ã‹ã—ã€`height` ã¨ `width` å¼•æ•°ã‚’ä½¿ãˆã°ç°¡å˜ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãã§ãã‚‹ã®ã§ã€ç¸¦é•·ã‚„æ¨ªé•·ã®é•·æ–¹å½¢ã®ç”»åƒã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "<!-- Stable Diffusion produces images of `512 Ã— 512` pixels by default.\n",
        "But it's very easy to override the default using the `height` and `width` arguments, so you can create rectangular images in portrait or landscape ratios. -->\n",
        "\n",
        "ä»¥ä¸‹ã¯é©åˆ‡ãªç”»åƒã‚µã‚¤ã‚ºã‚’é¸ã¶ãŸã‚ã®æ¨å¥¨äº‹é …ï¼š\n",
        "- `height` ã¨ `width` ãŒä¸¡æ–¹ã¨ã‚‚ `8` ã®å€æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã›ã‚ˆ\n",
        "- 512 ã‚’ä¸‹å›ã‚‹ã¨ä½ç”»è³ªã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚\n",
        "- ä¸¡æ–¹å‘ã§ 512 ã‚’è¶…ãˆã‚‹ã¨ã€ç”»åƒé ˜åŸŸãŒç¹°ã‚Šè¿”ã•ã‚Œã¾ã™ï¼ˆå…¨ä½“çš„ãªä¸€è²«æ€§ãŒå¤±ã‚ã‚Œã‚‹ï¼‰ã€‚\n",
        "- æ­£æ–¹å½¢ã§ãªã„ç”»åƒã‚’ä½œæˆã™ã‚‹æœ€å–„ã®æ–¹æ³•ã¯ã€1 ã¤ã®æ¬¡å…ƒã§ `512` ã‚’ä½¿ç”¨ã—ã€ã‚‚ã† 1ã¤ ã®æ¬¡å…ƒã§ãã‚Œã‚ˆã‚Šã‚‚å¤§ããªå€¤ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚\n",
        "\n",
        "<!--These are some recommendations to choose good image sizes:\n",
        "- Make sure `height` and `width` are both multiples of `8`.\n",
        "- Going below 512 might result in lower quality images.\n",
        "- Going over 512 in both directions will repeat image areas (global coherence is lost).\n",
        "- The best way to create non-square images is to use `512` in one dimension, and a value larger than that in the other one. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SXnxd-ZrSfy"
      },
      "outputs": [],
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "\n",
        "image = pipe(prompt, height=512, width=768).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW14FA-tDQ5n"
      },
      "source": [
        "\n",
        "## 2. Stable Diffusion ã¨ã¯ä½•ã‹<!--2. What is Stable Diffusion-->\n",
        "\n",
        "Stable Diffusion ã®ç†è«–çš„ãªéƒ¨åˆ†ã«å…¥ã‚ã†ã€‚\n",
        "<!-- Now, let's go into the theoretical part of Stable Diffusion ğŸ‘©â€ğŸ“.-->\n",
        "\n",
        "Stable Diffusion ã¯ [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) ã§ææ¡ˆã•ã‚ŒãŸã€**Latent Diffusion** ã¨å‘¼ã°ã‚Œã‚‹ç‰¹æ®Šãªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ã„ã‚‹ã€‚\n",
        "<!--Stable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752). -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHj_sllMaKTD"
      },
      "source": [
        "ä¸€èˆ¬çš„ãªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãªæ­£è¦é›‘éŸ³ã‚’æ®µéšçš„ã« **é›‘éŸ³é™¤å»** ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã•ã‚ŒãŸæ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚Šã€**ç”»åƒ** ã®ã‚ˆã†ãªé–¢å¿ƒã®ã‚ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã«åˆ°é”ã™ã‚‹ã€‚\n",
        "æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã®ã‚ˆã‚Šè©³ç´°ãªæ¦‚è¦ã«ã¤ã„ã¦ã¯ã€[ã“ã®ç ”ç©¶å®¤](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) ã‚’å‚ç…§ã—ã¦ã»ã—ã„ã€‚\n",
        "<!-- General diffusion models are machine learning systems that are trained to *denoise* random gaussian noise step by step, to get to a sample of interest, such as an *image*.\n",
        "For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb).-->\n",
        "\n",
        "æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯ï¼Œç”»åƒãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã«ãŠã„ã¦æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚\n",
        "ã—ã‹ã—æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æ¬ ç‚¹ã¯ï¼Œé€†é›‘éŸ³å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚\n",
        "ã•ã‚‰ã«ï¼Œã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ç”»ç´ ç©ºé–“ã§å‹•ä½œã™ã‚‹ãŸã‚å¤šãã®ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã—ï¼Œé«˜è§£åƒåº¦ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹å ´åˆã«ã¯ä¸å½“ã«é«˜ä¾¡ã«ãªã‚‹ã€‚\n",
        "ã—ãŸãŒã£ã¦ï¼Œã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ï¼Œæ¨è«–ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯å›°é›£ã§ã‚ã‚‹ã€‚\n",
        "<!-- Diffusion models have shown to achieve state-of-the-art results for generating image data.\n",
        "But one downside of diffusion models is that the reverse denoising process is slow.\n",
        "In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images.\n",
        "Therefore, it is challenging to train these models and also use them for inference. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsdAj9pDPOv"
      },
      "source": [
        "<br>\n",
        "\n",
        "æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯ã€å®Ÿéš›ã®ç”»ç´ ç©ºé–“ã‚’ä½¿ç”¨ã™ã‚‹ä»£ã‚ã‚Šã«ã€ä½æ¬¡å…ƒã® __æ½œåœ¨__ç©ºé–“ä¸Šã§æ‹¡æ•£å‡¦ç†ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šï¼Œãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ã®è¤‡é›‘ã•ã‚’è»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "ã“ã‚ŒãŒæ¨™æº–çš„ãªæ‹¡æ•£ã¨æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ä¸»ãªé•ã„ã§ã™ï¼š **æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«** ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯ç”»åƒã®æ½œåœ¨çš„ãªï¼ˆåœ§ç¸®ã•ã‚ŒãŸï¼‰è¡¨ç¾ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã‚Œã‚‹ã€‚\n",
        "<!-- Latent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional _latent_ space, instead of using the actual pixel space.\n",
        "This is the key difference between standard diffusion and latent diffusion models: **in latent diffusion the model is trained to generate latent (compressed) representations of the images.** -->\n",
        "\n",
        " æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ã¯ 3 ã¤ã®ä¸»è¦ãªæˆåˆ†ãŒã‚ã‚‹ã€‚\n",
        "<!--There are three main components in latent diffusion. -->\n",
        "\n",
        "1. è‡ªå·±ç¬¦å·åŒ–å™¨ (VAE)\n",
        "2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).\n",
        "3. ãƒ†ã‚­ã‚¹ãƒˆç¬¦å·åŒ–å™¨ï¼Œ[CLIP's ãƒ†ã‚­ã‚¹ãƒˆç¬¦å·åŒ–å™¨](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).\n",
        "\n",
        "<!-- 1. An autoencoder (VAE).\n",
        "2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).\n",
        "3. A text-encoder, *e.g.* [CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4leRMZzjTsA"
      },
      "source": [
        "**1. è‡ªå·±ç¬¦å·åŒ–å™¨ VAE**\n",
        "<!-- **1. The autoencoder (VAE)**-->\n",
        "\n",
        "VAE ãƒ¢ãƒ‡ãƒ«ã«ã¯ï¼Œç¬¦å·åŒ–å™¨ã¨å¾©å·åŒ–å™¨ã® 2 ã¤ã®éƒ¨åˆ†ãŒã‚ã‚‹ã€‚\n",
        "å¾©å·åŒ–å™¨ã¯ç”»åƒã‚’ä½æ¬¡å…ƒã®æ½œåœ¨è¡¨ç¾ã«å¤‰æ›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œï¼Œã“ã‚Œã¯ **U-Net** ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¨ãªã‚‹ã€‚\n",
        "å¾©å·åŒ–å™¨ã¯é€†ã«ï¼Œæ½œåœ¨è¡¨ç¾ã‚’ç”»åƒã«å¤‰æ›ã™ã‚‹ã€‚\n",
        "<!-- The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the *U-Net* model.\n",
        "The decoder, conversely, transforms the latent representation back into an image. -->\n",
        "\n",
        "æ½œåƒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ã€Œè¨“ç·´ã€ã®é–“ï¼Œç¬¦å·åŒ–å™¨ã¯é †æ–¹å‘æ‹¡æ•£å‡¦ç†ã®ãŸã‚ã«ç”»åƒã®æ½œåƒè¡¨ç¾ (_latents_) ã‚’å¾—ã‚‹ãŸã‚ã«ä½¿ã‚ã‚Œï¼Œå„æ®µéšã§ã‚ˆã‚Šå¤šãã®é›‘éŸ³ãŒé©ç”¨ã•ã‚Œã‚‹ã€‚\n",
        "æ¨è«–ä¸­ã¯ï¼Œé€†æ‹¡æ•£å‡¦ç†ã§ç”Ÿæˆã•ã‚ŒãŸé›‘éŸ³é™¤å»ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•°ã¯ï¼ŒVAE å¾©å·åŒ–å™¨ã‚’ä½¿ã£ã¦ç”»åƒã«å¤‰æ›ã•ã‚Œã‚‹ã€‚\n",
        "å¾Œè¿°ã™ã‚‹ã‚ˆã†ã«ï¼Œæ¨è«–ä¸­ã«å¿…è¦ãªã®ã¯ **VAE å¾©å·åŒ–å™¨** ã ã‘ã§ã‚ã‚‹ã€‚\n",
        "<!--During latent diffusion _training_, the encoder is used to get the latent representations (_latents_) of the images for the forward diffusion process, which applies more and more noise at each step. During _inference_, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. As we will see during inference we **only need the VAE decoder**. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr5ZCb66kmyE"
      },
      "source": [
        "**2. Uãƒãƒƒãƒˆ**<!-- **2. The U-Net**-->\n",
        "\n",
        "U-Net ã¯ ResNet ãƒ–ãƒ­ãƒƒã‚¯ã§æ§‹æˆã•ã‚ŒãŸç¬¦å·åŒ–å™¨éƒ¨ã¨å¾©å·åŒ–å™¨ãƒ€éƒ¨ã‚’æŒã¤ã€‚\n",
        "ç¬¦å·åŒ–å™¨ã¯ç”»åƒè¡¨ç¾ã‚’ä½è§£åƒåº¦ã®ç”»åƒè¡¨ç¾ã«åœ§ç¸®ã—ï¼Œå¾©å·åŒ–å™¨ã¯ä½è§£åƒåº¦ã®ç”»åƒè¡¨ç¾ã‚’é›‘éŸ³ã®å°‘ãªã„å…ƒã®é«˜è§£åƒåº¦ã®ç”»åƒè¡¨ç¾ã«å¾©å·åŒ–ã™ã‚‹ã€‚\n",
        "ã‚ˆã‚Šå…·ä½“çš„ã«ã¯ï¼ŒU-Net ã®å‡ºåŠ›ã¯é›‘éŸ³æ®‹å·®ã‚’äºˆæ¸¬ã—ï¼Œã“ã‚Œã‚’ç”¨ã„ã¦äºˆæ¸¬ã•ã‚ŒãŸé›‘éŸ³é™¤å»ç”»åƒè¡¨ç¾ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "<!-- The U-Net has an encoder part and a decoder part both comprised of ResNet blocks.\n",
        "The encoder compresses an image representation into a lower resolution image representation and the decoder decodes the lower resolution image representation back to the original higher resolution image representation that is supposedly less noisy.\n",
        "More specifically, the U-Net output predicts the noise residual which can be used to compute the predicted denoised image representation. -->\n",
        "\n",
        "U-Net ãŒãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸­ã«é‡è¦ãªæƒ…å ±ã‚’å¤±ã†ã®ã‚’é˜²ããŸã‚ï¼Œé€šå¸¸ï¼Œç¬¦å·åŒ–å™¨ã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ResNets ã¨å¾©å·åŒ–å™¨ã®ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ResNets ã®é–“ã«ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆæ¥ç¶šãŒè¿½åŠ ã•ã‚Œã‚‹ã€‚\n",
        "ã•ã‚‰ã«ï¼ŒStable diffusion U-Net ã¯äº¤å·®æ³¨æ„å±¤ã‚’ä»‹ã—ã¦ï¼Œãã®å‡ºåŠ›ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã«æ¡ä»¶ä»˜ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "äº¤å·®æ³¨æ„å±¤ã¯ U-Net ç¬¦å·åŒ–å™¨éƒ¨ã¨å¾©å·åŒ–å™¨éƒ¨ã®ä¸¡æ–¹ã«è¿½åŠ ã•ã‚Œï¼Œé€šå¸¸ã¯ ResNet ãƒ–ãƒ­ãƒƒã‚¯ã®é–“ã«é…ç½®ã•ã‚Œã‚‹ã€‚\n",
        "<!--To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder.\n",
        "Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers.\n",
        "The cross-attention layers are added to both the encoder and decoder part of the U-Net usually between ResNet blocks. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE7hhg5ArUu4"
      },
      "source": [
        "**3. ãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**\n",
        "<!-- **3. The Text-encoder**-->\n",
        "\n",
        "ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€å…¥åŠ›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ä¾‹ãˆã°*ã€ŒAn astronout riding a horseã€ï¼ˆé¦¬ã«ä¹—ã£ãŸå®‡å®™é£›è¡Œå£«ï¼‰ã‚’ã€U-NetãŒç†è§£ã§ãã‚‹åŸ‹ã‚è¾¼ã¿ç©ºé–“ã«å¤‰æ›ã™ã‚‹å½¹å‰²ã‚’æ‹…ã£ã¦ã„ã¾ã™ã€‚\n",
        "ãã‚Œã¯é€šå¸¸ã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ½œåœ¨çš„ãªãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹å˜ç´”ãª*å¤‰æ›å™¨ãƒ™ãƒ¼ã‚¹*ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã™ã€‚\n",
        "<!-- The text-encoder is responsible for transforming the input prompt, *e.g.* \"An astronout riding a horse\" into an embedding space that can be understood by the U-Net.\n",
        "It is usually a simple *transformer-based* encoder that maps a sequence of input tokens to a sequence of latent text-embeddings. -->\n",
        "\n",
        "\n",
        "[Imagen](https://imagen.research.google/) ã«è§¦ç™ºã•ã‚ŒãŸ Stable Diffusion ãƒ¢ãƒ‡ãƒ«ã¯ï¼Œå­¦ç¿’ä¸­ã«ãƒ†ã‚­ã‚¹ãƒˆç¬¦å·åŒ–å™¨ã®å­¦ç¿’ã‚’è¡Œã‚ãšï¼Œå˜ã« CLIP ã®å­¦ç¿’æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆç¬¦å·åŒ–å™¨ã§ã‚ã‚‹ [CLIP ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel) ã‚’ä½¿ç”¨ã™ã‚‹ã€‚\n",
        "<!--Inspired by [Imagen](https://imagen.research.google/), Stable Diffusion does **not** train the text-encoder during training and simply uses an CLIP's already trained text encoder, [CLIPTextModel](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-XnKTVfj2Jm"
      },
      "source": [
        "**ãªãœæ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯é«˜é€Ÿã§åŠ¹ç‡çš„ãªã®ã‹ï¼Ÿ**\n",
        "<!-- **Why is latent diffusion fast and efficient?**-->\n",
        "\n",
        "æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã® U-Net ã¯ä½æ¬¡å…ƒç©ºé–“ã§å‹•ä½œã™ã‚‹ãŸã‚ï¼Œç”»ç´ ç©ºé–“ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ï¼Œãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—è¦ä»¶ãŒå¤§å¹…ã«å‰Šæ¸›ã•ã‚Œã‚‹ã€‚\n",
        "ä¾‹ãˆã° Stable Diffusion ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹è‡ªå·±ç¬¦å·åŒ–å™¨ã¯ï¼Œå‰Šæ¸›ä¿‚æ•° reduction factor 8ã‚’æŒã£ã¦ã„ã‚‹ã€‚\n",
        "ã¤ã¾ã‚Š `(3, 512, 512)` ã®å½¢çŠ¶ã®ç”»åƒã¯ï¼Œæ½œåœ¨ç©ºé–“ã§ã¯ `(3, 64, 64)` ã¨ãªã‚Šï¼Œå¿…è¦ãªãƒ¡ãƒ¢ãƒªã¯ `8 Ã— 8 = 64` åˆ†ã® 1 ã«ãªã‚‹ã€‚\n",
        "<!-- Since the U-Net of latent diffusion models operates on a low dimensional space, it greatly reduces the memory and compute requirements compared to pixel-space diffusion models.\n",
        "For example, the autoencoder used in Stable Diffusion has a reduction factor of 8.\n",
        "This means that an image of shape `(3, 512, 512)` becomes `(3, 64, 64)` in latent space, which requires `8 Ã— 8 = 64` times less memory. -->\n",
        "\n",
        "ã“ã‚ŒãŒ 16 GBã® Colab GPU ã§ã‚‚`512 Ã— 512` ã®ç”»åƒã‚’é«˜é€Ÿã«ç”Ÿæˆã§ãã‚‹ç†ç”±ã§ã‚ã‚‹ï¼\n",
        "<!--This is why it's possible to generate `512 Ã— 512` images so quickly, even on 16GB Colab GPUs! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz5Ge_47jUaA"
      },
      "source": [
        "**æ¨è«–ä¸­ã®å®‰å®šæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«**\n",
        "<!-- **Stable Diffusion during inference**-->\n",
        "\n",
        "ã§ã¯ã€è«–ç†çš„ãªæµã‚Œã‚’èª¬æ˜ã—ãªãŒã‚‰ã€æ¨è«–ã«ãŠã„ã¦ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’è©³ã—ãè¦‹ã¦ã„ã“ã†ã€‚\n",
        "<!--Putting it all together, let's now take a closer look at how the model works in inference by illustrating the logical flow. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUBqX1sMsDR6"
      },
      "source": [
        "<p align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"500\"/>\n",
        "</p>\n",
        "\n",
        "Stable Diffusion ãƒ¢ãƒ‡ãƒ«ã¯æ½œåœ¨ç¨®ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã¨ã™ã‚‹ã€‚\n",
        "æ½œåœ¨ç¨®ã¯ï¼Œã‚µã‚¤ã‚º $64\\times 64$ ã®ãƒ©ãƒ³ãƒ€ãƒ ãªæ½œåœ¨çš„ãªç”»åƒè¡¨ç¾ã‚’ç”Ÿæˆã™ã‚‹ã®ã«ä½¿ã‚ã‚Œã€ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯CLIP ã®ãƒ†ã‚­ã‚¹ãƒˆç¬¦å·åŒ–å™¨ã‚’ä»‹ã—ã¦ã‚µã‚¤ã‚º $77\\times768$ ã®ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿è¡¨ç¾ã«å¤‰æ›ã•ã‚Œã‚‹ã€‚\n",
        "<!-- The stable diffusion model takes both a latent seed and a text prompt as an input.\n",
        "The latent seed is then used to generate random latent image representations of size $64 \\times 64$ where as the text prompt is transformed to text embeddings of size $77 \\times 768$ via CLIP's text encoder.-->\n",
        "\n",
        "\n",
        "æ¬¡ã«ï¼ŒU-Net  ã¯ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’æ¡ä»¶ã¨ã—ãªãŒã‚‰ï¼Œãƒ©ãƒ³ãƒ€ãƒ ãªæ½œåœ¨ç”»åƒè¡¨ç¾ã‚’ç¹°ã‚Šè¿”ã— **é›‘éŸ³é™¤å»** ã™ã‚‹ã€‚\n",
        "é›‘éŸ³æ®‹å·®ã§ã‚ã‚‹ U-Net ã®å‡ºåŠ›ã¯ï¼Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä»‹ã—ã¦é›‘éŸ³é™¤å»ã•ã‚ŒãŸæ½œåœ¨ç”»åƒè¡¨ç¾ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ä½¿ã‚ã‚Œã‚‹ã€‚\n",
        "ã“ã®è¨ˆç®—ã«ã¯å¤šãã®ç•°ãªã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã‚‹ã“ã¨ãŒã§ãï¼Œãã‚Œãã‚Œã«é•·æ‰€ã¨çŸ­æ‰€ãŒã‚ã‚‹ã€‚\n",
        "Stable Diffusion ã«ã¤ã„ã¦ã¯ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã™ã‚‹ï¼š\n",
        "<!--Next the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings.\n",
        "The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm.\n",
        "Many different scheduler algorithms can be used for this computation, each having its pros and cons.\n",
        "For Stable Diffusion, we recommend using one of: -->\n",
        "\n",
        "- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py) (used by default).\n",
        "- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py).\n",
        "- [Heun Discrete scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_heun_discrete.py).\n",
        "- [DPM Solver Multistep scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py).\n",
        "ã“ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ã€å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã§ç´ æ™´ã‚‰ã—ã„å“è³ªã‚’é”æˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® 50 ã§ã¯ãªãã€25 ã§è©¦ã—ã¦ã¿ã¦æ¬²ã—ã„ï¼\n",
        "<!-- This scheduler is able to achieve great quality in less steps.\n",
        "You can try with 25 instead of the default 50! -->\n",
        "\n",
        "\n",
        "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã«ã¤ã„ã¦ã®ç†è«–ã¯ï¼Œã“ã“ã§ã¯æ‰±ã‚ãªã„ãŒï¼Œè¦ã™ã‚‹ã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ï¼Œä»¥å‰ã®é›‘éŸ³è¡¨ç¾ã¨äºˆæ¸¬ã•ã‚ŒãŸé›‘éŸ³æ®‹å·®ã‹ã‚‰ï¼Œäºˆæ¸¬ã•ã‚ŒãŸé›‘éŸ³é™¤å»ç”»åƒè¡¨ç¾ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã‚’è¦šãˆã¦ãŠãå¿…è¦ãŒã‚ã‚‹ã€‚\n",
        "è©³ã—ãã¯ [æ‹¡æ•£ã«åŸºã¥ãç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆç©ºé–“ã®è§£æ˜](https://arxiv.org/abs/2206.00364) ã‚’å‚ç…§ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã™ã‚‹ã€‚\n",
        "<!-- Theory on how the scheduler algorithm function is out of scope for this notebook, but in short one should remember that they compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n",
        "For more information, we recommend looking into [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)-->\n",
        "\n",
        "\n",
        "ã“ã®  **é›‘éŸ³é™¤å»** å‡¦ç†ã¯ï¼Œã‚ˆã‚Šè‰¯ã„æ½œåƒè¡¨ç¾ã‚’æ®µéšçš„ã«å–ã‚Šå‡ºã™ãŸã‚ã« **ç´„50å›** ç¹°ã‚Šè¿”ã•ã‚Œã‚‹ã€‚\n",
        "ä¸€æ—¦å®Œäº†ã™ã‚‹ã¨ï¼Œæ½œåœ¨ç”»åƒè¡¨ç¾ã¯å¤‰åˆ†è‡ªå·±ç¬¦å·åŒ–å™¨ã®å¾©å·åŒ–å™¨éƒ¨ã«ã‚ˆã£ã¦å¾©å·åŒ–ã•ã‚Œã‚‹ã€‚\n",
        "<!--The *denoising* process is repeated *ca.* 50 times to step-by-step retrieve better latent image representations.\n",
        "Once complete, the latent image representation is decoded by the decoder part of the variational auto encoder. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rR2Udg5IYjn"
      },
      "source": [
        "\n",
        "æ½œåœ¨å¤‰æ•°ã¨ stable diffusion ã«ã¤ã„ã¦ç°¡å˜ã«ç´¹ä»‹ã—ãŸå¾Œ Huggingface ã® diffusers ã®é«˜åº¦ãªä½¿ã„æ–¹ã‚’è¦‹ã¦ã¿ã‚ˆã†ï¼\n",
        "<!-- After this brief introduction to Latent and Stable Diffusion, let's see how to make advanced use of ğŸ¤— Hugging Face Diffusers! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZp-ynZLrS-S"
      },
      "source": [
        "## 3. `diffusers` ã‚’ä½¿ã£ã¦ç‹¬è‡ªã®æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ›¸ãæ–¹æ³•<!-- ## 3. How to write your own inference pipeline with `diffusers`-->\n",
        "\n",
        "æœ€å¾Œã«ã€`diffusers` ã‚’ä½¿ã£ã¦ã‚«ã‚¹ã‚¿ãƒ ã®æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã™ã‚‹ã€‚\n",
        "ã“ã‚Œã¯ç³»ã®ç‰¹å®šã®æ©Ÿèƒ½ã‚’ã‚‚ã†å°‘ã—æ·±ãæ˜ã‚Šä¸‹ã’ãŸã‚Šï¼Œç‰¹å®šã®æˆåˆ†ã‚’åˆ‡ã‚Šæ›¿ãˆãŸã‚Šã™ã‚‹ã®ã«éå¸¸ã«ä¾¿åˆ©ã§ã‚ã‚‹ã€‚\n",
        "<!-- Finally, we show how you can create custom diffusion pipelines with `diffusers`.\n",
        "This is often very useful to dig a bit deeper into certain functionalities of the system and to potentially switch out certain components. -->\n",
        "\n",
        "æœ¬ç¯€ã§ã¯ã€ç•°ãªã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€ã™ãªã‚ã¡ [ã“ã®PR](https://github.com/huggingface/diffusers/pull/185#pullrequestreview-1074247365)ã§è¿½åŠ ã•ã‚ŒãŸ[Katherine Crowson](https://github.com/crowsonkb) ã® K-LMS ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§ Stable Diffusion ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã™ã€‚\n",
        "<!--In this section, we will demonstrate how to use Stable Diffusion with a different scheduler, namely [Katherine Crowson's](https://github.com/crowsonkb) K-LMS scheduler that was added in [this PR](https://github.com/huggingface/diffusers/pull/185#pullrequestreview-1074247365). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEXkmX3vDPRU"
      },
      "source": [
        "`StableDiffusionPipeline` ã‚’ãã‚Œãã‚Œè¦‹ã¦ã„ã“ã†ã€‚\n",
        "<!-- Let's go through the `StableDiffusionPipeline` step by step to see how we could have written it ourselves.-->\n",
        "\n",
        "ã¾ãšï¼Œãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã‚ˆã†ã€‚\n",
        "<!--We will start by loading the individual models involved. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Xw7qSEdTpt"
      },
      "source": [
        "[è¨“ç·´æ¸ˆãƒ¢ãƒ‡ãƒ«](https://huggingface.co/CompVis/stable-diffusion-v1-3-diffusers/tree/main) ã«ã¯ï¼Œå®Œå…¨ãª stable  diffusion ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã™ã¹ã¦ã®æˆåˆ†ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚\n",
        "ã“ã‚Œã‚‰ã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹ï¼š\n",
        "- `text_encoder`  Stable Diffusion ã¯ CLIP ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€ä»–ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã¯ `BERT` ã®ã‚ˆã†ãªä»–ã®ç¬¦å·åŒ–å™¨ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚\n",
        "- `tokenizer`:   `text_encoder` ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¨ä¸€è‡´ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„\n",
        "-  `schduler `ï¼š è¨“ç·´ä¸­ã«ç”»åƒã«é›‘éŸ³ã‚’å¾ã€…ã«è¿½åŠ ã™ã‚‹ãŸã‚ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ \n",
        "-` unet`ï¼š å…¥åŠ›ã®æ½œåœ¨è¡¨ç¾ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«\n",
        "- `vae`ï¼š æ½œåœ¨è¡¨ç¾ã‚’å®Ÿç”»åƒã«å¾©å·åŒ–ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹è‡ªå·±ç¬¦å·åŒ–å™¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
        "\n",
        "`from_pretrained` ã®å¼•æ•°  `subfolder` ã‚’ä½¿ã£ã¦ï¼Œæˆåˆ†ã‚’ä¿å­˜ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã‚’å‚ç…§ã™ã‚‹ã“ã¨ã§ï¼Œãã“ã«ã‚ã‚‹æˆåˆ†ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "\n",
        "<!-- The [pre-trained model](https://huggingface.co/CompVis/stable-diffusion-v1-3-diffusers/tree/main) includes all the components required to setup a complete diffusion pipeline. They are stored in the following folders:\n",
        "- `text_encoder`: Stable Diffusion uses CLIP, but other diffusion models may use other encoders such as `BERT`.\n",
        "- `tokenizer`. It must match the one used by the `text_encoder` model.\n",
        "- `scheduler`: The scheduling algorithm used to progressively add noise to the image during training.\n",
        "- `unet`: The model used to generate the latent representation of the input.\n",
        "- `vae`: Autoencoder module that we'll use to decode latent representations into real images.\n",
        "\n",
        "We can load the components by referring to the folder they were saved, using the `subfolder` argument to `from_pretrained`. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlsKwQijWMpL"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "# 1. Load the autoencoder model which will be used to decode the latents into image space.\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
        "\n",
        "# 2. Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# 3. The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eYstOqwVYGc"
      },
      "source": [
        "ã“ã“ã§ã¯ï¼Œå®šç¾©æ¸ˆã¿ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ä»£ã‚ã‚Šã« K-LMS ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ã†ã“ã¨ã«ã™ã‚‹ã€‚\n",
        "<!-- Now instead of loading the pre-defined scheduler, we'll use the K-LMS scheduler instead. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-0W8UG6VXpD"
      },
      "outputs": [],
      "source": [
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "scheduler = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBXqwuHFYgf4"
      },
      "source": [
        "æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»ã™ã€‚\n",
        "<!-- Next we move the models to the GPU. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3lJEXz7YgnC"
      },
      "outputs": [],
      "source": [
        "vae = vae.to(device)\n",
        "text_encoder = text_encoder.to(device)\n",
        "unet = unet.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtqGYl5SY6dm"
      },
      "source": [
        "ã“ã“ã§ï¼Œç”»åƒç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã™ã‚‹ã€‚\n",
        "<!-- We now define the parameters we'll use to generate images.-->\n",
        "\n",
        "`guidance_scale` ã¯ [Imagen è«–æ–‡](https://arxiv.org/pdf/2205.11487.pdf) ã®å¼(2) ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã®é‡ã¿ `w` ã«é¡ä¼¼ã—ã¦å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã€‚\n",
        "`guidance_scale == 1` ã¯åˆ†é¡å™¨ã‚’ä½¿ã‚ãªã„ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’è¡Œã†ã“ã¨ã«ç›¸å½“ã™ã‚‹ã€‚ã“ã“ã§ã¯7.5ã«è¨­å®šã™ã‚‹ã€‚\n",
        "<!-- Note that `guidance_scale` is defined analog to the guidance weight `w` of equation (2) in the [Imagen paper](https://arxiv.org/pdf/2205.11487.pdf). `guidance_scale == 1` corresponds to doing no classifier-free guidance. Here we set it to 7.5 as also done previously. -->\n",
        "\n",
        "å‰ä¾‹ã¨ã¯å¯¾ç…§çš„ã«ï¼Œã‚ˆã‚Šæ˜ç¢ºãªç”»åƒã‚’å¾—ã‚‹ãŸã‚ã« `num_inference_steps` ã‚’ 100 ã«è¨­å®šã™ã‚‹ã€‚\n",
        "<!--In contrast to the previous examples, we set `num_inference_steps` to 100 to get an even more defined image. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot8RDV-2Y6uE"
      },
      "outputs": [],
      "source": [
        "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
        "\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 512                         # default width of Stable Diffusion\n",
        "\n",
        "num_inference_steps = 100            # Number of denoising steps\n",
        "\n",
        "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
        "\n",
        "generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
        "\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G47gEbg9Z4sJ"
      },
      "source": [
        "ã¾ãšï¼Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã® text_embeddings ã‚’å¾—ã‚‹ã€‚\n",
        "ã“ã‚Œã‚‰ã®åŸ‹ã‚è¾¼ã¿ã¯ UNet ãƒ¢ãƒ‡ãƒ«ã®æ¡ä»¶ã«ä½¿ã‚ã‚Œã‚‹ã€‚\n",
        "<!-- First, we get the text_embeddings for the prompt.\n",
        "These embeddings will be used to condition the UNet model. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZpvyVT1Y6wq"
      },
      "outputs": [],
      "source": [
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(device))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INGdc9eFaeWz"
      },
      "source": [
        "ã“ã‚Œã¯ åŸ‹è‰ (ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°)ãƒˆãƒ¼ã‚¯ãƒ³ (ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆ) ã®åŸ‹ã‚è¾¼ã¿ã§ã‚ã‚‹ã€‚\n",
        "ã“ã‚Œã‚‰ã¯æ¡ä»¶ä»˜ãã® `text_embeddings` (`batch_size` ã¨ `seq_length`) ã¨åŒã˜æ¬¡å…ƒã‚’æŒã¤å¿…è¦ãŒã‚ã‚‹ã€‚\n",
        "<!-- We'll also get the unconditional text embeddings for classifier-free guidance, which are just the embeddings for the padding token (empty text).\n",
        "They need to have the same shape as the conditional `text_embeddings` (`batch_size` and `seq_length`) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkuwhbFrY6zo"
      },
      "outputs": [],
      "source": [
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lKMrvoYbxzf"
      },
      "source": [
        "åˆ†é¡å™¨ã‚’ä½¿ã‚ãªã„ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã§ã¯ï¼Œ2 ã¤ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’è¡Œã†å¿…è¦ãŒã‚ã‚‹ã€‚\n",
        "ä¸€ã¤ã¯æ¡ä»¶ä»˜ãå…¥åŠ›ï¼ˆ`text_embeddings`ï¼‰ã§ã€ã‚‚ã†ä¸€ã¤ã¯ç„¡æ¡ä»¶åŸ‹ã‚è¾¼ã¿ï¼ˆ`uncond_embeddings`ï¼‰ã§ã‚ã‚‹ã€‚\n",
        "å®Ÿéš›ã«ã¯ã€2 ã¤ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å›é¿ã™ã‚‹ãŸã‚ã«ã€ä¸¡æ–¹ã‚’ 1 ã¤ã®ãƒãƒƒãƒã«é€£çµã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "<!-- For classifier-free guidance, we need to do two forward passes.\n",
        "One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwnB7CIeY619"
      },
      "outputs": [],
      "source": [
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxcaMgD0DPUD"
      },
      "source": [
        "åˆæœŸãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚ºã‚’ç”Ÿæˆã™ã‚‹ã€‚\n",
        "<!-- Generate the intial random noise. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NsfjxA-chAL"
      },
      "outputs": [],
      "source": [
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC401krZfXOr"
      },
      "outputs": [],
      "source": [
        "latents.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDUOA1gHMp2Y"
      },
      "source": [
        "ã‚¯ãƒ¼ãƒ«ãª $64 \\times 64$ ãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚\n",
        "ãƒ¢ãƒ‡ãƒ«ã¯ï¼Œã“ã®æ½œåœ¨è¡¨ç¾ (ç´”ç²‹ãªãƒã‚¤ã‚º) ã‚’å¾Œã§ `512 Ã— 512` ç”»åƒã«å¤‰æ›ã™ã‚‹ã€‚\n",
        "<!-- Cool $64 \\times 64$ is expected. The model will transform this latent representation (pure noise) into a `512 Ã— 512` image later on.-->\n",
        "\n",
        "æ¬¡ã«ã€é¸æŠã—ãŸ `num_inference_steps` ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚\n",
        "\n",
        "ã“ã‚Œã«ã‚ˆã‚Šï¼Œãƒã‚¤ã‚ºé™¤å»å‡¦ç†ä¸­ã«ä½¿ç”¨ã•ã‚Œã‚‹ `sigmas` ã¨æ­£ç¢ºãªæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—å€¤ãŒè¨ˆç®—ã•ã‚Œã‚‹ã€‚\n",
        "<!--Next, we initialize the scheduler with our chosen `num_inference_steps`.\n",
        "This will compute the `sigmas` and exact time step values to be used during the denoising process. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6UDqCyKwBpx"
      },
      "outputs": [],
      "source": [
        "scheduler.set_timesteps(num_inference_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTOxOKeqW4XE"
      },
      "source": [
        "K-LMS ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ã€`latents` ã« `sigma` å€¤ã‚’æ›ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n",
        "ã“ã“ã§ã“ã‚Œã‚’è¡Œã†ã€‚\n",
        "<!-- The K-LMS scheduler needs to multiply the `latents` by its `sigma` values. Let's do this here -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTwTq9d-W_NP"
      },
      "outputs": [],
      "source": [
        "latents = latents * scheduler.init_noise_sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdVkvYuYdjc6"
      },
      "source": [
        "ã“ã‚Œã§ãƒã‚¤ã‚ºé™¤å»ãƒ«ãƒ¼ãƒ—ã‚’æ›¸ãæº–å‚™ãŒã§ããŸã€‚\n",
        "<!-- We are ready to write the denoising loop. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylc3AIdZkFhl"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "\n",
        "for t in tqdm(scheduler.timesteps):\n",
        "  # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "  latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "  # predict the noise residual\n",
        "  with torch.no_grad():\n",
        "    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "  # perform guidance\n",
        "  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "  # compute the previous noisy sample x_t -> x_t-1\n",
        "  latents = scheduler.step(noise_pred, t, latents).prev_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZUTKjm0kuDY"
      },
      "source": [
        "æ¬¡ã«ï¼Œ`vae` ã‚’ä½¿ã£ã¦ï¼Œç”Ÿæˆã•ã‚ŒãŸ `latents` ã‚’ç”»åƒã«å¾©å·åŒ–ã™ã‚‹ã€‚\n",
        "<!-- We now use the `vae` to decode the generated `latents` back into the image. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YRzuJP7kMZo"
      },
      "outputs": [],
      "source": [
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "  image = vae.decode(latents).sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmdOa4_Dqrl8"
      },
      "source": [
        "ãã—ã¦æœ€å¾Œã«ï¼Œç”»åƒã‚’ PIL ã«å¤‰æ›ã—ã¦è¡¨ç¤ºã¾ãŸã¯ä¿å­˜ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚\n",
        "<!-- And finally, let's convert the image to PIL so we can display or save it. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAVZStIokTVv"
      },
      "outputs": [],
      "source": [
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjYuICyxwpeO"
      },
      "source": [
        "ã“ã‚Œã§ï¼Œç‹¬è‡ªã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸã‚Šï¼Œdiffuser æˆåˆ†ã‚’å¥½ããªã‚ˆã†ã«ä½¿ç”¨ã—ãŸã‚Šã™ã‚‹ãŸã‚ã®ã™ã¹ã¦ã®ãƒ‘ãƒ¼ãƒ„ãŒæƒã£ãŸã€‚\n",
        "<!-- Now you have all the pieces to build your own pipelines or use diffusers components as you like ğŸ”¥. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwwaTOxRuzWj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}