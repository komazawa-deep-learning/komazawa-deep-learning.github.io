{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2025_0110stable_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-vX3cavOCt"
      },
      "source": [
        "# diffuser を用いた **[Stable Diffusion](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=2rR2Udg5IYjn)**\n",
        "\n",
        "Stable Diffusion は [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), [LAION](https://laion.ai/) の研究者とエンジニアによって作成されたテキストから画像へ変換する潜在拡散モデルである。\n",
        "これは [LAION-5B](https://laion.ai/blog/laion-5b/) データベースのサブセットから 512x512 画像で学習される。\n",
        "このモデルは，凍結された CLIP ViT-L/14 テキスト符号化器を使い，テキストプロンプトを条件としている。\n",
        "860M の U-Net と 123M のテキスト符号化器を持つこのモデルは，比較的軽量で，多くの民生用 GPU で実行可能である。\n",
        "詳しくは [モデルカード](https://huggingface.co/CompVis/stable-diffusion) を参照。\n",
        "<!-- Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/).\n",
        "It's trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database.\n",
        "This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts.\n",
        "With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on many consumer GPUs.\n",
        "See the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information. -->\n",
        "\n",
        "\n",
        "本 Colab ノートブックでは Hugging Face [Diffusers ライブラリ](https://github.com/huggingface/diffusers) を使った Stable Diffusion の使い方を紹介する。\n",
        "<!-- This Colab notebook shows how to use Stable Diffusion with the 🤗 Hugging Face [🧨 Diffusers library](https://github.com/huggingface/diffusers).\n",
        "\n",
        "さっそく始めましょう！<!--Let's get started! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xMJ6LaET6dT"
      },
      "source": [
        "## 1. `StableDiffusionPipline` の使い方\n",
        "<!-- ## 1. How to use `StableDiffusionPipeline` -->\n",
        "\n",
        "Stable Diffusion がどのように機能するか理論的な側面に飛び込む前に，少し試してみよう。\n",
        "<!-- Before diving into the theoretical aspects of how Stable Diffusion functions, let's try it out a bit 🤗.-->\n",
        "\n",
        "本節では，わずか数行のコードでテキストから画像への推論を実行する方法を紹介する！\n",
        "<!--In this section, we show how you can run text to image inference in just a few lines of code! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOlvQ1nQL7c"
      },
      "source": [
        "### セットアップ\n",
        "<!-- ### Setup-->\n",
        "\n",
        "最初に、このノートブックを実行するために GPU ランタイムを使用していることを確認せよ。\n",
        "以下のコマンドが失敗する場合は，上の  `Runtime`  メニューから `Change runtime type`  を選択せよ。\n",
        "<!--First, please make sure you are using a GPU runtime to run this notebook, so inference is much faster.\n",
        "If the following command fails, use the `Runtime` menu above and select `Change runtime type`. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zHkHsdtnry57"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lErpT8-EAgrN",
        "outputId": "daf1e35f-e866-4036-e32c-2b969716ab0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul 18 00:51:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    # Shows the nVidia GPUs, if this system has any\n",
        "    !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJt_cx5QgVz"
      },
      "source": [
        "次に `diffusers`  と `scipy`, `ftfy`, `transformers` をインストールする。\n",
        "`accelerate` は読み込みを高速化するために使用する。\n",
        "<!-- Next, you should install `diffusers` as well `scipy`, `ftfy` and `transformers`. `accelerate` is used to achieve much faster loading. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.11.1\n",
        "!pip install transformers scipy ftfy accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NnPOMAqAABv"
      },
      "source": [
        "### Stable Diffusion パイプライン\n",
        "<!-- ### Stable Diffusion Pipeline-->\n",
        "\n",
        "`StableDiffusionPipeline` は，わずか数行のコードでテキストから画像を生成するために使用できるエンドツーエンドの推論パイプラインである。\n",
        "<!-- `StableDiffusionPipeline` is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code. -->\n",
        "\n",
        "まず，モデルの全成分の事前学習された重みをロードする。\n",
        "本ノートブックでは Stable Diffusion version 1.4 ([CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)) を使うが，他にも試してみたいものがあるかもしれない：\n",
        "<!--First, we load the pre-trained weights of all components of the model. In this notebook we use Stable Diffusion version 1.4 ([CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)), but there are other variants that you may want to try: -->\n",
        "* [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
        "* [stabilityai/stable-diffusion-2-1-base](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)\n",
        "* [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1).\n",
        "このバージョンは 768x768 の解像度の画像を作成できるが、他のバージョンは 512x512 で動作する。\n",
        "<!-- This version can produce images with a resolution of 768x768, while the others work at 512x512. -->\n",
        "\n",
        "モデル ID [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) に加えて、特定の `revision` と `torch_dtype` を `from_pretrained` メソッドに渡している。\n",
        "<!-- In addition to the model id [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4), we're also passing a specific `revision` and `torch_dtype` to the `from_pretrained` method.-->\n",
        "\n",
        "\n",
        "フリーの Google Colab がすべて Stable Diffusion を実行できるようにしたいので、半精度ブランチ[`fp16`](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16) から重みをロードし、`torch_dtype=torch.float16` を渡すことで `diffusers` に float16 の精度で重みを期待するように伝えている。\n",
        "<!-- We want to ensure that every free Google Colab can run Stable Diffusion, hence we're loading the weights from the half-precision branch [`fp16`](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16) and also tell `diffusers` to expect the weights in float16 precision by passing `torch_dtype=torch.float16`. -->\n",
        "\n",
        "可能な限り高い精度を確保したい場合は、メモリ使用量が増えることを覚悟で `torch_dtype=torch.float16` を削除してください。\n",
        "<!-- If you want to ensure the highest possible precision, please make sure to remove `torch_dtype=torch.float16` at the cost of a higher memory usage. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSrhSdsGAgrO"
      },
      "outputs": [],
      "source": [
        "# This is added to get around some issues of Torch not loading models correctly (test on Mac OS X and Kubuntu Linux)\n",
        "!pip install --upgrade huggingface-hub==0.26.2 transformers==4.46.1 tokenizers==0.20.1 diffusers==0.31.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MgNzTxwbASv"
      },
      "source": [
        "より高速な推論を行うため，パイプラインを GPU に移す。\n",
        "<!-- Next, let's move the pipeline to GPU to have faster inference. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LA9myHTxbDhm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device=torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device=torch.device(\"mps\")\n",
        "\n",
        "pipe = pipe.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSTsT6McuzWW"
      },
      "source": [
        "画像を生成する準備が整った：\n",
        "<!-- And we are ready to generate images: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEErJFjlrSWS"
      },
      "outputs": [],
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "image = pipe(prompt).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "# Now to display an image you can either save it such as:\n",
        "image.save(f\"astronaut_rides_horse.png\")\n",
        "\n",
        "# or if you're in a google colab you can directly display it with\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZCwCecVJI3"
      },
      "source": [
        "上記のセルを複数回実行すると，毎回異なる画像が得られる。\n",
        "もし決定論的な出力を望むなら，パイプラインにランダムな種を渡すことができる。\n",
        "同じ種を使うたびに，同じ画像結果が得られる。\n",
        "<!-- Running the above cell multiple times will give you a different image every time.\n",
        "If you want deterministic output you can pass a random seed to the pipeline.\n",
        "Every time you use the same seed you'll have the same image result. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaAW4sSdV7vZ"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "generator = torch.Generator(device).manual_seed(seed)\n",
        "image = pipe(prompt, generator=generator).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RrbYQVQXK6I"
      },
      "source": [
        "`num_inference_steps` 引数で推論ステップ数を変更できる。\n",
        "一般的に，ステップ数が多いほど良い結果が得られる。\n",
        "最新のモデルの 1 つである Stable Diffusion は，比較的少ないステップ数でうまく機能するので，デフォルトの`50` を使用することを推奨する。\n",
        "より速い結果を得たいのであれば，より小さい数を使うことができる。\n",
        "<!-- You can change the number of inference steps using the `num_inference_steps` argument.\n",
        "In general, results are better the more steps you use.\n",
        "Stable Diffusion, being one of the latest models, works great with a relatively small number of steps, so we recommend to use the default of `50`.\n",
        "If you want faster results you can use a smaller number.-->\n",
        "\n",
        "\n",
        "次のセルは先ほどと同じ種を使用しているが，ステップ数は少なくなっている。\n",
        "馬の頭やヘルメットのような細部は，前の画像よりもリアルではっきりしていないことに注意：\n",
        "<!--The following cell uses the same seed as before, but with fewer steps.\n",
        "Note how some details, such as the horse's head or the helmet, are less defin realistic and less defined than in the previous image: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKiK67iTXQkt"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "\n",
        "steps=50\n",
        "steps=15\n",
        "generator = torch.Generator(device).manual_seed(seed)\n",
        "image = pipe(prompt, num_inference_steps=steps, generator=generator).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8wxFjba5zRc"
      },
      "source": [
        "パイプラインコールのもう1つのパラメータは `guidance_scale` である。\n",
        "これは、条件信号（この場合はテキスト）、および全体的なサンプルの品質への準拠を高める方法である。\n",
        "簡単に言うと、分類器を使わないガイダンスは、よりプロンプトにマッチするように生成を強制する。\n",
        "`7` や `8.5` のような数字は良い結果をもたらすが，非常に大きな数字を使用した場合，画像は良く見えるかもしれないが，多様性に欠ける。\n",
        "<!-- The other parameter in the pipeline call is `guidance_scale`.\n",
        "It is a way to increase the adherence to the conditional signal which in this case is text as well as overall sample quality.\n",
        "In simple terms classifier free guidance forces the generation to better match with the prompt.\n",
        "Numbers like `7` or `8.5` give good results, if you use a very large number the images might look good, but will be less diverse. -->\n",
        "\n",
        "\n",
        "このパラメータの技術的な詳細については，このノート の[最終節](https://colab.research.google.com/drive/1ALXuCM5iNnJDNW5vqBm5lCtUQtZJHN2f?authuser=1#scrollTo=UZp-ynZLrS-S) を参照。\n",
        "<!-- You can learn about the technical details of this parameter in [the last section](https://colab.research.google.com/drive/1ALXuCM5iNnJDNW5vqBm5lCtUQtZJHN2f?authuser=1#scrollTo=UZp-ynZLrS-S) of this notebook. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUbR3IszB1CD"
      },
      "source": [
        "同じプロンプトに対して複数の画像を生成するには、同じプロンプトを複数回繰り返したリストを使用する。\n",
        "先ほど使った文字列の代わりに，このリストをパイプラインに送る。\n",
        "<!-- To generate multiple images for the same prompt, we simply use a list with the same prompt repeated several times.\n",
        "We'll send the list to the pipeline instead of the string we used before. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZcgsflpBoEM"
      },
      "source": [
        "まずは画像のグリッドを表示するヘルパー関数を書いてみよう。\n",
        "以下のセルを実行するだけで、`image_grid`関数が作成される。\n",
        "<!-- Let's first write a helper function to display a grid of images.\n",
        "Just run the following cell to create the `image_grid` function, or disclose the code if you are interested in how it's done. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "REF_yuHprSa1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcHccTDWbQRU"
      },
      "source": [
        "これで、3 つのプロンプトのリストでパイプラインを実行すれば、グリッド画像を生成できる。\n",
        "<!-- Now, we can generate a grid image once having run the pipeline with a list of 3 prompts. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YAFLvWWrSdM"
      },
      "outputs": [],
      "source": [
        "num_images = 3\n",
        "prompt = [\"a photograph of an astronaut riding a horse\"] * num_images\n",
        "\n",
        "images = pipe(prompt).images\n",
        "\n",
        "grid = image_grid(images, rows=1, cols=3)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj-3lCCWYtMn"
      },
      "source": [
        "次に `n × m` の画像のグリッドを生成する方法\n",
        "<!-- And here's how to generate a grid of `n × m` images. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylscg48YYxfF"
      },
      "outputs": [],
      "source": [
        "num_cols = 3\n",
        "num_rows = 4\n",
        "\n",
        "prompt = [\"a photograph of an astronaut riding a horse\"] * num_cols\n",
        "\n",
        "all_images = []\n",
        "for i in range(num_rows):\n",
        "  images = pipe(prompt).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf9pbS3kCsUf"
      },
      "source": [
        "### 正方形以外の画像を生成\n",
        "<!-- ### Generate non-square images-->\n",
        "\n",
        "Stable Diffusion はデフォルトで `512 × 512` 画素の画像を生成する。\n",
        "しかし、`height` と `width` 引数を使えば簡単にデフォルトを上書きできるので、縦長や横長の長方形の画像を作成することができる。\n",
        "<!-- Stable Diffusion produces images of `512 × 512` pixels by default.\n",
        "But it's very easy to override the default using the `height` and `width` arguments, so you can create rectangular images in portrait or landscape ratios. -->\n",
        "\n",
        "以下は適切な画像サイズを選ぶための推奨事項：\n",
        "- `height` と `width` が両方とも `8` の倍数であることを確認せよ\n",
        "- 512 を下回ると低画質になる可能性がある。\n",
        "- 両方向で 512 を超えると、画像領域が繰り返されます（全体的な一貫性が失われる）。\n",
        "- 正方形でない画像を作成する最善の方法は、1 つの次元で `512` を使用し、もう 1つ の次元でそれよりも大きな値を使用することである。\n",
        "\n",
        "<!--These are some recommendations to choose good image sizes:\n",
        "- Make sure `height` and `width` are both multiples of `8`.\n",
        "- Going below 512 might result in lower quality images.\n",
        "- Going over 512 in both directions will repeat image areas (global coherence is lost).\n",
        "- The best way to create non-square images is to use `512` in one dimension, and a value larger than that in the other one. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SXnxd-ZrSfy"
      },
      "outputs": [],
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "\n",
        "image = pipe(prompt, height=512, width=768).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW14FA-tDQ5n"
      },
      "source": [
        "\n",
        "## 2. Stable Diffusion とは何か<!--2. What is Stable Diffusion-->\n",
        "\n",
        "Stable Diffusion の理論的な部分に入ろう。\n",
        "<!-- Now, let's go into the theoretical part of Stable Diffusion 👩‍🎓.-->\n",
        "\n",
        "Stable Diffusion は [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) で提案された、**Latent Diffusion** と呼ばれる特殊な拡散モデルに基づいている。\n",
        "<!--Stable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752). -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHj_sllMaKTD"
      },
      "source": [
        "一般的な拡散モデルは、ランダムな正規雑音を段階的に **雑音除去** するように訓練された機械学習システムであり、**画像** のような関心のあるサンプルに到達する。\n",
        "拡散モデルがどのように機能するかのより詳細な概要については、[この研究室](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) を参照してほしい。\n",
        "<!-- General diffusion models are machine learning systems that are trained to *denoise* random gaussian noise step by step, to get to a sample of interest, such as an *image*.\n",
        "For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb).-->\n",
        "\n",
        "拡散モデルは，画像データの生成において最先端の結果を達成することが示されている。\n",
        "しかし拡散モデルの欠点は，逆雑音処理に時間がかかることである。\n",
        "さらに，これらのモデルは画素空間で動作するため多くのメモリを消費し，高解像度の画像を生成する場合には不当に高価になる。\n",
        "したがって，これらのモデルを学習し，推論に使用することは困難である。\n",
        "<!-- Diffusion models have shown to achieve state-of-the-art results for generating image data.\n",
        "But one downside of diffusion models is that the reverse denoising process is slow.\n",
        "In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images.\n",
        "Therefore, it is challenging to train these models and also use them for inference. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsdAj9pDPOv"
      },
      "source": [
        "<br>\n",
        "\n",
        "潜在拡散モデルは、実際の画素空間を使用する代わりに、低次元の __潜在__空間上で拡散処理を適用することにより，メモリと計算の複雑さを軽減することができる。\n",
        "これが標準的な拡散と潜在拡散モデルの主な違いです： **潜在拡散モデル** では、モデルは画像の潜在的な（圧縮された）表現を生成するように学習される。\n",
        "<!-- Latent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional _latent_ space, instead of using the actual pixel space.\n",
        "This is the key difference between standard diffusion and latent diffusion models: **in latent diffusion the model is trained to generate latent (compressed) representations of the images.** -->\n",
        "\n",
        " 潜在拡散モデルには 3 つの主要な成分がある。\n",
        "<!--There are three main components in latent diffusion. -->\n",
        "\n",
        "1. 自己符号化器 (VAE)\n",
        "2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).\n",
        "3. テキスト符号化器，[CLIP's テキスト符号化器](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).\n",
        "\n",
        "<!-- 1. An autoencoder (VAE).\n",
        "2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).\n",
        "3. A text-encoder, *e.g.* [CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4leRMZzjTsA"
      },
      "source": [
        "**1. 自己符号化器 VAE**\n",
        "<!-- **1. The autoencoder (VAE)**-->\n",
        "\n",
        "VAE モデルには，符号化器と復号化器の 2 つの部分がある。\n",
        "復号化器は画像を低次元の潜在表現に変換するために使用され，これは **U-Net** モデルの入力となる。\n",
        "復号化器は逆に，潜在表現を画像に変換する。\n",
        "<!-- The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the *U-Net* model.\n",
        "The decoder, conversely, transforms the latent representation back into an image. -->\n",
        "\n",
        "潜像拡散モデルの「訓練」の間，符号化器は順方向拡散処理のために画像の潜像表現 (_latents_) を得るために使われ，各段階でより多くの雑音が適用される。\n",
        "推論中は，逆拡散処理で生成された雑音除去された潜在変数は，VAE 復号化器を使って画像に変換される。\n",
        "後述するように，推論中に必要なのは **VAE 復号化器** だけである。\n",
        "<!--During latent diffusion _training_, the encoder is used to get the latent representations (_latents_) of the images for the forward diffusion process, which applies more and more noise at each step. During _inference_, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. As we will see during inference we **only need the VAE decoder**. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr5ZCb66kmyE"
      },
      "source": [
        "**2. Uネット**<!-- **2. The U-Net**-->\n",
        "\n",
        "U-Net は ResNet ブロックで構成された符号化器部と復号化器ダ部を持つ。\n",
        "符号化器は画像表現を低解像度の画像表現に圧縮し，復号化器は低解像度の画像表現を雑音の少ない元の高解像度の画像表現に復号化する。\n",
        "より具体的には，U-Net の出力は雑音残差を予測し，これを用いて予測された雑音除去画像表現を計算することができる。\n",
        "<!-- The U-Net has an encoder part and a decoder part both comprised of ResNet blocks.\n",
        "The encoder compresses an image representation into a lower resolution image representation and the decoder decodes the lower resolution image representation back to the original higher resolution image representation that is supposedly less noisy.\n",
        "More specifically, the U-Net output predicts the noise residual which can be used to compute the predicted denoised image representation. -->\n",
        "\n",
        "U-Net がダウンサンプリング中に重要な情報を失うのを防ぐため，通常，符号化器のダウンサンプリング ResNets と復号化器のアップサンプリングResNets の間にショートカット接続が追加される。\n",
        "さらに，Stable diffusion U-Net は交差注意層を介して，その出力をテキスト埋め込みに条件付けることができる。\n",
        "交差注意層は U-Net 符号化器部と復号化器部の両方に追加され，通常は ResNet ブロックの間に配置される。\n",
        "<!--To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder.\n",
        "Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers.\n",
        "The cross-attention layers are added to both the encoder and decoder part of the U-Net usually between ResNet blocks. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE7hhg5ArUu4"
      },
      "source": [
        "**3. テキスト・エンコーダ**\n",
        "<!-- **3. The Text-encoder**-->\n",
        "\n",
        "テキストエンコーダは、入力されたプロンプト、例えば*「An astronout riding a horse」（馬に乗った宇宙飛行士）を、U-Netが理解できる埋め込み空間に変換する役割を担っています。\n",
        "それは通常、入力トークンのシーケンスを潜在的なテキスト埋め込みシーケンスにマッピングする単純な*変換器ベース*エンコーダです。\n",
        "<!-- The text-encoder is responsible for transforming the input prompt, *e.g.* \"An astronout riding a horse\" into an embedding space that can be understood by the U-Net.\n",
        "It is usually a simple *transformer-based* encoder that maps a sequence of input tokens to a sequence of latent text-embeddings. -->\n",
        "\n",
        "\n",
        "[Imagen](https://imagen.research.google/) に触発された Stable Diffusion モデルは，学習中にテキスト符号化器の学習を行わず，単に CLIP の学習済みのテキスト符号化器である [CLIP テキストモデル](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel) を使用する。\n",
        "<!--Inspired by [Imagen](https://imagen.research.google/), Stable Diffusion does **not** train the text-encoder during training and simply uses an CLIP's already trained text encoder, [CLIPTextModel](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-XnKTVfj2Jm"
      },
      "source": [
        "**なぜ潜在拡散モデルは高速で効率的なのか？**\n",
        "<!-- **Why is latent diffusion fast and efficient?**-->\n",
        "\n",
        "潜在拡散モデルの U-Net は低次元空間で動作するため，画素空間の拡散モデルと比較して，メモリと計算要件が大幅に削減される。\n",
        "例えば Stable Diffusion で使用されている自己符号化器は，削減係数 reduction factor 8を持っている。\n",
        "つまり `(3, 512, 512)` の形状の画像は，潜在空間では `(3, 64, 64)` となり，必要なメモリは `8 × 8 = 64` 分の 1 になる。\n",
        "<!-- Since the U-Net of latent diffusion models operates on a low dimensional space, it greatly reduces the memory and compute requirements compared to pixel-space diffusion models.\n",
        "For example, the autoencoder used in Stable Diffusion has a reduction factor of 8.\n",
        "This means that an image of shape `(3, 512, 512)` becomes `(3, 64, 64)` in latent space, which requires `8 × 8 = 64` times less memory. -->\n",
        "\n",
        "これが 16 GBの Colab GPU でも`512 × 512` の画像を高速に生成できる理由である！\n",
        "<!--This is why it's possible to generate `512 × 512` images so quickly, even on 16GB Colab GPUs! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz5Ge_47jUaA"
      },
      "source": [
        "**推論中の安定拡散モデル**\n",
        "<!-- **Stable Diffusion during inference**-->\n",
        "\n",
        "では、論理的な流れを説明しながら、推論においてモデルがどのように機能するかを詳しく見ていこう。\n",
        "<!--Putting it all together, let's now take a closer look at how the model works in inference by illustrating the logical flow. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUBqX1sMsDR6"
      },
      "source": [
        "<p align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"500\"/>\n",
        "</p>\n",
        "\n",
        "Stable Diffusion モデルは潜在種とテキストプロンプトを入力とする。\n",
        "潜在種は，サイズ $64\\times 64$ のランダムな潜在的な画像表現を生成するのに使われ、テキストプロンプトはCLIP のテキスト符号化器を介してサイズ $77\\times768$ のテキスト埋め込み表現に変換される。\n",
        "<!-- The stable diffusion model takes both a latent seed and a text prompt as an input.\n",
        "The latent seed is then used to generate random latent image representations of size $64 \\times 64$ where as the text prompt is transformed to text embeddings of size $77 \\times 768$ via CLIP's text encoder.-->\n",
        "\n",
        "\n",
        "次に，U-Net  はテキスト埋め込みを条件としながら，ランダムな潜在画像表現を繰り返し **雑音除去** する。\n",
        "雑音残差である U-Net の出力は，スケジューラアルゴリズムを介して雑音除去された潜在画像表現を計算するために使われる。\n",
        "この計算には多くの異なるスケジューラアルゴリズムを用いることができ，それぞれに長所と短所がある。\n",
        "Stable Diffusion については、以下のいずれかを使用することを推奨する：\n",
        "<!--Next the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings.\n",
        "The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm.\n",
        "Many different scheduler algorithms can be used for this computation, each having its pros and cons.\n",
        "For Stable Diffusion, we recommend using one of: -->\n",
        "\n",
        "- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py) (used by default).\n",
        "- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py).\n",
        "- [Heun Discrete scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_heun_discrete.py).\n",
        "- [DPM Solver Multistep scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py).\n",
        "このスケジューラは、少ないステップで素晴らしい品質を達成することができる。\n",
        "デフォルトの 50 ではなく、25 で試してみて欲しい！\n",
        "<!-- This scheduler is able to achieve great quality in less steps.\n",
        "You can try with 25 instead of the default 50! -->\n",
        "\n",
        "\n",
        "スケジューラアルゴリズムがどのように機能するかについての理論は，ここでは扱わないが，要するにスケジューラは，以前の雑音表現と予測された雑音残差から，予測された雑音除去画像表現を計算することを覚えておく必要がある。\n",
        "詳しくは [拡散に基づく生成モデルの設計空間の解明](https://arxiv.org/abs/2206.00364) を参照することをお勧めする。\n",
        "<!-- Theory on how the scheduler algorithm function is out of scope for this notebook, but in short one should remember that they compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n",
        "For more information, we recommend looking into [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)-->\n",
        "\n",
        "\n",
        "この  **雑音除去** 処理は，より良い潜像表現を段階的に取り出すために **約50回** 繰り返される。\n",
        "一旦完了すると，潜在画像表現は変分自己符号化器の復号化器部によって復号化される。\n",
        "<!--The *denoising* process is repeated *ca.* 50 times to step-by-step retrieve better latent image representations.\n",
        "Once complete, the latent image representation is decoded by the decoder part of the variational auto encoder. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rR2Udg5IYjn"
      },
      "source": [
        "\n",
        "潜在変数と stable diffusion について簡単に紹介した後 Huggingface の diffusers の高度な使い方を見てみよう！\n",
        "<!-- After this brief introduction to Latent and Stable Diffusion, let's see how to make advanced use of 🤗 Hugging Face Diffusers! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZp-ynZLrS-S"
      },
      "source": [
        "## 3. `diffusers` を使って独自の推論パイプラインを書く方法<!-- ## 3. How to write your own inference pipeline with `diffusers`-->\n",
        "\n",
        "最後に、`diffusers` を使ってカスタムの推論パイプラインを作成する方法を紹介する。\n",
        "これは系の特定の機能をもう少し深く掘り下げたり，特定の成分を切り替えたりするのに非常に便利である。\n",
        "<!-- Finally, we show how you can create custom diffusion pipelines with `diffusers`.\n",
        "This is often very useful to dig a bit deeper into certain functionalities of the system and to potentially switch out certain components. -->\n",
        "\n",
        "本節では、異なるスケジューラ、すなわち [このPR](https://github.com/huggingface/diffusers/pull/185#pullrequestreview-1074247365)で追加された[Katherine Crowson](https://github.com/crowsonkb) の K-LMS スケジューラで Stable Diffusion を使用する方法を示す。\n",
        "<!--In this section, we will demonstrate how to use Stable Diffusion with a different scheduler, namely [Katherine Crowson's](https://github.com/crowsonkb) K-LMS scheduler that was added in [this PR](https://github.com/huggingface/diffusers/pull/185#pullrequestreview-1074247365). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEXkmX3vDPRU"
      },
      "source": [
        "`StableDiffusionPipeline` をそれぞれ見ていこう。\n",
        "<!-- Let's go through the `StableDiffusionPipeline` step by step to see how we could have written it ourselves.-->\n",
        "\n",
        "まず，それぞれのモデルをロードすることから始めよう。\n",
        "<!--We will start by loading the individual models involved. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Xw7qSEdTpt"
      },
      "source": [
        "[訓練済モデル](https://huggingface.co/CompVis/stable-diffusion-v1-3-diffusers/tree/main) には，完全な stable  diffusion パイプラインをセットアップするために必要なすべての成分が含まれている。\n",
        "これらは以下のフォルダに格納されている：\n",
        "- `text_encoder`  Stable Diffusion は CLIP を使用しているが、他の拡散モデルでは `BERT` のような他の符号化器を使用することもある。\n",
        "- `tokenizer`:   `text_encoder` モデルで使用されているものと一致しなければならない\n",
        "-  `schduler `： 訓練中に画像に雑音を徐々に追加するためのスケジューリングアルゴリズム\n",
        "-` unet`： 入力の潜在表現を生成するためのモデル\n",
        "- `vae`： 潜在表現を実画像に復号化するために使用する自己符号化器モジュール\n",
        "\n",
        "`from_pretrained` の引数  `subfolder` を使って，成分を保存したフォルダを参照することで，そこにある成分をロードすることができる。\n",
        "\n",
        "<!-- The [pre-trained model](https://huggingface.co/CompVis/stable-diffusion-v1-3-diffusers/tree/main) includes all the components required to setup a complete diffusion pipeline. They are stored in the following folders:\n",
        "- `text_encoder`: Stable Diffusion uses CLIP, but other diffusion models may use other encoders such as `BERT`.\n",
        "- `tokenizer`. It must match the one used by the `text_encoder` model.\n",
        "- `scheduler`: The scheduling algorithm used to progressively add noise to the image during training.\n",
        "- `unet`: The model used to generate the latent representation of the input.\n",
        "- `vae`: Autoencoder module that we'll use to decode latent representations into real images.\n",
        "\n",
        "We can load the components by referring to the folder they were saved, using the `subfolder` argument to `from_pretrained`. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlsKwQijWMpL"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "# 1. Load the autoencoder model which will be used to decode the latents into image space.\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
        "\n",
        "# 2. Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# 3. The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eYstOqwVYGc"
      },
      "source": [
        "ここでは，定義済みのスケジューラーをロードする代わりに K-LMS スケジューラーを使うことにする。\n",
        "<!-- Now instead of loading the pre-defined scheduler, we'll use the K-LMS scheduler instead. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-0W8UG6VXpD"
      },
      "outputs": [],
      "source": [
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "scheduler = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBXqwuHFYgf4"
      },
      "source": [
        "次に、モデルをGPUに移す。\n",
        "<!-- Next we move the models to the GPU. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3lJEXz7YgnC"
      },
      "outputs": [],
      "source": [
        "vae = vae.to(device)\n",
        "text_encoder = text_encoder.to(device)\n",
        "unet = unet.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtqGYl5SY6dm"
      },
      "source": [
        "ここで，画像生成に使用するパラメータを定義する。\n",
        "<!-- We now define the parameters we'll use to generate images.-->\n",
        "\n",
        "`guidance_scale` は [Imagen 論文](https://arxiv.org/pdf/2205.11487.pdf) の式(2) のガイダンスの重み `w` に類似して定義されていることに注意。\n",
        "`guidance_scale == 1` は分類器を使わないガイダンスを行うことに相当する。ここでは7.5に設定する。\n",
        "<!-- Note that `guidance_scale` is defined analog to the guidance weight `w` of equation (2) in the [Imagen paper](https://arxiv.org/pdf/2205.11487.pdf). `guidance_scale == 1` corresponds to doing no classifier-free guidance. Here we set it to 7.5 as also done previously. -->\n",
        "\n",
        "前例とは対照的に，より明確な画像を得るために `num_inference_steps` を 100 に設定する。\n",
        "<!--In contrast to the previous examples, we set `num_inference_steps` to 100 to get an even more defined image. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot8RDV-2Y6uE"
      },
      "outputs": [],
      "source": [
        "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
        "\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 512                         # default width of Stable Diffusion\n",
        "\n",
        "num_inference_steps = 100            # Number of denoising steps\n",
        "\n",
        "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
        "\n",
        "generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
        "\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G47gEbg9Z4sJ"
      },
      "source": [
        "まず，プロンプトの text_embeddings を得る。\n",
        "これらの埋め込みは UNet モデルの条件に使われる。\n",
        "<!-- First, we get the text_embeddings for the prompt.\n",
        "These embeddings will be used to condition the UNet model. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZpvyVT1Y6wq"
      },
      "outputs": [],
      "source": [
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(device))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INGdc9eFaeWz"
      },
      "source": [
        "これは 埋草 (パディング)トークン (空のテキスト) の埋め込みである。\n",
        "これらは条件付きの `text_embeddings` (`batch_size` と `seq_length`) と同じ次元を持つ必要がある。\n",
        "<!-- We'll also get the unconditional text embeddings for classifier-free guidance, which are just the embeddings for the padding token (empty text).\n",
        "They need to have the same shape as the conditional `text_embeddings` (`batch_size` and `seq_length`) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkuwhbFrY6zo"
      },
      "outputs": [],
      "source": [
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lKMrvoYbxzf"
      },
      "source": [
        "分類器を使わないガイダンスでは，2 つのフォワードパスを行う必要がある。\n",
        "一つは条件付き入力（`text_embeddings`）で、もう一つは無条件埋め込み（`uncond_embeddings`）である。\n",
        "実際には、2 つのフォワードパスを回避するために、両方を 1 つのバッチに連結することができる。\n",
        "<!-- For classifier-free guidance, we need to do two forward passes.\n",
        "One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwnB7CIeY619"
      },
      "outputs": [],
      "source": [
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxcaMgD0DPUD"
      },
      "source": [
        "初期ランダムノイズを生成する。\n",
        "<!-- Generate the intial random noise. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NsfjxA-chAL"
      },
      "outputs": [],
      "source": [
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC401krZfXOr"
      },
      "outputs": [],
      "source": [
        "latents.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDUOA1gHMp2Y"
      },
      "source": [
        "クールな $64 \\times 64$ が期待される。\n",
        "モデルは，この潜在表現 (純粋なノイズ) を後で `512 × 512` 画像に変換する。\n",
        "<!-- Cool $64 \\times 64$ is expected. The model will transform this latent representation (pure noise) into a `512 × 512` image later on.-->\n",
        "\n",
        "次に、選択した `num_inference_steps` を使用してスケジューラを初期化する。\n",
        "\n",
        "これにより，ノイズ除去処理中に使用される `sigmas` と正確な時間ステップ値が計算される。\n",
        "<!--Next, we initialize the scheduler with our chosen `num_inference_steps`.\n",
        "This will compute the `sigmas` and exact time step values to be used during the denoising process. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6UDqCyKwBpx"
      },
      "outputs": [],
      "source": [
        "scheduler.set_timesteps(num_inference_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTOxOKeqW4XE"
      },
      "source": [
        "K-LMS スケジューラは、`latents` に `sigma` 値を掛ける必要がある。\n",
        "ここでこれを行う。\n",
        "<!-- The K-LMS scheduler needs to multiply the `latents` by its `sigma` values. Let's do this here -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTwTq9d-W_NP"
      },
      "outputs": [],
      "source": [
        "latents = latents * scheduler.init_noise_sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdVkvYuYdjc6"
      },
      "source": [
        "これでノイズ除去ループを書く準備ができた。\n",
        "<!-- We are ready to write the denoising loop. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylc3AIdZkFhl"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "\n",
        "for t in tqdm(scheduler.timesteps):\n",
        "  # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "  latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "  # predict the noise residual\n",
        "  with torch.no_grad():\n",
        "    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "  # perform guidance\n",
        "  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "  # compute the previous noisy sample x_t -> x_t-1\n",
        "  latents = scheduler.step(noise_pred, t, latents).prev_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZUTKjm0kuDY"
      },
      "source": [
        "次に，`vae` を使って，生成された `latents` を画像に復号化する。\n",
        "<!-- We now use the `vae` to decode the generated `latents` back into the image. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YRzuJP7kMZo"
      },
      "outputs": [],
      "source": [
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "  image = vae.decode(latents).sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmdOa4_Dqrl8"
      },
      "source": [
        "そして最後に，画像を PIL に変換して表示または保存できるようにする。\n",
        "<!-- And finally, let's convert the image to PIL so we can display or save it. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAVZStIokTVv"
      },
      "outputs": [],
      "source": [
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjYuICyxwpeO"
      },
      "source": [
        "これで，独自のパイプラインを構築したり，diffuser 成分を好きなように使用したりするためのすべてのパーツが揃った。\n",
        "<!-- Now you have all the pieces to build your own pipelines or use diffusers components as you like 🔥. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwwaTOxRuzWj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}