{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_0425olivetti_face_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# オリベッティ顔データセットを用いた機械学習実習\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5-amTpixBS_o"
      },
      "id": "5-amTpixBS_o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0956f10b-f104-44e8-bf7d-8f516b27b3c8",
      "metadata": {
        "id": "0956f10b-f104-44e8-bf7d-8f516b27b3c8"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# オリベッティ顔データセットの読み込み\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "data = fetch_olivetti_faces()\n",
        "X, y = data.data, data.target\n",
        "print(f'訓練データサイズ X.shape:{X.shape}')\n",
        "print(f'教師データサイズ y.shape:{y.shape}')\n",
        "print(data.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a06cd9f-036f-4b7c-8c90-80444efe935b",
      "metadata": {
        "id": "9a06cd9f-036f-4b7c-8c90-80444efe935b"
      },
      "outputs": [],
      "source": [
        "# データの表示\n",
        "nrows = 40   # nrows 人分のデータを表示\n",
        "ncols = 10\n",
        "fig, fig_axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 1.4, nrows * 1.4), constrained_layout=True)\n",
        "# constrained_layout は subplot や 凡例やカラーバーなどの装飾を自動的に調整して，\n",
        "# ユーザが要求する論理的なレイアウトをできるだけ維持しながら， 図ウィンドウに収まるようにします。\n",
        "\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        x = i * 10 + j\n",
        "        fig_axes[i][j].imshow(X[x].reshape(64,64), cmap='gray')\n",
        "        fig_axes[i][j].axis('off')\n",
        "        fig_axes[i][j].set_title(f'num:{x}, ID:{y[x]}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 男女の判別のため教師データを作成\n",
        "# 男であれば 0, 女であれば 1 とする\n",
        "y_sex = np.zeros_like(y)\n",
        "for woman_start in [70, 310, 340]:\n",
        "    for i in range(10):\n",
        "        y_sex[woman_start+i]=1\n",
        "\n",
        "for i in range(0, len(X),10):\n",
        "    print(f'{i:3d}', end=\":\")\n",
        "    for j in range(10):\n",
        "        print(y_sex[i+j], end=\" \")\n",
        "    print(\"\")\n",
        "\n",
        "    #print(y_sex[65:85])"
      ],
      "metadata": {
        "id": "1bNppGAoBhAp"
      },
      "id": "1bNppGAoBhAp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import seaborn as sns  # ヒートマップ描画のために使用\n",
        "\n",
        "#from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "#split_ratio = 0.1 としているので，訓練データ対テストデータが 9:1 になります\n",
        "split_ratio = 0.1\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y_sex, test_size=split_ratio, stratify=y, random_state=42)\n",
        "#X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=split_ratio, stratify=y, random_state=42)\n",
        "print(f'X_train 訓練画像のサイズ: {X_train.shape}')\n",
        "print(f'y_train 教師信号データのサイズ: {y_train.shape}')"
      ],
      "metadata": {
        "id": "QM9GNboxBo3i"
      },
      "id": "QM9GNboxBo3i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'max_iter':10 ** 3,\n",
        "          'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "model = LogisticRegression(**params)\n",
        "model.fit(X_train, y_train)    # 訓練データを用いて線形判別分析モデルを訓練\\n\",\n",
        "y_hat = model.predict(X_test)  # テストデータを使って予測を行い結果を y_hat に格納\\n\",\n",
        "print(f\"ロジスティック回帰を用いた分類精度: {metrics.accuracy_score(y_test, y_hat):.3f}\")\n",
        "\n",
        "# 混同行列の表示\n",
        "print(metrics.confusion_matrix(y_test,y_hat))\n",
        "# plt.figure(figsize=(8,6))\n",
        "# sns.heatmap(metrics.confusion_matrix(y_test, y_hat))\n"
      ],
      "metadata": {
        "id": "e1a30yIeCGRP"
      },
      "id": "e1a30yIeCGRP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {#'max_iter':10 ** 3,\n",
        "          #'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "model = LinearDiscriminantAnalysis(**params)\n",
        "model.fit(X_train, y_train)    # 訓練データを用いて線形判別分析モデルを訓練\n",
        "y_hat = model.predict(X_test)  # テストデータを使って予測を行い結果を y_hat に格納\n",
        "print(f\"線形判別分析を用いた分類精度: {metrics.accuracy_score(y_test, y_hat):.3f}\")\n",
        "\n",
        "# 混同行列の表示\n",
        "print(metrics.confusion_matrix(y_test,y_hat))\n",
        "# plt.figure(figsize=(8,6))\n",
        "# sns.heatmap(metrics.confusion_matrix(y_test, y_hat))"
      ],
      "metadata": {
        "id": "i1zDPV2ABuKl"
      },
      "id": "i1zDPV2ABuKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'max_iter':10 ** 4,\n",
        "          'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "svc_model = SVC(**params)\n",
        "logistic_model = LogisticRegression(**params)\n",
        "lda_model = LinearDiscriminantAnalysis()\n",
        "\n",
        "# fig, fig_axes = plt.subplots(ncols=ncols, nrows=1, figsize=(ncols * 1.4, nrows * 1.4), constrained_layout=True)\n",
        "# constrained_layout は subplot や 凡例やカラーバーなどの装飾を自動的に調整して，\n",
        "# ユーザが要求する論理的なレイアウトをできるだけ維持しながら， 図ウィンドウに収まるようにします。\n",
        "\n",
        "# for i in range(nrows):\n",
        "#     for j in range(ncols):\n",
        "#         x = i * 10 + j\n",
        "#         fig_axes[i][j].imshow(X[x].reshape(64,64), cmap='gray')\n",
        "#         fig_axes[i][j].axis('off')\n",
        "#         fig_axes[i][j].set_title(f'num:{x}, ID:{y[x]}')\n",
        "\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=split_ratio, stratify=y, random_state=42)\n",
        "for model in [svc_model, logistic_model, lda_model]:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_hat = model.predict(X_test)\n",
        "    print(f\"分類精度: {metrics.accuracy_score(y_test, y_hat):.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xTig2ErwBxZU"
      },
      "id": "xTig2ErwBxZU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4a10a932-ea1a-416a-89d3-30dece958de8",
      "metadata": {
        "id": "4a10a932-ea1a-416a-89d3-30dece958de8"
      },
      "source": [
        "# 機械学習手法による顔認識\n",
        "\n",
        "## データの分割，訓練データとテストデータ\n",
        "\n",
        "データを 2 分割して，訓練データセットとテストデータセットに分割します。\n",
        "分割した訓練データセットでモデルのパラメータを学習し，しかる後に，テストデータセットで，その汎化性能を評価します。\n",
        "このとき，テストデータセットでの性能が高いモデルが良いモデルということになります。\n",
        "\n",
        "オリベッティ顔データセットには， 各被験者の 10 枚の顔画像が含まれています。\n",
        "このうち，例えば 90% を訓練データとし，10% をテストデータとして使用することを考えます。\n",
        "各顔データの訓練画像とテスト画像の数が同じになるように stratify 機能を使用してます。\n",
        "したがって，各被験者には 9 枚の訓練用画像と 1 枚のテスト用画像が用意されることになります。\n",
        "訓練データとテストデータの割合は split_ratio 変更することができます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d69392d5-b9b5-4990-8b20-93e8703618a8",
      "metadata": {
        "id": "d69392d5-b9b5-4990-8b20-93e8703618a8"
      },
      "source": [
        "## ロジスティック回帰による予測"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train, X_test, y_train, y_test=train_test_split(X, y_sex, test_size=split_ratio, stratify=y, random_state=42)\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=split_ratio, stratify=y, random_state=42)\n"
      ],
      "metadata": {
        "id": "L5gqzqhbCWPG"
      },
      "id": "L5gqzqhbCWPG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c446cb39-3556-4913-9996-e500e1c13aa4",
      "metadata": {
        "id": "c446cb39-3556-4913-9996-e500e1c13aa4"
      },
      "outputs": [],
      "source": [
        "params = {'max_iter':10 ** 3,\n",
        "          'C':1e3,\n",
        "          'penalty':'l2'}\n",
        "\n",
        "model = LogisticRegression(**params)\n",
        "model.fit(X_train, y_train)    # 訓練データを用いて線形判別分析モデルを訓練\n",
        "y_hat = model.predict(X_test)  # テストデータを使って予測を行い結果を y_hat に格納\n",
        "print(f\"ロジスティック回帰を用いた分類精度: {metrics.accuracy_score(y_test, y_hat) * 100:.2f}%\")\n",
        "\n",
        "# 混同行列の表示\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(metrics.confusion_matrix(y_test, y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701b9057-1abd-478f-9e17-5a30b7b0ce20",
      "metadata": {
        "id": "701b9057-1abd-478f-9e17-5a30b7b0ce20"
      },
      "source": [
        "## 交差検証"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e27bda-c885-4f53-a236-c444d77e2f44",
      "metadata": {
        "id": "d8e27bda-c885-4f53-a236-c444d77e2f44",
        "outputId": "9d6b7a00-10e4-4abf-a2e3-523fde7e9b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ロジスティック回帰 平均交差検証得点: 0.88\n",
            "CPU times: user 34.5 s, sys: 2.11 s, total: 36.7 s\n",
            "Wall time: 3.85 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "name = 'ロジスティック回帰'\n",
        "model = LogisticRegression(**params)\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=kfold)\n",
        "print(f\"{name} 平均交差検証得点: {cv_scores.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f0794b-5c93-4641-9e94-e8788495f399",
      "metadata": {
        "id": "55f0794b-5c93-4641-9e94-e8788495f399"
      },
      "source": [
        "## リーブ・ワン・アウト 交差検証\n",
        "\n",
        "オリベッティ顔データセットには，各被験者に対して 10 枚の顔画像が含まれています。\n",
        "これは， 機械学習モデルの学習やテストには少ない数です。\n",
        "\n",
        "クラスの例が少ない機械学習モデルをよりよく評価するために，採用される交差検証法にリーブ・ワン・アウト leave-one-out (LOO) 交差検証法があります。\n",
        "LOO 法では，あるクラスのサンプルのうち 1 つだけをテストに使用します。\n",
        "他のサンプルは訓練に使用します。 この手順を， 全サンプルを一度づつテストに使用して繰り返さします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96313718-4fe2-4e70-81c0-e5be115cf9c8",
      "metadata": {
        "id": "96313718-4fe2-4e70-81c0-e5be115cf9c8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "loo_cv = LeaveOneOut()\n",
        "model = LogisticRegression(**params)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=loo_cv)\n",
        "\n",
        "print(f\"{model.__class__.__name__} リーブ・ワン・アウト交差検証法による平均得点:{cv_scores.mean():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8819b8-db47-44db-b5e8-fc2fa3dd79e6",
      "metadata": {
        "id": "ca8819b8-db47-44db-b5e8-fc2fa3dd79e6"
      },
      "source": [
        "## ハイパーパラメータの調整: GridSearcCV\n",
        "\n",
        "モデルの汎化性能向上のために GridSearchCV を行います。\n",
        "ロジスティック回帰分類器のハイパーパラメータを調整してみます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6952b8de-87a5-4c4d-afac-2c411e7c6bf0",
      "metadata": {
        "id": "6952b8de-87a5-4c4d-afac-2c411e7c6bf0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "params2={'penalty':['l1', 'l2'],\n",
        "        'C':np.logspace(0, 4, 10),\n",
        "        'max_iter': [10 ** 4],\n",
        "       }\n",
        "model = LogisticRegression()\n",
        "loo_cv = LeaveOneOut()\n",
        "gridSearchCV = GridSearchCV(model, params2, cv=loo_cv)\n",
        "gridSearchCV.fit(X_train, y_train)\n",
        "print(\"Grid search fitted..\")\n",
        "print(gridSearchCV.best_params_)\n",
        "print(gridSearchCV.best_score_)\n",
        "print(f\"グリッドサーチによる交差妥当性得点:{gridSearchCV.score(X_test, y_test):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a8c08d-0222-45e7-ac36-ee675d0e306a",
      "metadata": {
        "id": "44a8c08d-0222-45e7-ac36-ee675d0e306a"
      },
      "source": [
        "# PCA\n",
        "\n",
        "## 固有顔 Eigenfaces\n",
        "\n",
        "与えられた画像表現の問題は、その高次元性である。\n",
        "2次元の $p\\times q$ の濃淡画像は $m=pq$ 次元のベクトル空間にまたがるので、$100\\times100$ 画素の画像はすでに 10,000 次元の画像空間にある。\n",
        "これはどんな計算をするにも多すぎるが、すべての次元が本当に我々にとって有用なのだろうか？\n",
        "我々はデータに分散がある場合にのみ判断を下すことができるので、我々が探しているのは情報の大部分を占める成分である。\n",
        "主成分分析 (PCA) は、カール・ピアソン(1901) とハロルド・ホテリング (1933) によって独自に提案されたもので，相関している可能性のある変数の集合を，より小さな相関していない変数の集合に変えるものである。\n",
        "このアイデアは、高次元のデータ集合は、しばしば相関のある変数によって記述され、したがって、いくつかの意味のある次元だけが情報の大部分を占めるというものである。\n",
        "PCA は、主成分と呼ばれるデータ中の最大の分散を持つ方向を見つける。\n",
        "<!-- The problem with the image representation we are given is its high dimensionality.\n",
        "Two-dimensional pxq grayscale images span a m=pq-dimensional vector space, so an image with 100x100 pixels lies in a 10,000-dimensional image space already.\n",
        "That's way too much for any computations, but are all dimensions really useful for us?\n",
        "We can only make a decision if there's any variance in data, so what we are looking for are the components that account for most of the information.\n",
        "The Principal Component Analysis (PCA) was independently proposed by Karl Pearson (1901) and Harold Hotelling (1933) to turn a set of possibly correlated variables into a smaller set of uncorrelated variables.\n",
        "The idea is that a high-dimensional dataset is often described by correlated variables and therefore only a few meaningful dimensions account for most of the information.\n",
        "The PCA method finds the directions with the greatest variance in the data, called principal components. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf5815a",
      "metadata": {
        "id": "faf5815a"
      },
      "source": [
        "### アルゴリズム<!--Algorithmic Description-->\n",
        "\n",
        "$X=\\left\\{x_1,x_2,\\ldots,x_n\\right\\}$ は $x_i\\in\\mathbb{R}^{d}$ からの確率ベクトルとする:\n",
        "<!-- Let $X=\\left\\{x_1,x_2,\\ldots,x_n\\right\\}$ be a random vector with observation $x_i\\in\\mathbb{R}^{d}$. -->\n",
        "\n",
        "1. Compute the mean $\\mu$\n",
        "$$\\tag{1}\n",
        "\\mu=\\frac{1}{n}\\sum_{i=1}^{n}x_i.\n",
        "$$\n",
        "2. Compute the Covariance matrix $C$\n",
        "$$\\tag{2}\n",
        "C=\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)(x_i-\\mu)^{\\top}.\n",
        "$$\n",
        "3. Compute the eigenvalues $\\lambda_i$ and eigenvectors $\\nu_i$ of $C$\n",
        "$$\\tag{3}\n",
        "C\\nu_i=\\lambda_i\\nu_i, \\text{\\hspace{1cm} $i=1,2,\\ldots,n$}\n",
        "$$\n",
        "4. Order the eigenvalues descending by their eigenvalue.\n",
        "The $k$ principla components are the eigenvectors corresponding to the $k$ largest eigenvalues.\n",
        "\n",
        "観測ベクトル $x$ の $k$ 個の主成分は次式で与えられる：<!-- The k principal components of the observed vector $x$ are then given by: -->\n",
        "$$\\tag{4}\n",
        "y = W^{\\top}(x-\\mu),\n",
        "$$\n",
        "ここで ＄W=\\left(v_1,v_2,\\ldots,v_k\\right)$. <!--where $W=left(v_1,v_2,\\ldots,v_k\\right)$.-->\n",
        "PCA 基底からの再構成は次式:<!--The reconstruction from the PCA basis is given by:-->\n",
        "$$\\tag{5}\n",
        "x= Wy + \\mu.\n",
        "$$\n",
        "そして固有顔法は、次のようにして顔認識を行う：<!-- The Eigenfaces method then performs face recognition by: -->\n",
        "1. すべての訓練サンプルを PCA の部分空間に射影する  (式 (4) )。\n",
        "2. クエリ画像を PCA 部分空間に射影する (リスト 5 を使用)。\n",
        "3. 投影された訓練サンプルと投影されたクエリ画像の間の最近傍を見つける。\n",
        "\n",
        "<!-- 1. Projecting all training samples into the PCA subspace (using Equation 4).\n",
        "2. Projecting the query image into the PCA subspace (using Listing 5).\n",
        "3. Finding the nearest neighbor between the projected training samples and the projected query image. -->\n",
        "\n",
        "まだ、解決しなければならない問題が 1 つ残っている。\n",
        "100x100 画素の画像が 400 枚与えられたとする。\n",
        "主成分分析（PCA）は共分散行列 $C=XX^{\\top}$ を解くが、この例では size(X)=$10000\\times400$ である。\n",
        "結局 10000x10000 の行列になり、ざっと 0.8GB になる。\n",
        "この問題を解くのは現実的ではないので、トリックを適用する必要がある。\n",
        "線形代数の授業で、$M>N$ の $M\\times N$ 行列は $N-1$ 個の非ゼロ固有値しか持てないことを知っているだろう。\n",
        "そこで、代わりにサイズ $N\\times N$ の固有値分解 $S=X^{\\top}X$ を取ることができる：\n",
        "<!-- Still there is one problem left to solve.\n",
        "Imagin we are given 400 images sized 100x100 pixels.\n",
        "The Principal Component Analysis (PCA) solves the covariance matrix $C=XX^{\\top}$, where size(X)=10000X400 in our example.\n",
        "You would end up with a 10000x10000 matrix, rougly 0.8GB.\n",
        "Solving this  problem is not feasible, so we will need to apply a trick.\n",
        "From your linear algebra lessons you know that a MxN matrix with M>N can only have N-1 non-zero eigenvalues.\n",
        "So it is possible to take the eigenvalue decompostion $S=X^{\\top}X$ of size NxN instead: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c511f0b9-9eee-4998-b600-ecce4f435f01",
      "metadata": {
        "id": "c511f0b9-9eee-4998-b600-ecce4f435f01"
      },
      "outputs": [],
      "source": [
        "# X を n 行 m 列の行列として，列方向 m x m の相関係数行列を求める\n",
        "x_bar = np.mean(X, axis=0)         # 各列の平均値を計算\n",
        "X_bar = X - x_bar                  # 各列の平均値を減じて平均偏差ベクトルとする\n",
        "Cov = X_bar.T @ X_bar / X.shape[0] # 共分散行列\n",
        "X_std = np.std(X, axis=0)          # 各列の標準偏差\n",
        "R = Cov / np.outer(X_std.T, X_std) # 共分散行列の各列を対応する標準偏差の積で除して相関係数行列にする\n",
        "print(f'R.shape:{R.shape}')        # 確認用 相関係数行列のサイズ\n",
        "print(f'R\\n:{R[:3,:3]}')           # 確認用 相関係数行列の最初の 3 行 3 列を表示する\n",
        "\n",
        "R2 = np.corrcoef(X.T)              # 上記を一行で行う numpy コマンド\n",
        "print(f'R2\\n:{R2[:3,:3]}')         # 結果の表示\n",
        "\n",
        "\n",
        "def pca(X, n_dim = 5):\n",
        "    \"\"\"N 行 x D 列 の行列の PCA を実施\"\"\"\n",
        "    (n, m) = X.shape  # 行列のサイズを取得\n",
        "    #X = X - np.tile(np.mean(X, 0), (n, 1))  # X から\n",
        "    X = X - np.mean(X, axis=0)\n",
        "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
        "    Y = np.dot(X, M[:,0:n_dim])\n",
        "    return Y\n",
        "\n",
        "print(pca(X.T)[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbd8e200-23b4-4c8e-b7eb-d5ef8c4736ba",
      "metadata": {
        "id": "cbd8e200-23b4-4c8e-b7eb-d5ef8c4736ba"
      },
      "outputs": [],
      "source": [
        "def pca (X, y, num_components=0):\n",
        "    [n,d] = X. shape\n",
        "    if ( num_components <= 0) or ( num_components >n):\n",
        "        num_components = n\n",
        "    mu = X. mean (axis=0)\n",
        "    X = X - mu\n",
        "    if n>d:\n",
        "        C = np.dot(X.T, X)\n",
        "        [eigenvalues, eigenvectors] = np.linalg.eigh(C)\n",
        "    else:\n",
        "        C = np.dot(X, X.T)\n",
        "        [eigenvalues, eigenvectors] = np.linalg.eigh(C)\n",
        "        eigenvectors = np.dot(X.T, eigenvectors)\n",
        "        for i in range (n):\n",
        "            eigenvectors[:,i] = eigenvectors[:,i]/ np.linalg.norm(eigenvectors[:,i])\n",
        "\n",
        "    # or simply perform an economy size decomposition\n",
        "    # eigenvectors, eigenvalues, variance = np.linalg.svd(X.T, full_matrices=False)\n",
        "    # sort eigenvectors descending by their eigenvalue\n",
        "    idx = np.argsort(-eigenvalues)\n",
        "    eigenvalues = eigenvalues[idx]\n",
        "    eigenvectors = eigenvectors[:,idx]\n",
        "\n",
        "    # select only num_components\n",
        "    eigenvalues = eigenvalues[0:num_components].copy()\n",
        "    eigenvectors = eigenvectors[:,0:num_components].copy()\n",
        "    return [eigenvalues, eigenvectors, mu]\n",
        "\n",
        "def project(W, X, mu=None):\n",
        "     if mu is None :\n",
        "         return np. dot(X, W)\n",
        "     return np. dot (X-mu, W)\n",
        "\n",
        "def reconstruct(W, Y, mu=None):\n",
        "    if mu is None:\n",
        "        return np.dot(Y, W.T)\n",
        "    return np.dot(Y, W.T) + mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c13ca7f5",
      "metadata": {
        "id": "c13ca7f5"
      },
      "outputs": [],
      "source": [
        "n_comp = 3\n",
        "eigs, eigvec, mu = pca(X, y, num_components=n_comp)\n",
        "print(eigs.shape, eigvec.shape, mu.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ab8ea0",
      "metadata": {
        "id": "92ab8ea0"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_sklearn = PCA(n_components=200)\n",
        "pca_sklearn.fit(X)\n",
        "pca_result = pca_sklearn.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100cfac8",
      "metadata": {
        "id": "100cfac8"
      },
      "outputs": [],
      "source": [
        "#print(pca_result[:10])\n",
        "print(f'各主成分の寄与率:{pca_sklearn.explained_variance_ratio_[:30]}')  # 各主成分の寄与率\n",
        "print(f'各主成分の固有値:{pca_sklearn.explained_variance_[:30]}')      # 各主成分の固有値\n",
        "print(f'主成分ベクトル:{pca_sklearn.components_[:30].shape}')             # 主成分ベクトル\n",
        "recov = pca_sklearn.inverse_transform(pca_result[:3])  # 主成分を元の次元に戻す\n",
        "print(recov.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "249d9899",
      "metadata": {
        "id": "249d9899"
      },
      "outputs": [],
      "source": [
        "#print(recov[0].min(),recov[0].max())\n",
        "\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(recov[0].reshape(64,64), cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(recov[1].reshape(64,64), cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(recov[2].reshape(64,64), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a2376b",
      "metadata": {
        "id": "13a2376b"
      },
      "outputs": [],
      "source": [
        "#data.data[0].reshape(92,112).shape\n",
        "4096 / 92"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14fe935",
      "metadata": {
        "id": "e14fe935"
      },
      "source": [
        "## フィッシャー顔 Fisherfaces\n",
        "\n",
        "線形判別分析は、偉大な統計学者である R.A.フィッシャー 卿によって発明され、彼は1936年の論文 `The use of multiple measurements in classificationonomic problems の中で、アヤメの分類に用いることに成功した。\n",
        "しかし、主成分分析 (PCA) がこれほどうまくいったのに、なぜ別の次元削減法が必要なのだろうか？\n",
        "PCA は、データの全分散を最大化する特徴の線形結合を見つける。\n",
        "これは明らかにデータを表現する強力な方法だが、クラスを考慮しないため、成分を捨てるときに多くの識別情報が失われる可能性がある。\n",
        "分散が外部ソースによって生成される状況を想像してみよう。\n",
        "PCA によって同定された成分は、必ずしも識別情報を全く含まないので、投影されたサンプルは互いに混ざり合い、分類は不可能になる。\n",
        "クラス間を最もよく分離する特徴の組み合わせを見つけるために、線形判別分析はクラス間のばらつきとクラス内のばらつきの比率を最大化する。\n",
        "考え方は単純で、同じクラスは互いに密に集まり、異なるクラスはできるだけ離れるべきである。\n",
        "このこと はBelhumeur_Hespanha_Kriegman も認識しており、彼らは [3] で顔認識に判別分析を適用した。\n",
        "<!-- The Linear Discriminant Analysis was invented by the great statistician Sir R. A. Fisher, who successfully used it for classifying owers in his 1936 paper The use of multiple measurements in taxonomic problems [8].\n",
        "But why do we need another dimensionality reduction method, if the Principal Component Analysis (PCA) did such a good job?\n",
        "The PCA  nds a linear combination of features that maximizes the total variance in data.\n",
        "While this is clearly a powerful way to represuccsent data, it doesn't consider any classes and so a lot of discriminative information may be lost when throwing components away.\n",
        "Imagine a situation where the variance is generated by an external source, let it be the light.\n",
        "The components identi ed by a PCA do not necessarily contain any discriminative information at all, so the projected samples are smeared together and a classi cation becomes impossible.\n",
        "In order to  nd the combination of features that separates best between classes the Linear Discriminant Analysis maximizes the ratio of between-classes to within-classes scatter.\n",
        "The idea is simple: same classes should cluster tightly together, while di erent classes are as far away as possible from each other.\n",
        "This was also recognized by Belhumeur, Hespanha and Kriegman and so they applied a Discriminant Analysis to face recognition in [3]. -->\n",
        "\n",
        "* [3] Belhumeur, P. N., Hespanha, J., and Kriegman, D. Eigenfaces vs.  sherfaces: Recognition using class speci c linear projection. IEEE Transactions on Pattern Analysis and Machine Intelligence 19, 7 (1997), 711-720.\n",
        "* [4] Brunelli, R., and Poggio, T. Face recognition through geometrical features. In European Conference on Computer Vision (ECCV) (1992), pp. 792-800.\n",
        "* [5] Cardinaux, F., Sanderson, C., and Bengio, S. User authentication via adapted statistical models of face images. IEEE Transactions on Signal Processing 54 (January 2006), 361-373.\n",
        "* [6] Chiara Turati, Viola Macchi Cassia, F. S., and Leo, I. Newborns face recognition: Role of inner and outer facial features. Child Development 77, 2 (2006), 297-311.\n",
        "* [7] Duda, R. O., Hart, P. E., and Stork, D. G. Pattern Classi cation (2nd Edition), 2 ed. November 2001.\n",
        "* [8] Fisher, R. A. The use of multiple measurements in taxonomic problems. Annals Eugen. 7 (1936), 179-188.\n",
        "\n",
        "### 2.3.1 アルゴリズムの説明<!-- ### 2.3.1 Algorithmic Description-->\n",
        "\n",
        "$X$ を $c$ 個のクラスから抽出したランダムベクトルとする：\n",
        "<!--Let $X$ be a random vector with samples drawn from c classes: -->\n",
        "$$\\tag{8}\n",
        "X = \\left\\{X_1,X_2,\\ldots,X_{c}\\right\\}\n",
        "$$\n",
        "$$\\tag{9}\n",
        "X_i = \\left\\{x_1, x_2,\\ldots, x_{n}\\right\\}\n",
        "$$\n",
        "\n",
        "分散行列 $S_B$ (級間分散) と $S_W$ (級内分散) は次式のように定義される：\n",
        "<!-- The scatter matrices $S_B$ and $S_W$ are defined as: -->\n",
        "$$\\tag{10}\n",
        "S_B = \\sum_{i=1}^{c} N_i (\\mu_i - \\mu)(\\mu_i - \\mu)^{\\top},\n",
        "$$\n",
        "$$\\tag{11}\n",
        "S_W = \\sum_{i=1}^{c} \\sum_{x \\in X_i} (x - \\mu_i)(x - \\mu_i)^{\\top},\n",
        "$$\n",
        "ここで $\\mu$ は全平均:<!--where $\\mu$ is the total mean:-->\n",
        "$$\\tag{12}\n",
        "\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i,\n",
        "$$\n",
        "さらに $\\mu_i$ は群内平均:<!--And $\\mu_i$ is the mean of class $i\\in\\left\\{1,\\ldots,C\\right\\}$:-->\n",
        "$$\\tag{13}\n",
        "\\mu_i = \\frac{1}{\\left|X_i\\right|} \\sum_{x_{j} \\in X_{i}} x_{j}.\n",
        "$$\n",
        "フィッシャーの古典的アルゴリズムは、クラス分離可能性基準を最大化する射影行列 $W$ を探す：\n",
        "<!-- Fisher's classic algorithm now looks for a projection matrix $W$, that maximizes the class separability criterion: -->\n",
        "$$\\tag{14}\n",
        "W_{opt}=\\arg\\max_{W} \\frac{\\left|W^{\\top} S_B W\\right|}{\\left|W^{\\top} S_W W\\right|},\n",
        "$$\n",
        "[3] に従って、この最適化問題の解は、一般固有値問題を解くことによって与えられる：\n",
        "<!-- Following[3], a solution for this optimization problem is given by solving the Genral Eigenvalue Problem: -->\n",
        "$$\\tag{15}\\begin{aligned}\n",
        "S_{B}\\nu_i &= \\lambda_i S_{W}\\nu_i,\\\\\n",
        "S_{W}^{-1} S_{B} \\nu_i &= \\lambda_i \\nu_i,\n",
        "\\end{aligned}$$\n",
        "\n",
        "解決すべき問題が 1 つ残っている：\n",
        "$S_W$ のランクは最大でも $(N-c)$ であり，$N$ 個のサンプルと $c$ 個のクラスが存在する。\n",
        "パターン認識問題では，サンプル数 $N$ は入力データの次元 (画素数) より小さいことがほとんどなので，分散行列 $S_W$ は特異行列になる ([2]を参照)。\n",
        "[3] では、データに対して主成分分析を実行し、サンプルを (N-c) 次元空間に射影することでこれを解決した。\n",
        "$S_W$ が特異でなくなったので、線形判別分析が縮小データに対して実行される。\n",
        "最適化問題は次のように書き換えられる:\n",
        "<!-- There's one problem left to solve:\n",
        "The rank of S_W is at most (N-c), with N samples and c classes.\n",
        "In pattern recognition problems the number of samples N is almost always smaller than the dimension of the input data (the number of pixels), so the scatter matrix SW becomes singular (see [2]).\n",
        "In [3] this was solved by performing a Principal Component Analysis on the data and projecting the samples int the (N-c)-dimensional space.\n",
        "A Linear Discriminant Analysis is then performed on the reduced data, becausse S_W is not singular anymore.\n",
        "The optimization problem is can be rewritten as: -->\n",
        "$$\\tag{16}\n",
        "W_{pca}=\\arg\\max_{W} \\left|W^{\\top} S_T W\\right|\n",
        "$$\n",
        "\n",
        "$$\\tag{17}\n",
        "W_{fld} = \\arg\\max_W \\frac{\\left|W^{\\top} W_{pca}^{\\top} S_B W_{pca}W\\right|}{\\left|W^{\\top} W_{pca}^{\\top}S_W W_{pca}W\\right|},\n",
        "$$\n",
        "The transformation matrix $W$, that projects a sample into the (c-1)-dimensional space is then given by:\n",
        "$$\\tag{18}\n",
        "W = W_{fld}^{\\top}W_{pca}^{\\top}.\n",
        "$$\n",
        "\n",
        "最後に注意点：\n",
        "＄S_W$ と $S_B$ とは対称行列であるが，対称行列同士の積は必ずしも対称ではないので，一般行列用の固有値ソルバを使う必要がある。\n",
        "OpenCV の `cv:eigen` は，現在のバージョンでは対称行列に対してのみ動作する。\n",
        "非対称行列に対しては，特異値の固有値は等価ではないので，特異値分解（SVD）ソルバーを利用することはできない．\n",
        "\n",
        "<!-- one final note:\n",
        "Although $S_W$ and $S_B$ are symmetric matrices, the product of two symmetric matrices is not necessarily symmetric; thus, so you have to use an eigenvalue solver for general matrices.\n",
        "OpenCV's `cv:eigen` only works for symmetric matrices in it scurrent version; since eigenvlues of singular values are not equivalent for non-symmetric matrices you can not use a Singular Value Decomposition (SVD) eigher. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5750a4",
      "metadata": {
        "id": "7f5750a4"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}