{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_1104seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42349768-cc89-4622-acf6-98978643a8d2",
      "metadata": {
        "id": "42349768-cc89-4622-acf6-98978643a8d2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "import random\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "elif torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "print(f'device:{device}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "# 乱数のシードを設定\n",
        "def init_seed(seed:int=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed=42\n",
        "init_seed(seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce1f370-27f2-418a-bf53-84cfd429a913",
      "metadata": {
        "id": "6ce1f370-27f2-418a-bf53-84cfd429a913"
      },
      "source": [
        "# データ生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6d894c5c-e7b7-413c-8092-13c24a1bbf1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d894c5c-e7b7-413c-8092-13c24a1bbf1f",
        "outputId": "656d9853-4d52-4845-dd67-1d4dec5bf880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 1, 11, 9, 2, 12] [1, 4, 3]\n",
            "inps:51+92=\n",
            "tchs:143\n"
          ]
        }
      ],
      "source": [
        "def gen_randint(_min:int=0, _max:int=1000):\n",
        "    \"\"\"_min から _max までの整数を一つ返す\"\"\"\n",
        "    _range = _max - _min\n",
        "    return np.random.randint(_range) + _min\n",
        "\n",
        "def gen_addition_data(_min:int=0, _max:int=100):\n",
        "    \"\"\"足し算問題を作成し，その答えとともに inp, tch として返す\"\"\"\n",
        "    X = gen_randint(_min=_min, _max=_max)\n",
        "    Y = gen_randint(_min=_min, _max=_max)\n",
        "    A = X + Y\n",
        "    Q = str(X)+'+'+str(Y)+'='\n",
        "    A = str(A)\n",
        "    return Q, A\n",
        "\n",
        "\n",
        "class _digit_tokenizer():\n",
        "    \"\"\"PyTorch で活用するために文字としての数字をトークン化\n",
        "    encode(), decode() を実装\"\"\"\n",
        "    def __init__(self, N:int=10000, tokens:list=list('0123456789.+= ')+['<PAD>','<SOS>','<EOS>','<UNK>']):\n",
        "        self.tokens=tokens\n",
        "\n",
        "    def encode(self, string:str):\n",
        "        ret = [self.tokens.index(ch) for ch in string]\n",
        "        return ret\n",
        "\n",
        "    def decode(self, ids):\n",
        "        ret = [self.tokens[idx] for idx in ids]\n",
        "        return ret\n",
        "\n",
        "    def __call__(self, X):\n",
        "        return self.encode(X)\n",
        "\n",
        "\n",
        "inps, tchs = gen_addition_data()\n",
        "_tokenizer = _digit_tokenizer()\n",
        "print(_tokenizer.encode(inps), _tokenizer.encode(tchs))\n",
        "print(f'inps:{inps}')\n",
        "print(f'tchs:{tchs}')\n",
        "#_tokenizer(inps), _tokenizer.decode(_tokenizer(inps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e2552de5-dda8-4a58-b6d1-7f060c61f1c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2552de5-dda8-4a58-b6d1-7f060c61f1c3",
        "outputId": "338fcb2d-13d7-4269-e2b3-0c4682ce9663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練データセット数:9,000\n",
            "検査データセット数:1,000\n"
          ]
        }
      ],
      "source": [
        "class addition_ds(torch.utils.data.Dataset):\n",
        "    \"\"\"PyTorch のデータセット：足し算の入力データと教師データを管理\"\"\"\n",
        "    def __init__(\n",
        "        self, N:int=1000, # 生成する総データ数\n",
        "        tokenizer=_tokenizer, # トークナイザ\n",
        "        generator=gen_addition_data # 問題生成器\n",
        "            ):\n",
        "        super().__init__()\n",
        "        self.tokenizer=tokenizer\n",
        "        inps, tchs = [], []\n",
        "        for _ in range(N):\n",
        "            inp, tch = generator()\n",
        "            inps.append(inp)\n",
        "            tchs.append(tch)\n",
        "\n",
        "        self.inps = inps\n",
        "        self.tchs = tchs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inps)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp = self.tokenizer(self.inps[idx])\n",
        "        tch = [self.tokenizer.tokens.index('<SOS>')]+self.tokenizer(self.tchs[idx])+[self.tokenizer.tokens.index('<EOS>')]\n",
        "        return torch.tensor(inp), torch.tensor(tch)\n",
        "\n",
        "# データセットの定義\n",
        "ds = addition_ds(N=10000)\n",
        "#ds.__len__(), ds.__getitem__(99), ds.tokenizer.decode(ds.__getitem__(99)[0])\n",
        "\n",
        "# 定義したデータセットを分割\n",
        "N_train = int (ds.__len__() * 0.9)\n",
        "N_test  = ds.__len__() - N_train\n",
        "train_ds, test_ds = torch.utils.data.random_split(dataset=ds, lengths=(N_train, N_test), generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts\n",
        "\n",
        "# ミニバッチ処理を管理するデータローダ\n",
        "batch_size = 512\n",
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, collate_fn=_collate_fn)\n",
        "test_dl  = torch.utils.data.DataLoader(dataset=test_ds,  batch_size=8, shuffle=False, collate_fn=_collate_fn)\n",
        "print(f'訓練データセット数:{train_ds.__len__():,d}')\n",
        "print(f'検査データセット数:{test_ds.__len__():,d}')\n",
        "#next(iter(train_dl))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd8c388-483b-4400-860d-56c33af95368",
      "metadata": {
        "id": "2cd8c388-483b-4400-860d-56c33af95368"
      },
      "source": [
        "# Encoder, Decoder の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "52ce286f-c5eb-46fa-9939-7ed11d907b48",
      "metadata": {
        "id": "52ce286f-c5eb-46fa-9939-7ed11d907b48"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Encoderクラス\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_vocab:int, n_emb:int, n_hid:int,\n",
        "                 num_layers:int=1, dropout:float=0.5,\n",
        "                 padding_idx:int=_tokenizer.tokens.index('<PAD>')):\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_hid = n_hid\n",
        "        self.emb_layer = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=n_emb,\n",
        "            padding_idx=padding_idx)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=n_emb,\n",
        "            hidden_size=n_hid,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True)\n",
        "\n",
        "    def forward(self, inp:int):\n",
        "        emb = self.emb_layer(inp)\n",
        "        out, state = self.rnn(emb)\n",
        "        return out, state\n",
        "\n",
        "# Decoderクラス\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_vocab:int, n_emb:int, n_hid:int,\n",
        "        num_layers:int=1, dropout:float=0.5,\n",
        "        padding_idx:int=_tokenizer.tokens.index('<PAD>')):\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_hid = n_hid\n",
        "        self.emb_layer = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=n_emb,\n",
        "            padding_idx=padding_idx)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=n_emb,\n",
        "            hidden_size=n_hid,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=n_hid, out_features=n_vocab)\n",
        "\n",
        "    def forward(self, inp, enc_state):\n",
        "        emb = self.emb_layer(inp)\n",
        "        out, state = self.rnn(emb, enc_state) # 第２戻り値は推論時に次の文字を生成するときに使う\n",
        "        out = self.fc(out)\n",
        "        return out, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "314293fa-2365-446c-9bf9-ccef02ce0367",
      "metadata": {
        "id": "314293fa-2365-446c-9bf9-ccef02ce0367"
      },
      "outputs": [],
      "source": [
        "# #n_emb, n_hid = 64, 64     # 文字の埋め込み次元数\n",
        "# #n_emb, n_hid = 32, 32   # 文字の埋め込み次元数\n",
        "# #n_emb, n_hid =  128, 128  # 文字の埋め込み次元数\n",
        "# n_emb, n_hid = 256, 256   # 文字の埋め込み次元数\n",
        "# #n_emb, n_hid = 512, 512   # 文字の埋め込み次元数\n",
        "\n",
        "# n_vocab = len(_tokenizer.tokens) # 扱う文字の数。今回は 18 文字\n",
        "# num_layers, dropout = 2, 0.5\n",
        "# num_layers, dropout = 3, 0.25\n",
        "# # num_layers, dropout = 3, 0.5\n",
        "# # num_layers, dropout = 2, 0.01\n",
        "# #num_layers, dropout = 1, 0.0\n",
        "\n",
        "# encoder = Encoder(n_vocab, n_emb, n_hid, num_layers=num_layers, dropout=dropout).to(device)\n",
        "# decoder = Decoder(n_vocab, n_emb, n_hid, num_layers=num_layers, dropout=dropout).to(device)\n",
        "\n",
        "class _Seq2Seq(torch.nn.Module):\n",
        "    def __init__(self, Encoder:torch.nn.Module=None, Decoder:torch.nn.Module=None,\n",
        "    # def __init__(self, Encoder:nn.Module=encoder, Decoder:nn.Module=decoder,\n",
        "                 n_vocab:int=0, n_emb:int=0, n_hid:int=0, num_layers:int=1, dropout:float=0.0,\n",
        "                 padding_idx=_tokenizer.tokens.index('<PAD>')):\n",
        "\n",
        "        super().__init__()\n",
        "        if Encoder == None:\n",
        "            self.encoder = self.Encoder(n_vocab, n_emb, n_hid, num_layers=num_layers, dropout=dropout).to(device)\n",
        "        else:\n",
        "            self.encoder = Encoder\n",
        "        if Decoder == None:\n",
        "            self.decoder = self.Decoder(n_vocab, n_emb, n_hid, num_layers=num_layers, dropout=dropout).to(device)\n",
        "        else:\n",
        "            self.decoder = Decoder\n",
        "\n",
        "        self.n_vocab=n_vocab\n",
        "        self.n_emb=n_emb\n",
        "        self.n_hid=n_hid\n",
        "        self.padding_idx=padding_idx\n",
        "        self.num_layers=num_layers\n",
        "\n",
        "    def forward(self, inps, tchs):\n",
        "        enc_out, enc_state = self.encoder(inps)\n",
        "        dec_out, dec_state = self.decoder(tchs, enc_state)\n",
        "        return dec_out, dec_state\n",
        "\n",
        "    # Encoderクラス\n",
        "    class Encoder(torch.nn.Module):\n",
        "        def __init__(\n",
        "            self, n_vocab:int, n_emb:int, n_hid:int, num_layers:int=1, dropout:float=0.5,\n",
        "                 padding_idx:int=_tokenizer.tokens.index('<PAD>')):\n",
        "\n",
        "            super().__init__()\n",
        "            self.n_hid = n_hid\n",
        "            self.emb_layer = torch.nn.Embedding(num_embeddings=n_vocab, embedding_dim=n_emb, padding_idx=padding_idx)\n",
        "            self.rnn = torch.nn.LSTM(input_size=n_emb, hidden_size=n_hid, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "            self.fc = torch.nn.Linear(in_features=n_hid, out_features=n_vocab)\n",
        "\n",
        "        def forward(self, inp:int):\n",
        "            emb = self.emb_layer(inp)\n",
        "            out, state = self.rnn(emb)\n",
        "            out = self.fc(out)\n",
        "            return out, state\n",
        "\n",
        "    # Decoderクラス\n",
        "    class Decoder(torch.nn.Module):\n",
        "        def __init__(\n",
        "            self, n_vocab:int, n_emb:int, n_hid:int, num_layers:int=1, dropout:float=0.5,\n",
        "            padding_idx:int=_tokenizer.tokens.index('<PAD>')):\n",
        "\n",
        "            super().__init__()\n",
        "            self.n_hid = n_hid\n",
        "            self.emb_layer = torch.nn.Embedding(num_embeddings=n_vocab, embedding_dim=n_emb, padding_idx=padding_idx)\n",
        "            self.rnn = torch.nn.LSTM(input_size=n_emb, hidden_size=n_hid, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "            self.fc = torch.nn.Linear(in_features=n_hid, out_features=n_vocab)\n",
        "\n",
        "        def forward(self, inp, enc_state):\n",
        "            emb = self.emb_layer(inp)\n",
        "            out, state = self.rnn(emb, enc_state) # 第２戻り値は推論時に次の文字を生成するときに使う\n",
        "            out = self.fc(out)\n",
        "            return out, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5e7a01b9-5dd5-4d40-ae97-6752b798a358",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e7a01b9-5dd5-4d40-ae97-6752b798a358",
        "outputId": "1aa53779-a8c6-4f88-ca96-ed83308aa51e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (emb_layer): Embedding(18, 256, padding_idx=14)\n",
              "    (rnn): LSTM(256, 256, num_layers=2, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=18, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (emb_layer): Embedding(18, 256, padding_idx=14)\n",
              "    (rnn): LSTM(256, 256, num_layers=2, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=18, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 損失関数\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "n_emb, n_hid = 256, 256   # 文字の埋め込み次元数\n",
        "#n_emb, n_hid = 512, 512   # 文字の埋め込み次元数\n",
        "\n",
        "n_vocab = len(_tokenizer.tokens) # 扱う文字の数\n",
        "num_layers, dropout = 2, 0.5\n",
        "num_layers, dropout = 2, 0.0\n",
        "# num_layers, dropout = 3, 0.25\n",
        "#num_layers, dropout = 3, 0.5\n",
        "# num_layers, dropout = 2, 0.01\n",
        "# num_layers, dropout = 1, 0.0\n",
        "\n",
        "#seq2seq = _Seq2Seq()\n",
        "#seq2seq = _Seq2Seq(Encoder=Encoder, Decoder=Decoder, n_vocab=n_vocab, n_emb=n_emb, n_hid=n_hid, num_layers=num_layers, dropout=dropout).to(device)\n",
        "seq2seq = _Seq2Seq(Encoder=None, Decoder=None, n_vocab=n_vocab, n_emb=n_emb, n_hid=n_hid, num_layers=num_layers, dropout=dropout).to(device)\n",
        "_model = seq2seq\n",
        "\n",
        "# 最適化関数の定義\n",
        "lr = 1e-2\n",
        "_optimizer = torch.optim.Adam(_model.parameters(), lr=lr)\n",
        "# encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
        "# decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc09b02e-22df-4a8f-ac54-1f7ec75dfeaa",
      "metadata": {
        "id": "bc09b02e-22df-4a8f-ac54-1f7ec75dfeaa"
      },
      "source": [
        "## fit() の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa780c94-fced-4354-bd83-a6c1ce072c77",
      "metadata": {
        "id": "fa780c94-fced-4354-bd83-a6c1ce072c77"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def count_correct(outs:torch.Tensor, tchs:torch.Tensor):\n",
        "    _nc, _n = 0, 0\n",
        "    outs = outs.argmax(dim=-1).detach().cpu().numpy()\n",
        "    tchs = tchs.detach().cpu().numpy()\n",
        "    for a, b in zip(outs, tchs):\n",
        "        a, b = a[a<10], b[b<10]\n",
        "        yesno = False\n",
        "        if len(a) != len(b):\n",
        "            # print(f'a:{a}, b:{b}')\n",
        "            continue\n",
        "        else:\n",
        "            yesno = np.array((a == b) * 1).sum() == len(b)\n",
        "        _nc += 1 if yesno else 0\n",
        "\n",
        "    _n = len(tchs)\n",
        "    return _nc, _n\n",
        "\n",
        "def _fit(\n",
        "    _model:torch.nn.Module=None, _optimizer:torch.optim=None, train_dl=train_dl, test_dl=test_dl,\n",
        "    epochs:int=200, interval:int=10,\n",
        "    train_losses:list=[], test_losses:list=[], train_crs:list=[], test_crs:list=[]):\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "\n",
        "        train_epoch_loss = 0  # epoch 毎の 損失値\n",
        "        _model.train()\n",
        "        for inps, tchs in train_dl:\n",
        "            _model.zero_grad()    # 勾配の初期化\n",
        "            _optimizer.zero_grad()\n",
        "\n",
        "            # 系列長を揃える\n",
        "            inps = pad_sequence(inps, batch_first=True, padding_value=_tokenizer.tokens.index('<PAD>')).to(device)\n",
        "            tchs = pad_sequence(tchs, batch_first=True, padding_value=_tokenizer.tokens.index('<PAD>')).to(device)\n",
        "\n",
        "            dec_out, dec_state = _model(inps, tchs)  # 順伝搬\n",
        "            loss = 0\n",
        "            for j in range(dec_out.size()[1]-1):\n",
        "                loss += criterion(dec_out[:, j, :], tchs[:, j+1])\n",
        "            train_epoch_loss += loss.item()\n",
        "            loss.backward()                           # 誤差逆伝播\n",
        "            _optimizer.step()                         # パラメータ更新\n",
        "        train_losses.append(train_epoch_loss)\n",
        "\n",
        "        test_epoch_loss = 0.\n",
        "        _model.eval()\n",
        "        for inps, tchs in test_dl:\n",
        "            inps = pad_sequence(inps, batch_first=True, padding_value=_tokenizer.tokens.index('<PAD>')).to(device)\n",
        "            tchs = pad_sequence(tchs, batch_first=True, padding_value=_tokenizer.tokens.index('<PAD>')).to(device)\n",
        "\n",
        "            dec_out, dec_state = _model(inps, tchs)\n",
        "            loss = 0\n",
        "            for j in range(dec_out.size()[1]-1):\n",
        "                loss += criterion(dec_out[:, j, :], tchs[:, j+1])\n",
        "            test_epoch_loss += loss.item()\n",
        "        test_losses.append(test_epoch_loss)\n",
        "\n",
        "        _model.eval()\n",
        "        Nc, N = 0, 0\n",
        "        for inps, tchs in train_dl:\n",
        "            inps = pad_sequence(inps, batch_first=True, padding_value=_tokenizer.tokens.index('<PAD>')).to(device)\n",
        "            tchs = pad_sequence(tchs, batch_first=True, padding_value=_tokenizer.tokens.index('<PAD>')).to(device)\n",
        "            dec_out, dec_state = _model(inps, tchs)\n",
        "            _Nc, _N = count_correct(dec_out, tchs)\n",
        "            Nc += _Nc\n",
        "            N += _N\n",
        "        train_crs.append((Nc,N))\n",
        "\n",
        "        Nc, N = 0, 0\n",
        "        for inps, tchs in test_dl:\n",
        "            inps = pad_sequence(inps, batch_first=True, padding_value=_tokenizer.tokens.index('<PAD>')).to(device)\n",
        "            tchs = pad_sequence(tchs, batch_first=True, padding_value=_tokenizer.tokens.index('<PAD>')).to(device)\n",
        "            dec_out, dec_state = _model(inps, tchs)\n",
        "            _Nc, _N = count_correct(dec_out, tchs)\n",
        "            Nc += _Nc\n",
        "            N += _N\n",
        "        test_crs.append((Nc,N))\n",
        "\n",
        "        # 損失を表示\n",
        "        if ((epoch % interval) == 0) or (epoch == 1):\n",
        "            print(f\"Epoch {epoch:03d}:\", f\"訓練損失:{train_epoch_loss:7.3f}\", f\"検査損失:{test_epoch_loss:7.3f}\",\n",
        "                  f\"訓練データ正解率:{train_crs[-1][0]/train_crs[-1][1]:.02f}\", f\"検査データ正解率:{test_crs[-1][0]/test_crs[-1][1]:.02f}\")\n",
        "        if train_epoch_loss < 0.01:\n",
        "            break\n",
        "\n",
        "    return _model, _optimizer, train_losses, test_losses, train_crs, test_crs\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "n_emb, n_hid = 256, 256   # 文字の埋め込み次元数\n",
        "n_emb, n_hid = 32, 128   # 文字の埋め込み次元数\n",
        "n_emb, n_hid = 64, 256    # 文字の埋め込み次元数\n",
        "#n_emb, n_hid = 512, 512   # 文字の埋め込み次元数\n",
        "n_emb, n_hid = 64, 512\n",
        "\n",
        "n_vocab = len(_tokenizer.tokens) # 扱う文字の数。\n",
        "num_layers, dropout = 2, 0.1\n",
        "#num_layers, dropout = 3, 0.5\n",
        "#num_layers, dropout = 2, 0.0\n",
        "# num_layers, dropout = 3, 0.25\n",
        "#num_layers, dropout = 3, 0.5\n",
        "# num_layers, dropout = 2, 0.01\n",
        "num_layers, dropout = 2, 0.25\n",
        "#num_layers, dropout = 2, 0.5\n",
        "\n",
        "seq2seq = _Seq2Seq(Encoder=None, Decoder=None, n_vocab=n_vocab, n_emb=n_emb, n_hid=n_hid, num_layers=num_layers, dropout=dropout).to(device)\n",
        "_model = seq2seq\n",
        "\n",
        "# 最適化関数の定義\n",
        "lr = 1e-3\n",
        "_optimizer = torch.optim.Adam(_model.parameters(), lr=lr)\n",
        "_model.eval()\n",
        "_model, _optimizer, train_losses, test_losses, train_crs, test_crs = _fit(\n",
        "    _model=_model, _optimizer=_optimizer, train_dl=train_dl, test_dl=test_dl, epochs=20, interval=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c40ad46b-d35d-4063-be7d-ae1877beb55b",
      "metadata": {
        "id": "c40ad46b-d35d-4063-be7d-ae1877beb55b"
      },
      "outputs": [],
      "source": [
        "_model, _optimizer, train_losses, test_losses, train_crs, test_crs = _fit(\n",
        "    _model=_model, _optimizer=_optimizer, train_dl=train_dl, test_dl=test_dl, epochs=300, interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a151dd77-732a-4170-9569-d6659b1360a9",
      "metadata": {
        "id": "a151dd77-732a-4170-9569-d6659b1360a9"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label=\"訓練損失\")\n",
        "plt.plot(test_losses, label=\"検査損失\")\n",
        "plt.xlabel('エポック')\n",
        "plt.title(f'dropout:{dropout:.2f}, n_hid:{n_hid}, num_layers:{num_layers}, lr:{lr:.3f}')\n",
        "plt.legend()\n",
        "#plt.xkcd()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}