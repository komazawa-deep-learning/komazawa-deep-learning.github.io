{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOUEnA8tnheRCLXzKWxZXGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_0926tiktoken_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9JREPqu5ZZM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import tiktoken\n",
        "except ImportError:\n",
        "    !pip install tiktoken\n",
        "    import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# モデルに対応したエンコーディングを取得\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "# テキストをトークン化してカウント\n",
        "inp = \"吾輩は猫である。名前はまだ無い。\"\n",
        "inp_ids = tokenizer.encode(inp)\n",
        "\n",
        "print(f\"入力文: {inp}\")\n",
        "print(f\"トークン数: {len(inp_ids)}\")\n",
        "print(f\"トークン IDs: {inp_ids}\")\n",
        "print(f\"逆変換: {tokenizer.decode(token_ids)}\")\n"
      ],
      "metadata": {
        "id": "2pBMYaEq5flM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 例えばどのように変換されているかを見てみる\n",
        "tokenizer.decode([7305,122]), tokenizer.decode([164,120,102])"
      ],
      "metadata": {
        "id": "gIh-HYeV5tzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer  には以下のような関数や定数などが定義されている\n",
        "for k in dir(tokenizer):\n",
        "    if not str(k).startswith('_'):\n",
        "        print(k)"
      ],
      "metadata": {
        "id": "6sH8U3hD6GZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# さらに例示\n",
        "tokenizer.n_vocab"
      ],
      "metadata": {
        "id": "iy_rLGGc63gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 利用可能なエンコーディング一覧\n",
        "print(\"利用可能なエンコーディング:\")\n",
        "for encoding_name in tiktoken.list_encoding_names():\n",
        "    print(f\"  - {encoding_name}\")\n"
      ],
      "metadata": {
        "id": "RdJ0NASZ7Mo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルごとの対応\n",
        "model_encodings = {\n",
        "    \"gpt-4\": \"cl100k_base\",\n",
        "    \"gpt-3.5-turbo\": \"cl100k_base\",\n",
        "    \"text-embedding-ada-002\": \"cl100k_base\",\n",
        "    \"gpt-2\": \"gpt2\",\n",
        "}\n",
        "\n",
        "# 特定のエンコーディングを直接使用\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n"
      ],
      "metadata": {
        "id": "58IKfBQa7rs1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}