{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_1017olivetti_face_LSTM_SRN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5-amTpixBS_o",
      "metadata": {
        "id": "5-amTpixBS_o"
      },
      "source": [
        "# ã‚ªãƒªãƒ™ãƒƒãƒ†ã‚£é¡”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’å®Ÿç¿’\n",
        "\n",
        "* filename: 2025_1017olivetti_face_LSTM_SRN.ipynb\n",
        "* author: æµ…å· ä¼¸ä¸€ asakawa@ieee.org\n",
        "* date: 2025_1010\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0956f10b-f104-44e8-bf7d-8f516b27b3c8",
      "metadata": {
        "id": "0956f10b-f104-44e8-bf7d-8f516b27b3c8"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "import numpy as np\n",
        "# numpy ã®è¡¨ç¤ºæ¡è¨­å®š\n",
        "np.set_printoptions(precision=5, suppress=False)\n",
        "\n",
        "# ã‚ªãƒªãƒ™ãƒƒãƒ†ã‚£é¡”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "data = fetch_olivetti_faces()\n",
        "X, y = data.data, data.target\n",
        "print(f'è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º X.shape:{X.shape}')\n",
        "print(f'æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º y.shape:{y.shape}')\n",
        "print(data.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a06cd9f-036f-4b7c-8c90-80444efe935b",
      "metadata": {
        "id": "9a06cd9f-036f-4b7c-8c90-80444efe935b"
      },
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º\n",
        "nrows = 40   # nrows äººåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
        "ncols = 10\n",
        "fig, fig_axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 1.4, nrows * 1.4), constrained_layout=True)\n",
        "# constrained_layout ã¯ subplot ã‚„ å‡¡ä¾‹ã‚„ã‚«ãƒ©ãƒ¼ãƒãƒ¼ãªã©ã®è£…é£¾ã‚’è‡ªå‹•çš„ã«èª¿æ•´ã—ã¦ï¼Œ\n",
        "# ãƒ¦ãƒ¼ã‚¶ãŒè¦æ±‚ã™ã‚‹è«–ç†çš„ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ã§ãã‚‹ã ã‘ç¶­æŒã—ãªãŒã‚‰ï¼Œ å›³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«åã¾ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
        "\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        x = i * 10 + j\n",
        "        fig_axes[i][j].imshow(X[x].reshape(64,64), cmap='gray')\n",
        "        fig_axes[i][j].axis('off')\n",
        "        fig_axes[i][j].set_title(f'num:{x}, ID:{y[x]}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bNppGAoBhAp",
      "metadata": {
        "id": "1bNppGAoBhAp"
      },
      "outputs": [],
      "source": [
        "# ç”·å¥³ã®åˆ¤åˆ¥ã®ãŸã‚æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
        "# ç”·ã§ã‚ã‚Œã° 0, å¥³ã§ã‚ã‚Œã° 1 ã¨ã™ã‚‹\n",
        "y_sex = np.zeros_like(y)\n",
        "for woman_start in [70, 90, 310, 340]:\n",
        "    for i in range(10):\n",
        "        y_sex[woman_start+i]=1\n",
        "\n",
        "for i in range(0, len(X),10):\n",
        "    print(f'{i:3d}', end=\":\")\n",
        "    for j in range(10):\n",
        "        print(y_sex[i+j], end=\" \")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d665e752-c7d5-48dd-9cbc-622e143046ca",
      "metadata": {
        "id": "d665e752-c7d5-48dd-9cbc-622e143046ca"
      },
      "source": [
        "# ç³»åˆ—æƒ…å ±å‡¦ç†å®Ÿç¿’\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ 400 æšã®ç”»åƒã¯ãã‚Œãã‚Œç¸¦æ¨ª 64 ç”»ç´ ã§ã‚ã‚‹ã€‚ã“ã‚Œã‚’ 1 æ¬¡å…ƒã«ã™ã‚Œã° 1 æšã®ç”»åƒã¯ 4096 æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹ã€‚\n",
        "ã—ãŸãŒã£ã¦ï¼Œç³»åˆ—äºˆæ¸¬èª²é¡Œã¨ã¿ãªã™ã“ã¨ã‚‚ã§ãã‚‹ã€‚\n",
        "\n",
        "ä»¥ä¸‹ã§ã¯ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ¬¡å…ƒç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¦–è¦šåŒ–ã‚’è¡Œã†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34748e56-f5d1-45ea-b88e-b5ae2de7d773",
      "metadata": {
        "id": "34748e56-f5d1-45ea-b88e-b5ae2de7d773"
      },
      "outputs": [],
      "source": [
        "# è¡¨ç¤ºã•ã›ãŸã„ç”»åƒç•ªå·ã‚’æŒ‡å®šã™ã‚‹ãƒªã‚¹ãƒˆ\n",
        "image_nums = [0, 10, 20, 30]\n",
        "image_nums = [70, 90, 310, 340]\n",
        "\n",
        "figs, axes = plt.subplots(len(image_nums), 2, figsize=(12, 1.5 * len(image_nums)), tight_layout=False)\n",
        "for i, image_num in enumerate(image_nums):\n",
        "    x = X[image_num]\n",
        "\n",
        "    #figs, axes = plt.subplots(1,2, figsize=(12,3), tight_layout=False)\n",
        "    #figs.suptitle(f'ãƒ‡ãƒ¼ã‚¿ç•ªå·:{N}')\n",
        "\n",
        "    axes[i][0].plot(x)\n",
        "    axes[i][0].set_ylim(0,1)\n",
        "\n",
        "    axes[i][1].imshow(x.reshape(64,64), cmap='gray')\n",
        "    axes[i][1].set_xticks([])\n",
        "    axes[i][1].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031c1057-2f8a-4958-bcac-ef4bae613ff1",
      "metadata": {
        "id": "031c1057-2f8a-4958-bcac-ef4bae613ff1"
      },
      "source": [
        "# SRN, LSTM ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹è¿‘ä¼¼"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ PyTorch ã§ä½¿ãˆã‚‹ã‚ˆã†ãªå½¢å¼ã®ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›\n",
        "import torch\n",
        "\n",
        "class _dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X:np.array=X, y:np.array=y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx:int):\n",
        "        inp = torch.Tensor(self.X[idx].reshape(64,64))\n",
        "        tch = self.y[idx]\n",
        "        return inp, tch\n",
        "\n",
        "_ds = _dataset(X=X, y=y)       # ã“ã¡ã‚‰ã¯äººç‰©åŒå®š\n",
        "#_ds = _dataset(X=X, y=y_sex)  # æ€§åˆ¥åˆ¤å®š\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ã«åˆ†å‰²\n",
        "N_train = int(_ds.__len__() * 0.9)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°\n",
        "N_val = _ds.__len__() - N_train     # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ•° = å…¨ãƒ‡ãƒ¼ã‚¿ - è¨“ç·´ãƒ‡ãƒ¼ã‚¿\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
        "train_ds, val_ds = torch.utils.data.random_split(\n",
        "    dataset=_ds,\n",
        "    lengths=(N_train,N_val), generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# ä¸Šã§åˆ†å‰²ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã«å¤‰æ›\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_dl   = torch.utils.data.DataLoader(val_ds,   batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "KXcFoXyo3sgP"
      },
      "id": "KXcFoXyo3sgP",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class _SRN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim # ä¸­é–“å±¤ã®æ¬¡å…ƒæ•°\n",
        "        self.layer_dim = layer_dim   # ä¸­é–“å±¤ã®å±¤æ•°\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hid = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
        "        out, hid = self.rnn(x, hid)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "class _LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim # ä¸­é–“å±¤ã®æ¬¡å…ƒæ•°\n",
        "        self.layer_dim = layer_dim   # ä¸­é–“å±¤ã®å±¤æ•°\n",
        "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
        "        h1 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
        "        hid = (h0, h1)\n",
        "        out, hn = self.rnn(x, hid)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "input_dim = 64    # å…¥åŠ›å±¤ã®æ¬¡å…ƒæ•°\n",
        "hidden_dim = 256  # ä¸­é–“å±¤ã®æ¬¡å…ƒæ•°\n",
        "layer_dim = 2     # ä¸­é–“å±¤ã®å±¤æ•°\n",
        "output_dim = 40    # å‡ºåŠ›å±¤ã®æ¬¡å…ƒæ•°\n",
        "#output_dim = 2    # å‡ºåŠ›å±¤ã®æ¬¡å…ƒæ•°\n",
        "\n",
        "model0 = _SRN(input_dim, hidden_dim, layer_dim, output_dim)\n",
        "model1 = _LSTM(input_dim, hidden_dim, layer_dim, output_dim)\n",
        "error = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer0 = torch.optim.Adam(model0.parameters(), lr=learning_rate)\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "jywRHvRN2BPA"
      },
      "id": "jywRHvRN2BPA",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_dim = 64\n",
        "num_epochs = 50\n",
        "\n",
        "model = model1\n",
        "optimizer = optimizer1\n",
        "# model = model0\n",
        "# optimizer = optimizer0\n",
        "\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "count = 0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    losses = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    model.eval()\n",
        "    for inps, tchs in val_dl:\n",
        "        outs = model(inps)\n",
        "        preds = torch.max(outs.data, 1)[1] # Get predictions from the maximum value\n",
        "        total += tchs.size(0)\n",
        "        correct += (preds == tchs).sum()\n",
        "\n",
        "    accuracy = 100 * correct / float(total)\n",
        "    iteration_list.append(epoch+1)\n",
        "\n",
        "    model.train()\n",
        "    for i, (inps, tchs) in enumerate(train_dl):\n",
        "        optimizer.zero_grad()\n",
        "        outs = model(inps)\n",
        "        loss = error(outs, tchs)\n",
        "        loss.backward() # Calculating gradients\n",
        "        optimizer.step() # Update parameters\n",
        "        losses += loss.item()\n",
        "\n",
        "    print(f'epoch:{epoch:03d}  Loss: {losses:8.3f}  Accuracy: {accuracy:6.3f}%')\n",
        "    accuracy_list.append(accuracy)\n",
        "    loss_list.append(losses) # store loss and iteration"
      ],
      "metadata": {
        "id": "fm7x7HgE8kml"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fm7x7HgE8kml"
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization loss\n",
        "plt.plot(iteration_list,loss_list)\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"RNN: Loss vs Number of iteration\")\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy\n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
        "#plt.savefig('graph.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cvvv6MsVEbFA"
      },
      "id": "cvvv6MsVEbFA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e47b2258-a955-4834-98d1-a67ceaa10040",
      "metadata": {
        "id": "e47b2258-a955-4834-98d1-a67ceaa10040"
      },
      "outputs": [],
      "source": [
        "# ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©\n",
        "class _SRN(nn.Module):\n",
        "    \"\"\"å˜ç´”å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ a.k.a. ã‚¨ãƒ«ãƒãƒ³ãƒãƒƒãƒˆ\"\"\"\n",
        "    def __init__(self, input_size=1, hidden_size=16, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)         # RNN output for all time steps\n",
        "        out = out[:, -1, :]          # Take output from the last time step\n",
        "        return self.fc(out)          # Pass through linear layer\n",
        "\n",
        "\n",
        "class _LSTM(nn.Module):\n",
        "    \"\"\"é•·â€çŸ­æœŸè¨˜æ†¶ LSTM: Long-Short Term Memory ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\"\"\"\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)         # RNN output for all time steps\n",
        "        out = out[:, -1, :]          # Take output from the last time step\n",
        "        return self.fc(out)          # Pass through linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f323d14c-0705-4699-b5e3-ddd2de50f483",
      "metadata": {
        "id": "f323d14c-0705-4699-b5e3-ddd2de50f483"
      },
      "outputs": [],
      "source": [
        "# ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ PyTorch ã§ä½¿ãˆã‚‹ã‚ˆã†ãªå½¢å¼ã®ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›\n",
        "class seq_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data:np.array, seq_len:int=10):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "        _X, _y = self.make_sequences(data)\n",
        "        self._X = _X\n",
        "        self._y = _y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._X)\n",
        "\n",
        "    def __getitem__(self, idx:int):\n",
        "        inp = torch.Tensor(self._X[idx]).reshape(-1,1)\n",
        "        tch = torch.Tensor([self._y[idx]])\n",
        "        return inp, tch\n",
        "\n",
        "    def make_sequences(self, data:np.array):\n",
        "        xs, ys = [], []\n",
        "        for i in range(len(data) - self.seq_len):\n",
        "            x = data[i:i+self.seq_len] # Sequence of length `seq_length`\n",
        "            y = data[i+self.seq_len]   # Target is the next value\n",
        "            xs.append(x)\n",
        "            ys.append(y)\n",
        "        return np.array(xs), np.array(ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b40c31-818a-4179-9888-f5a63a1e0d8e",
      "metadata": {
        "id": "f7b40c31-818a-4179-9888-f5a63a1e0d8e"
      },
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿ã®é¸æŠ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398f5015-4d8e-4641-8310-4e05cde0de9b",
      "metadata": {
        "id": "398f5015-4d8e-4641-8310-4e05cde0de9b"
      },
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’é¸ã¶\n",
        "N = 70  # N ã®å€¤ã¯ 0 ã‹ã‚‰ 399 ã¾ã§ã®æ•´æ•°ï¼Œã‚ªãƒªãƒ™ãƒƒãƒ†ã‚£é¡”ç”»åƒãƒ‡ãƒ¼ã‚¿ã®åˆºæ¿€ç•ªå·\n",
        "data = X[N]\n",
        "s_ds = seq_dataset(data=data)\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ã«åˆ†å‰²\n",
        "N_train = int(s_ds.__len__() * 0.9)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°\n",
        "N_val = s_ds.__len__() - N_train     # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ•° = å…¨ãƒ‡ãƒ¼ã‚¿ - è¨“ç·´ãƒ‡ãƒ¼ã‚¿\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
        "train_ds, val_ds = torch.utils.data.random_split(\n",
        "    dataset=s_ds,\n",
        "    lengths=(N_train,N_val), generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# ä¸Šã§åˆ†å‰²ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã«å¤‰æ›\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_dl   = torch.utils.data.DataLoader(val_ds,   batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "9wcp0mLbg3gI"
      },
      "id": "9wcp0mLbg3gI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«ã®å®Ÿä½“åŒ–ã¨è¨“ç·´ã®å®Ÿæ–½:\n",
        "\n",
        "ä»¥ä¸‹ã®ã‚»ãƒ« 2 è¡Œç›®ã® hidden_size ã‚’å¤‰åŒ–ã•ã›ã¦æ€§èƒ½ã‚’æ¯”è¼ƒã›ã‚ˆ"
      ],
      "metadata": {
        "id": "9cPbRp5Qg_lu"
      },
      "id": "9cPbRp5Qg_lu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e679768-9df6-4866-bf6d-7b397034df12",
      "metadata": {
        "id": "8e679768-9df6-4866-bf6d-7b397034df12"
      },
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿä½“åŒ–\n",
        "hidden_size = 8  # ä¸­é–“å±¤ã®ç´ å­æ•°\n",
        "model0 = _SRN(hidden_size=hidden_size);model0.eval()\n",
        "model1 = _LSTM(hidden_size=hidden_size);model1.eval()\n",
        "\n",
        "# æå¤±é–¢æ•°ã®å®šç¾©: MSE å¹³å‡äºŒä¹—èª¤å·® Mean Square Error\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# å­¦ç¿’ã«ç”¨ã„ã‚‹æœ€é©åŒ–é–¢æ•°ã®å®šç¾©\n",
        "lr = 1e-2  # lr: learning rate\n",
        "optimizer0 = optim.Adam(model0.parameters(), lr=lr)\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=lr)\n",
        "\n",
        "# å®Ÿç¿’ç”¨ï¼Œä¸Šã§å®šç¾©ã—ãŸ SRN ã‹ LSTM ã‹ã‚’é¸æŠã™ã‚‹\n",
        "_model = model0\n",
        "_optimizer = optimizer0\n",
        "\n",
        "\n",
        "EPOCHS = 30                        # è¨“ç·´å›æ•°ã‚’è¨­å®š\n",
        "train_losses, val_losses = [], []  # è¨“ç·´æå¤±å€¤ã¨æ¤œè¨¼æå¤±å€¤ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ç©ºãƒªã‚¹ãƒˆã‚’å®šç¾©\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    _model.eval()\n",
        "    losses = []\n",
        "    for inps, tchs in val_dl:\n",
        "        outs = _model(inps)\n",
        "        loss = criterion(outs, tchs)\n",
        "        losses.append(loss.item())\n",
        "    val_losses.append(np.mean(losses))\n",
        "\n",
        "    _model.train()\n",
        "    losses = []\n",
        "    for inps, tchs in train_dl:\n",
        "        outs = _model(inps)\n",
        "        loss = criterion(outs, tchs)\n",
        "\n",
        "        _optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        _optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    train_losses.append(np.mean(losses))\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch [{epoch+1:03d}/{EPOCHS:03d}]\",\n",
        "              f'è¨“ç·´æå¤±:{train_losses[-1]:8.4f}',\n",
        "              f'æ¤œè¨¼æå¤±:{val_losses[-1]:8.4f}' )\n",
        "\n",
        "# å­¦ç¿’æ›²ç·šã‚’æç”»\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(train_losses, label='è¨“ç·´èª¤å·®')\n",
        "plt.plot(val_losses, label='æ¤œè¨¼èª¤å·®')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224b3514-0207-4b69-b95e-84254ed7c80d",
      "metadata": {
        "id": "224b3514-0207-4b69-b95e-84254ed7c80d"
      },
      "outputs": [],
      "source": [
        "# ç”»åƒã®å¾©å…ƒ\n",
        "_model.eval()\n",
        "y_hats, ys = [], []\n",
        "for i in range(s_ds.__len__()):\n",
        "    x, y = s_ds.__getitem__(i)\n",
        "    y_hat = _model(x.unsqueeze(0))\n",
        "    y_hats.append(y_hat.cpu().detach().numpy()[0])\n",
        "    ys.append(y.cpu().detach().numpy()[0])\n",
        "\n",
        "y_hats, ys = np.array(y_hats), np.array(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145a0873-b1f4-4dc9-958b-e20dc8760655",
      "metadata": {
        "id": "145a0873-b1f4-4dc9-958b-e20dc8760655"
      },
      "outputs": [],
      "source": [
        "from matplotlib import gridspec\n",
        "\n",
        "fig = plt.figure(figsize=(12, 2)) # å…¨ä½“ã®å›³ã®å¤§ãã•ã‚’æŒ‡å®š\n",
        "gs = gridspec.GridSpec(ncols=3, nrows=1,\n",
        "                       width_ratios=[4, 1,1],\n",
        "                       #height_ratios=[1, 2],\n",
        "                       wspace=0.1) # , hspace=0.4) # Adjust spacing between subplots\n",
        "\n",
        "ax0 = fig.add_subplot(gs[0, 0]) # left\n",
        "ax0.plot(range(2000), y_hats[-2000:], color='red', label='äºˆæ¸¬')\n",
        "ax0.plot(range(2000), ys[-2000:], color='green', label='å®Ÿãƒ‡ãƒ¼ã‚¿')\n",
        "ax0.legend()\n",
        "ax0.set_title('RNN ã«ã‚ˆã‚‹äºˆæ¸¬çµæœ')\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, 1]) # middle\n",
        "ax1.imshow(np.concatenate( (np.zeros((10,1)), y_hats)).reshape(64,64), cmap='gray')\n",
        "ax1.set_title('å†æ§‹æˆ')\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "\n",
        "ax2 = fig.add_subplot(gs[0, 2]) # right\n",
        "ax2.imshow(data.reshape(64,64), cmap='gray')\n",
        "ax2.set_title('å®Ÿãƒ‡ãƒ¼ã‚¿')\n",
        "ax2.set_xticks([])\n",
        "ax2.set_yticks([])\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ Kalman filter å®Ÿç¿’"
      ],
      "metadata": {
        "id": "JRlXHAh3li4Q"
      },
      "id": "JRlXHAh3li4Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea97b10-904f-49bb-91b1-c0eb4eacff50",
      "metadata": {
        "id": "fea97b10-904f-49bb-91b1-c0eb4eacff50"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import filterpy\n",
        "except ImportError:\n",
        "    !pip install filterpy\n",
        "    import filterpy\n",
        "\n",
        "from filterpy.kalman import KalmanFilter\n",
        "from filterpy.common import Q_discrete_white_noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9ba3b6-d7fb-42cf-9b16-3448c7764588",
      "metadata": {
        "id": "1f9ba3b6-d7fb-42cf-9b16-3448c7764588"
      },
      "outputs": [],
      "source": [
        "N=0\n",
        "zs = X[N] #.reshape(1,-1)\n",
        "zs_mean = zs.mean(axis=0)\n",
        "zs_cov = np.cov(zs)\n",
        "\n",
        "kf = KalmanFilter(dim_x=1, dim_z=1) # zs.shape[0])\n",
        "#kf = KalmanFilter(dim_x=zs.shape[0],dim_z=zs.shape[0])\n",
        "\n",
        "kf.x = zs_mean\n",
        "kf.F = zs_cov\n",
        "kf.H = zs_cov\n",
        "\n",
        "xs, Cov = [], []\n",
        "for z in zs:\n",
        "    kf.predict()\n",
        "    kf.update(z)\n",
        "    xs.append(kf.x)\n",
        "    Cov.append(kf.P)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d8d5c0-8a9c-484f-ace2-f1e860a92089",
      "metadata": {
        "id": "68d8d5c0-8a9c-484f-ace2-f1e860a92089"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(2,2))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.imshow(np.array(xs).reshape(64,64), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71b2d50-918d-467a-82e9-666b38238abf",
      "metadata": {
        "id": "e71b2d50-918d-467a-82e9-666b38238abf"
      },
      "source": [
        "# ARIMA å®Ÿç¿’\n",
        "\n",
        "### è‡ªå·±ç›¸é–¢ (AR) å’Œåˆ† (I) ç§»å‹•å¹³å‡ MA ãƒ¢ãƒ‡ãƒ«\n",
        "\n",
        "ARIMA ãƒ¢ãƒ‡ãƒ«ã¨ã¯è‡ªå·±ç›¸é–¢ AR, ç§»å‹•å¹³å‡ MA, ãŠã‚ˆã³ãã®å’Œåˆ† I ã‚’ã‚ã‚ã›ãŸç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚\n",
        "\n",
        "æ•™ç§‘æ›¸ã«ã¯ p, d, q ã¨ã—ã¦ãã‚Œãã‚Œ AR, I, MA ã®æ¬¡æ•°ã‚’æ±ºã‚ã‚‹ã€‚\n",
        "(p,d,q) = (1,0,0) ã§ã‚ã‚Œã°ï¼Œ1 æ¬¡ã®è‡ªå·±ç›¸é–¢ã¨ãªã‚‹ã€‚ã™ãªã‚ã¡æ¬¡å¼ã§ã‚ã‚‹ï¼š\n",
        "$$\n",
        "AR(1): y_{t} = \\alpha y_{t-1} + c + \\epsilon\n",
        "$$\n",
        "ã“ã“ã§æ™‚åˆ» $t$ ã®å€¤ $y_{t}$ ã‚’äºˆæ¸¬ã™ã‚‹éš›ã«ï¼Œç›´å‰ã®æ™‚åˆ» $t-1$ ã®å€¤ $y_{t-1}$ ã®å€¤ã‹ã‚‰äºˆæ¸¬ã™ã‚‹ã“ã¨ã«ãªã‚‹ã€‚\n",
        "$c$ ã¯åˆ‡ç‰‡ï¼Œ$\\alpha$ ã¯å›å¸°ä¿‚æ•°ï¼Œ$\\epsilon$ ã¯èª¤å·®ã§ã‚ã‚‹ã€‚\n",
        "\n",
        "AR(2) ã™ãªã‚ã¡ 2 æ¬¡ã®è‡ªå·±ç›¸é–¢ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Œã°æ¬¡å¼ã¨ãªã‚‹ã€‚\n",
        "$$\n",
        "AR(2): y_{t} = \\alpha_{1}y_{t-1} + \\alpha_{2}y_{t-2} + c + \\epsilon\n",
        "$$\n",
        "\n",
        "ãŸã ã—ä»¥ä¸‹ã®ã‚ˆã†ãªåˆ¶ç´„ãŒå­˜åœ¨ã™ã‚‹ã€‚\n",
        "* AR(1)ãƒ¢ãƒ‡ãƒ«: $âˆ’1<\\phi_1<1$<br/>\n",
        "* AR(2)ãƒ¢ãƒ‡ãƒ«: $âˆ’1<\\phi_2<1,\\phi_1+\\phi_2<1,\\phi_2âˆ’\\phi_1<1$\n",
        "\n",
        "AR(m) m > 2 ã®å ´åˆåˆ¶ç´„ã¯è¤‡é›‘ã«ãªã‚‹ã€‚\n",
        "\n",
        "$m$ æ¬¡ã®è‡ªå·±ç›¸é–¢ãƒ¢ãƒ‡ãƒ« AR(m) ã§ã‚ã‚Œã°ï¼Œæ¬¡å¼ã¨ãªã‚‹ï¼š\n",
        "$$\n",
        "y_{t} = \\sum_{i=1}^{m}\\alpha_{i}y_{i} + c + \\epsilon\n",
        "$$\n",
        "\n",
        "ARIMA ãƒ¢ãƒ‡ãƒ«ï¼Œ$d$ éšå·®åˆ†ç³»åˆ—, $y_t-y_{t-d}$ ã‚’ ARMA ãƒ¢ãƒ‡ãƒ«ã§è¡¨ç¾ã™ã‚‹ã®ãŒ ARIMA ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚\n",
        "$$\n",
        "ARIMA(p,d,q):y_{t} - y_{t-d} = c + \\sum_{i=1}^{p}\\phi_{i}y_{t-i} + \\sum_{i=1}^{q}\\theta_{i}\\epsilon_{t-1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2da8df6-89e2-4a73-8d06-54ee739f56ea",
      "metadata": {
        "id": "a2da8df6-89e2-4a73-8d06-54ee739f56ea"
      },
      "source": [
        "## ç§»å‹•å¹³å‡ãƒ¢ãƒ‡ãƒ«\n",
        "\n",
        "$$\n",
        "y_t = c + \\epsilon_t + \\theta_1\\epsilon_1+\\cdots+\\theta_{t-q}\\epsilon_{t-q}\n",
        "= c + \\epsilon_t + \\sum_{i=1}^{q}\\theta_{i}\\epsilon_{t-i},\n",
        "$$\n",
        "ã“ã“ã§ $\\epsilon_{t}$ ã¯ç™½è‰²é›‘éŸ³ã§ã‚ã‚‹ã€‚ä¸Šå¼ã‚’ q æ¬¡ã®ç§»å‹•å¹³å‡ãƒ¢ãƒ‡ãƒ« MA(q) ã¨å‘¼ã¶ã€‚\n",
        "\n",
        "MA(1) ã§ã‚ã‚Œã°æ¬¡å¼ã¨ãªã‚‹ï¼š\n",
        "$$\n",
        "MA(1):y_t = c + \\epsilon + \\theta_1\\epsilon_{t-1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae8f2c3-ae43-42dc-80dd-922c2aba39e4",
      "metadata": {
        "id": "2ae8f2c3-ae43-42dc-80dd-922c2aba39e4"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "x = X[0]\n",
        "# ARIMA Model\n",
        "for order in [(1,0,0), (1,1,0), (1,1,2)]:\n",
        "    model = ARIMA(x, order=order)\n",
        "    model_fit = model.fit()\n",
        "    print(model_fit.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f1ae023-5178-4fbb-b6de-594b9ad81ed2",
      "metadata": {
        "id": "7f1ae023-5178-4fbb-b6de-594b9ad81ed2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b84e16-768d-4542-a109-42180bd3b112",
      "metadata": {
        "id": "f5b84e16-768d-4542-a109-42180bd3b112"
      },
      "outputs": [],
      "source": [
        "# Plot residual errors\n",
        "residuals = pd.DataFrame(model_fit.resid)\n",
        "fig, ax = plt.subplots(1,2, figsize=(12,3))\n",
        "residuals.plot(title=\"Residuals\", ax=ax[0])\n",
        "residuals.plot(kind='kde', title='Density', ax=ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369f538d-21b5-4529-a694-9cf202e55a72",
      "metadata": {
        "id": "369f538d-21b5-4529-a694-9cf202e55a72"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_predict\n",
        "#plot_predict(model_fit)\n",
        "#plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(1,2, figsize=(4,2))\n",
        "axes[0].imshow(model_fit.predict().reshape(64,64), cmap='gray')\n",
        "axes[1].imshow(x.reshape(64,64),cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04146c6c-a7bd-4b31-bd66-2952cbec1346",
      "metadata": {
        "id": "04146c6c-a7bd-4b31-bd66-2952cbec1346"
      },
      "outputs": [],
      "source": [
        "#model_fit.conf_int?\n",
        "\n",
        "model_fit.standardized_forecasts_error.shape\n",
        "#(steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FFT å®Ÿç¿’"
      ],
      "metadata": {
        "id": "GyhThchClFf3"
      },
      "id": "GyhThchClFf3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3393d402-895c-4188-9f06-eb4377b46ded",
      "metadata": {
        "id": "3393d402-895c-4188-9f06-eb4377b46ded"
      },
      "outputs": [],
      "source": [
        "x = X[N]\n",
        "y_fft = np.fft.fft(x)\n",
        "\n",
        "y_fft1 = y_fft.copy()\n",
        "y_fft2 = y_fft.copy()\n",
        "\n",
        "threshold = 32\n",
        "threshold = 64\n",
        "# threshold = 128\n",
        "# threshold = 512\n",
        "\n",
        "# ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿\n",
        "y_fft1.real[threshold:] = 0.\n",
        "y_fft1.imag[threshold:] = 0.\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿\n",
        "y_fft2.real[:threshold] = 0.\n",
        "y_fft2.imag[:threshold] = 0.\n",
        "# ç›´äº¤æˆåˆ†ã‚’è¶³ã—åˆã‚ã›ã‚‹\n",
        "y_fft2.real[0] = y_fft.real[0]\n",
        "y_fft2.imag[0] = y_fft.imag[0]\n",
        "\n",
        "# é€†å¤‰æ›\n",
        "y_ifft1 = np.fft.ifft(y_fft1)\n",
        "y_ifft2 = np.fft.ifft(y_fft2)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "#plt.plot(x, label='ã‚ªãƒªã‚¸ãƒŠãƒ«')\n",
        "plt.plot(y_ifft2.real, label='ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿')\n",
        "plt.plot(y_ifft1.real, label='ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿', lw=4)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01012e63-c634-4eab-aeb5-d5c6cc002823",
      "metadata": {
        "id": "01012e63-c634-4eab-aeb5-d5c6cc002823"
      },
      "outputs": [],
      "source": [
        "figs, axes = plt.subplots(1,2,figsize=(4,2))\n",
        "axes[0].imshow(y_ifft2.real.reshape(64,64),cmap='gray')\n",
        "axes[0].set_title('ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿')\n",
        "axes[1].imshow(y_ifft1.real.reshape(64,64),cmap='gray')\n",
        "axes[1].set_title('ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53434c1f-04e9-48e8-9f70-59b463a10253",
      "metadata": {
        "id": "53434c1f-04e9-48e8-9f70-59b463a10253"
      },
      "outputs": [],
      "source": [
        "# 2d fft\n",
        "y_fft2d = np.fft.fft2(x.reshape(64,64))\n",
        "\n",
        "# é€†å¤‰æ›\n",
        "y_ifft2d_1 = np.fft.ifft2(y_fft2d)\n",
        "#y_ifft2 = np.fft.fft2.ifft(y_fft2d)\n",
        "\n",
        "#np.set_printoptions(precision=3)\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(y_ifft2d_1.real, cmap='gray') #.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86af60b-0399-435d-b11d-1797f55e0379",
      "metadata": {
        "id": "a86af60b-0399-435d-b11d-1797f55e0379"
      },
      "outputs": [],
      "source": [
        "y_fft2d_1 = y_fft2d.copy()\n",
        "y_fft2d_2 = y_fft2d.copy()\n",
        "\n",
        "threshold = 32\n",
        "threshold = 64\n",
        "# threshold = 128\n",
        "#threshold = 1024\n",
        "\n",
        "# ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿\n",
        "y_fft2d_1.real[threshold:, threshold:] = 0.\n",
        "y_fft2d_1.imag[threshold:, threshold:] = 0.\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿\n",
        "y_fft2d_2.real[:threshold, :threshold] = 0.\n",
        "y_fft2d_2.imag[:threshold, :threshold] = 0.\n",
        "# ç›´äº¤æˆåˆ†ã‚’è¶³ã—åˆã‚ã›ã‚‹\n",
        "y_fft2d_2.real[0,0] = y_fft2d.real[0,0]\n",
        "y_fft2d_2.imag[0,0] = y_fft2d.imag[0,0]\n",
        "\n",
        "# é€†å¤‰æ›\n",
        "y_ifft2d_1 = np.fft.ifft2(y_fft2d_1)\n",
        "y_ifft2d_2 = np.fft.ifft2(y_fft2d_2)\n",
        "\n",
        "figs, axes = plt.subplots(1,2,figsize=(4,2))\n",
        "axes[0].imshow(y_ifft2d_1.real.reshape(64,64),cmap='gray')\n",
        "axes[0].set_title('ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿')\n",
        "axes[1].imshow(y_ifft2d_2.real.reshape(64,64),cmap='gray')\n",
        "axes[1].set_title('ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f182fe3c-8354-40fd-920f-b5df80af9b66",
      "metadata": {
        "id": "f182fe3c-8354-40fd-920f-b5df80af9b66"
      },
      "outputs": [],
      "source": [
        "N = 4096 >> 0\n",
        "np.random.seed(42)\n",
        "epsilons = [np.random.randn() for _ in range(N+3)]\n",
        "\n",
        "theta0, theta1, theta2 = 1.0, 0.2, 0.5\n",
        "X = [(epsilons[i],epsilons[i+1], epsilons[i+2]) for i in range(N)]\n",
        "y = [theta0 * _x[2] + theta1 * _x[1] + theta2 * _x[0] for _x in X]\n",
        "#len(y), y[:5]\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.title('ãƒ‡ãƒ¼ã‚¿')\n",
        "plt.plot(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "789da913-4506-4635-95ab-82e3e5f1f97e",
      "metadata": {
        "id": "789da913-4506-4635-95ab-82e3e5f1f97e"
      },
      "outputs": [],
      "source": [
        "len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417200b9-064a-4784-826b-4590a805877b",
      "metadata": {
        "id": "417200b9-064a-4784-826b-4590a805877b"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import statsmodels\n",
        "print(f'statsmodels.__version__:{statsmodels.__version__}')\n",
        "model = ARIMA(y, order=(0, 0, 2))\n",
        "result = model.fit()\n",
        "print(result.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87fa6355-fa9c-4e82-aae8-3ac1a52745d1",
      "metadata": {
        "id": "87fa6355-fa9c-4e82-aae8-3ac1a52745d1"
      },
      "outputs": [],
      "source": [
        "predct = result.predict()\n",
        "\n",
        "# æ®‹å·® plot\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(result.resid)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c6e5c5-5abd-46a8-ba27-1281eccf9b97",
      "metadata": {
        "id": "01c6e5c5-5abd-46a8-ba27-1281eccf9b97"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import pacf, acf\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "#len(y), len(pacf(y,lags=1))\n",
        "plot_acf(y, lags=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bfcd06a-26a1-4315-8ad6-f1e37cf3e583",
      "metadata": {
        "id": "1bfcd06a-26a1-4315-8ad6-f1e37cf3e583"
      },
      "outputs": [],
      "source": [
        "y = X[0]\n",
        "\n",
        "model = ARIMA(y, order=(1, 0, 1))\n",
        "result = model.fit()\n",
        "print(result.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2baaab8-3819-4650-ac06-34d7ff38d4aa",
      "metadata": {
        "id": "c2baaab8-3819-4650-ac06-34d7ff38d4aa"
      },
      "outputs": [],
      "source": [
        "X.shape\n",
        "x = X[0]\n",
        "# x.shape, zs.shape\n",
        "# print(zs[:10])\n",
        "# plt.plot(zs)\n",
        "# plt.plot(estimates)\n",
        "# plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(1,2,figsize=(4,2))\n",
        "axes[0].set_title('KF estimates')\n",
        "axes[0].imshow(np.array(estimates).reshape(64,64), cmap='gray')\n",
        "axes[0].axis(False)\n",
        "axes[1].imshow(x.reshape(64,64), cmap='gray')\n",
        "axes[1].axis(False)\n",
        "axes[1].set_title('Original')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ce0e03-d9a2-45a8-980d-3dac07bd1a3f",
      "metadata": {
        "id": "45ce0e03-d9a2-45a8-980d-3dac07bd1a3f"
      },
      "outputs": [],
      "source": [
        "#from filterpy.stats import gaussian\n",
        "X.shape\n",
        "N = 70\n",
        "zs = X[N]\n",
        "\n",
        "from collections import namedtuple\n",
        "gaussian = namedtuple('Gaussian', ['mean', 'var'])\n",
        "gaussian.__repr__ = lambda s: 'ğ’©(Î¼={:.3f}, ğœÂ²={:.3f})'.format(s[0], s[1])\n",
        "\n",
        "def update(likelihood, prior):\n",
        "    posterior = likelihood * prior\n",
        "    return normalize(posterior)\n",
        "\n",
        "def update(prior, measurement):\n",
        "    x, P = prior        # äº‹å‰åˆ†å¸ƒã®å¹³å‡ã¨åˆ†æ•£\n",
        "    z, R = measurement  # è¦³æ¸¬å€¤ã®å¹³å‡ã¨åˆ†æ•£\n",
        "\n",
        "    y = z - x        # æ®‹å·®\n",
        "    K = P / (P + R)  # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³\n",
        "\n",
        "    x = x + K*y      # äº‹å¾Œåˆ†å¸ƒã®å¹³å‡\n",
        "    P = (1 - K) * P  # äº‹å¾Œåˆ†å¸ƒã®åˆ†æ•£\n",
        "    return gaussian(x, P)\n",
        "\n",
        "def predict(posterior, movement):\n",
        "    x, P = posterior # äº‹å¾Œåˆ†å¸ƒã®å¹³å‡ã¨åˆ†æ•£\n",
        "    dx, Q = movement # ç§»å‹•é‡ã®å¹³å‡ã¨åˆ†æ•£\n",
        "    x = x + dx\n",
        "    P = P + Q\n",
        "    return gaussian(x, P)\n",
        "\n",
        "voltage_std = .13\n",
        "x = gaussian(.5, 1.) # åˆæœŸçŠ¶æ…‹\n",
        "process_var = 0.05**2\n",
        "#process_var = 1.**2\n",
        "process_model = gaussian(0., process_var)\n",
        "\n",
        "N = 50\n",
        "# zs = [volt(actual_voltage, voltage_std) for i in range(N)]\n",
        "ps = []\n",
        "estimates = []\n",
        "\n",
        "for z in zs:\n",
        "    prior = predict(x, process_model)\n",
        "    x = update(prior, gaussian(z, voltage_std**2))\n",
        "\n",
        "    # ã‚°ãƒ©ãƒ•ã«ã™ã‚‹ãŸã‚ã«è¨˜éŒ²ã™ã‚‹\n",
        "    estimates.append(x.mean)\n",
        "    ps.append(x.var)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14,3))\n",
        "plt.plot(zs, label='zs')\n",
        "plt.plot(estimates, label='estimates')\n",
        "plt.legend()\n",
        "#plt.ylim(16, 17)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(ps)\n",
        "plt.title('Variance')\n",
        "print('Variance converges to {:.3f}'.format(ps[-1]))\n",
        "\n",
        "fig, axes = plt.subplots(1,2,figsize=(3,2))\n",
        "axes[0].set_title('KF estimates')\n",
        "axes[0].imshow(np.array(estimates).reshape(64,64), cmap='gray')\n",
        "axes[0].axis(False)\n",
        "axes[1].imshow(zs.reshape(64,64), cmap='gray')\n",
        "axes[1].axis(False)\n",
        "axes[1].set_title('Original')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3adcf1f9-fe39-44a9-a9e6-d4b091e61d70",
      "metadata": {
        "id": "3adcf1f9-fe39-44a9-a9e6-d4b091e61d70"
      },
      "outputs": [],
      "source": [
        "zs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a10a932-ea1a-416a-89d3-30dece958de8",
      "metadata": {
        "id": "4a10a932-ea1a-416a-89d3-30dece958de8"
      },
      "source": [
        "# æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã«ã‚ˆã‚‹é¡”èªè­˜\n",
        "\n",
        "## ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ï¼Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚’ 2 åˆ†å‰²ã—ã¦ï¼Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚\n",
        "åˆ†å‰²ã—ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ï¼Œã—ã‹ã‚‹å¾Œã«ï¼Œãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ï¼Œãã®æ±åŒ–æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n",
        "ã“ã®ã¨ãï¼Œãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½ãŒé«˜ã„ãƒ¢ãƒ‡ãƒ«ãŒè‰¯ã„ãƒ¢ãƒ‡ãƒ«ã¨ã„ã†ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã‚ªãƒªãƒ™ãƒƒãƒ†ã‚£é¡”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ï¼Œ å„è¢«é¨“è€…ã® 10 æšã®é¡”ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "ã“ã®ã†ã¡ï¼Œä¾‹ãˆã° 90% ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã—ï¼Œ10% ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã¾ã™ã€‚\n",
        "å„é¡”ãƒ‡ãƒ¼ã‚¿ã®è¨“ç·´ç”»åƒã¨ãƒ†ã‚¹ãƒˆç”»åƒã®æ•°ãŒåŒã˜ã«ãªã‚‹ã‚ˆã†ã« stratify æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ã¾ã™ã€‚\n",
        "ã—ãŸãŒã£ã¦ï¼Œå„è¢«é¨“è€…ã«ã¯ 9 æšã®è¨“ç·´ç”¨ç”»åƒã¨ 1 æšã®ãƒ†ã‚¹ãƒˆç”¨ç”»åƒãŒç”¨æ„ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
        "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆã¯ split_ratio å¤‰æ›´ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QM9GNboxBo3i",
      "metadata": {
        "id": "QM9GNboxBo3i"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "#from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "#split_ratio = 0.3 ã¨ã—ã¦ã„ã‚‹ã®ã§ï¼Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿å¯¾ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒ 7:3 ã¨ãªã‚‹\n",
        "split_ratio = 0.3\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y_sex, test_size=split_ratio, stratify=y, random_state=42)\n",
        "#X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=split_ratio, stratify=y, random_state=42)\n",
        "print(f'X_train è¨“ç·´ç”»åƒã®ã‚µã‚¤ã‚º: {X_train.shape}')\n",
        "print(f'y_train æ•™å¸«ä¿¡å·ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚º: {y_train.shape}')\n",
        "\n",
        "print(f'X_test æ¤œè¨¼ç”»åƒã®ã‚µã‚¤ã‚º: {X_test.shape}')\n",
        "print(f'y_test æ•™å¸«ä¿¡å·ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚º: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "keMwxtpHz94M",
      "metadata": {
        "id": "keMwxtpHz94M"
      },
      "source": [
        "## ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a30yIeCGRP",
      "metadata": {
        "id": "e1a30yIeCGRP"
      },
      "outputs": [],
      "source": [
        "params = {'max_iter':10 ** 3,\n",
        "          'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "model = LogisticRegression(**params)\n",
        "model.fit(X_train, y_train)    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ç·šå½¢åˆ¤åˆ¥åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´\\n\",\n",
        "y_hat = model.predict(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦äºˆæ¸¬ã‚’è¡Œã„çµæœã‚’ y_hat ã«æ ¼ç´\\n\",\n",
        "print(f\"ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ç”¨ã„ãŸåˆ†é¡ç²¾åº¦: {metrics.accuracy_score(y_test, y_hat):.3f}\")\n",
        "print(f'åˆ†é¡å ±å‘Š:\\n{metrics.classification_report(y_test,y_hat)}')\n",
        "\n",
        "# æ··åŒè¡Œåˆ—ã®è¡¨ç¤º\n",
        "print('æ··åŒè¡Œåˆ—:\\n', metrics.confusion_matrix(y_test,y_hat))\n",
        "# plt.figure(figsize=(8,6))\n",
        "# sns.heatmap(metrics.confusion_matrix(y_test, y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X_OmJduo0NAE",
      "metadata": {
        "id": "X_OmJduo0NAE"
      },
      "source": [
        "## ç·šå½¢åˆ¤åˆ¥åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i1zDPV2ABuKl",
      "metadata": {
        "id": "i1zDPV2ABuKl"
      },
      "outputs": [],
      "source": [
        "params = {#'max_iter':10 ** 3,\n",
        "          #'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "model = LinearDiscriminantAnalysis(**params)\n",
        "model.fit(X_train, y_train)    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ç·šå½¢åˆ¤åˆ¥åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´\n",
        "y_hat = model.predict(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦äºˆæ¸¬ã‚’è¡Œã„çµæœã‚’ y_hat ã«æ ¼ç´\n",
        "print(f\"ç·šå½¢åˆ¤åˆ¥åˆ†æã‚’ç”¨ã„ãŸåˆ†é¡ç²¾åº¦: {metrics.accuracy_score(y_test, y_hat):.3f}\")\n",
        "print(f'åˆ†é¡å ±å‘Š:\\n{metrics.classification_report(y_test,y_hat)}')\n",
        "\n",
        "# æ··åŒè¡Œåˆ—ã®è¡¨ç¤º\n",
        "print('æ··åŒè¡Œåˆ—:\\n', metrics.confusion_matrix(y_test,y_hat))\n",
        "# plt.figure(figsize=(8,6))\n",
        "# sns.heatmap(metrics.confusion_matrix(y_test, y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tnBPgcEa0WG_",
      "metadata": {
        "id": "tnBPgcEa0WG_"
      },
      "source": [
        "## ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xTig2ErwBxZU",
      "metadata": {
        "id": "xTig2ErwBxZU"
      },
      "outputs": [],
      "source": [
        "params = {'max_iter':10 ** 3,\n",
        "          'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "model = SVC(**params)\n",
        "model.fit(X_train, y_train)    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ç·šå½¢åˆ¤åˆ¥åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´\n",
        "y_hat = model.predict(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦äºˆæ¸¬ã‚’è¡Œã„çµæœã‚’ y_hat ã«æ ¼ç´\n",
        "print(f\"ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ç”¨ã„ãŸåˆ†é¡ç²¾åº¦: {metrics.accuracy_score(y_test, y_hat):.3f}\")\n",
        "print(f'åˆ†é¡å ±å‘Š:\\n{metrics.classification_report(y_test,y_hat)}')\n",
        "# print(f'é©åˆç‡ precision score:{metrics.precision_score(y_test, y_hat):.3f}')\n",
        "# print(f'å†ç¾ç‡ recall  score  :{metrics.recall_score(y_test, y_hat):.3f}')\n",
        "# print(f'F1 å€¤  F1 score       :{metrics.f1_score(y_test, y_hat):.3f}')\n",
        "\n",
        "# æ··åŒè¡Œåˆ—ã®è¡¨ç¤º\n",
        "print('æ··åŒè¡Œåˆ—:\\n', metrics.confusion_matrix(y_test,y_hat))\n",
        "# plt.figure(figsize=(8,6))\n",
        "# sns.heatmap(metrics.confusion_matrix(y_test, y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gom1BG_v0f-J",
      "metadata": {
        "id": "gom1BG_v0f-J"
      },
      "source": [
        "## 3 æ‰‹æ³•æ¯”è¼ƒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CYVUL4ik0jcA",
      "metadata": {
        "id": "CYVUL4ik0jcA"
      },
      "outputs": [],
      "source": [
        "# 40 åã®é¡”èªè­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¡ã„ã¦ 3 æ‰‹æ³•ã‚’æ¯”è¼ƒ\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=split_ratio, stratify=y, random_state=42)\n",
        "\n",
        "# å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "params = {'max_iter':10 ** 4,\n",
        "          #'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "# 3 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©\n",
        "svc_model = SVC(**params)\n",
        "logistic_model = LogisticRegression(**params)\n",
        "lda_model = LinearDiscriminantAnalysis()\n",
        "\n",
        "# å‡ºåŠ›ã™ã‚‹å›³ã®ã‚µã‚¤ã‚ºã‚’å®šç¾©\n",
        "fig, _axes = plt.subplots(ncols=3, nrows=1, figsize=(12,4), constrained_layout=True)\n",
        "\n",
        "# 3 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãã‚Œãã‚Œå®Ÿè¡Œã—ã¦çµæœã‚’æç”»\n",
        "for i, (model_name, model) in enumerate([('ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒã‚·ãƒ³',svc_model),\n",
        "                                         ('ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°',logistic_model),\n",
        "                                         ('ç·šå½¢åˆ¤åˆ¥åˆ†æ',lda_model)]):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_hat = model.predict(X_test)\n",
        "    y_hat = model.predict(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦äºˆæ¸¬ã‚’è¡Œã„çµæœã‚’ y_hat ã«æ ¼ç´\n",
        "\n",
        "    _axes[i].imshow(metrics.confusion_matrix(y_test,y_hat), cmap='gray')\n",
        "    _axes[i].set_title(f'{model_name}:åˆ†é¡ç²¾åº¦:{metrics.accuracy_score(y_test,y_hat):.2f}')\n",
        "    _axes[i].axis('off')\n",
        "    print(f'åˆ†é¡å ±å‘Š:\\n{metrics.classification_report(y_test,y_hat)}')\n",
        "    # print(f'é©åˆç‡ precision score:{metrics.precision_score(y_test, y_hat, average='macro'):.3f}')\n",
        "    # print(f'å†ç¾ç‡ recall  score  :{metrics.recall_score(y_test, y_hat, average='macro'):.3f}')\n",
        "    # print(f'F1 å€¤  F1 score       :{metrics.f1_score(y_test, y_hat, average='macro'):.3f}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1z4Bwuo9aTEO",
      "metadata": {
        "id": "1z4Bwuo9aTEO"
      },
      "source": [
        "## ãƒªãƒ¼ãƒ–ãƒ»ãƒ¯ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ äº¤å·®æ¤œè¨¼\n",
        "\n",
        "ã‚ªãƒªãƒ™ãƒƒãƒ†ã‚£é¡”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ï¼Œå„è¢«é¨“è€…ã«å¯¾ã—ã¦ 10 æšã®é¡”ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ï¼Œ æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„ãƒ†ã‚¹ãƒˆã«ã¯å°‘ãªã„æ•°ã§ã™ã€‚\n",
        "\n",
        "ã‚¯ãƒ©ã‚¹ã®ä¾‹ãŒå°‘ãªã„æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ˆã‚Šã‚ˆãè©•ä¾¡ã™ã‚‹ãŸã‚ã«ï¼Œæ¡ç”¨ã•ã‚Œã‚‹äº¤å·®æ¤œè¨¼æ³•ã«ãƒªãƒ¼ãƒ–ãƒ»ãƒ¯ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ leave-one-out (LOO) äº¤å·®æ¤œè¨¼æ³•ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "LOO æ³•ã§ã¯ï¼Œã‚ã‚‹ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã†ã¡ 1 ã¤ã ã‘ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "ä»–ã®ã‚µãƒ³ãƒ—ãƒ«ã¯è¨“ç·´ã«ä½¿ç”¨ã—ã¾ã™ã€‚ ã“ã®æ‰‹é †ã‚’ï¼Œ å…¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¸€åº¦ã¥ã¤ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã—ã¦ç¹°ã‚Šè¿”ã•ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b408wi0faU0V",
      "metadata": {
        "id": "b408wi0faU0V"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "loo_cv = LeaveOneOut()\n",
        "model = LogisticRegression(**params)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=loo_cv)\n",
        "\n",
        "print(f\"{model.__class__.__name__} ãƒªãƒ¼ãƒ–ãƒ»ãƒ¯ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆäº¤å·®æ¤œè¨¼æ³•ã«ã‚ˆã‚‹å¹³å‡å¾—ç‚¹:{cv_scores.mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hNCGjJ6FHCft",
      "metadata": {
        "id": "hNCGjJ6FHCft"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã®æç”»\n",
        "# logits = [_x/(1.-_x) for _x in x]\n",
        "# plt.plot(x, logits)\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(x, np.log(logits))\n",
        "# plt.show()\n",
        "\n",
        "# x = np.arange(-10, 10, 0.01)\n",
        "# sigmoids = [1./(1.+np.exp(-_x)) for _x in x]\n",
        "# plt.plot(x, sigmoids)\n",
        "# plt.show()\n",
        "\n",
        "# x = np.arange(-10, 10, 0.01)\n",
        "# sigmoids = [1./(1.+np.exp(-_x)) for _x in x]\n",
        "# plt.plot(x, sigmoids, label=\"sigmoid\", c='red')\n",
        "# plt.plot(x, np.tanh(x), label=\"tanh\", c='green')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701b9057-1abd-478f-9e17-5a30b7b0ce20",
      "metadata": {
        "id": "701b9057-1abd-478f-9e17-5a30b7b0ce20"
      },
      "source": [
        "## äº¤å·®æ¤œè¨¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e27bda-c885-4f53-a236-c444d77e2f44",
      "metadata": {
        "id": "d8e27bda-c885-4f53-a236-c444d77e2f44"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "name = 'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°'\n",
        "model = LogisticRegression(**params)\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=kfold)\n",
        "print(f\"{name} å¹³å‡äº¤å·®æ¤œè¨¼å¾—ç‚¹: {cv_scores.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8819b8-db47-44db-b5e8-fc2fa3dd79e6",
      "metadata": {
        "id": "ca8819b8-db47-44db-b5e8-fc2fa3dd79e6"
      },
      "source": [
        "## ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´: GridSearcCV\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Šã®ãŸã‚ã« GridSearchCV ã‚’è¡Œã„ã¾ã™ã€‚\n",
        "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†é¡å™¨ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ã¿ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6952b8de-87a5-4c4d-afac-2c411e7c6bf0",
      "metadata": {
        "id": "6952b8de-87a5-4c4d-afac-2c411e7c6bf0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "params2={'penalty':['l1', 'l2'],\n",
        "        'C':np.logspace(0, 4, 10),\n",
        "        'max_iter': [10 ** 4],\n",
        "       }\n",
        "model = LogisticRegression()\n",
        "loo_cv = LeaveOneOut()\n",
        "gridSearchCV = GridSearchCV(model, params2, cv=loo_cv)\n",
        "gridSearchCV.fit(X_train, y_train)\n",
        "print(\"Grid search fitted..\")\n",
        "print(gridSearchCV.best_params_)\n",
        "print(gridSearchCV.best_score_)\n",
        "print(f\"ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹äº¤å·®å¦¥å½“æ€§å¾—ç‚¹:{gridSearchCV.score(X_test, y_test):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_8TrzyS6IYqj",
      "metadata": {
        "id": "_8TrzyS6IYqj"
      },
      "source": [
        "# PCA å›ºæœ‰é¡” Eigenfaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z60qpsFHIdKT",
      "metadata": {
        "id": "Z60qpsFHIdKT"
      },
      "outputs": [],
      "source": [
        "# X ã‚’ n è¡Œ m åˆ—ã®è¡Œåˆ—ã¨ã—ã¦ï¼Œåˆ—æ–¹å‘ m x m ã®ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã‚’æ±‚ã‚ã‚‹\n",
        "# ã™ãªã‚ã¡å„ç”»ç´ ã”ã¨ã®ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã‚’è¨ˆç®—ã™ã‚‹\n",
        "# ã‚ªãƒªãƒ™ãƒƒãƒ†ã‚£é¡”ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ 64 x 64 = 4096 ç”»ç´ åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãªã®ã§,ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã¯ 4096 x 4096 ã®å¤§ãã•ã¨ãªã‚‹\n",
        "#np.set_printoptions(precision=3)\n",
        "np.set_printoptions(formatter={'float': '{:.2f}'.format})\n",
        "\n",
        "x_mean = np.mean(X, axis=0)   # å„åˆ—ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
        "_X = X - x_mean               # å„åˆ—ã®å¹³å‡å€¤ã‚’æ¸›ã˜ã¦å¹³å‡åå·®ãƒ™ã‚¯ãƒˆãƒ«ã¨ã™ã‚‹\n",
        "Cov = _X.T @ _X / _X.shape[0] # å…±åˆ†æ•£è¡Œåˆ—\n",
        "_X_std = np.std(_X, axis=0)     # å„åˆ—ã®æ¨™æº–åå·®\n",
        "R = Cov / np.outer(_X_std.T, _X_std) # å…±åˆ†æ•£è¡Œåˆ—ã®å„åˆ—ã‚’å¯¾å¿œã™ã‚‹æ¨™æº–åå·®ã®ç©ã§é™¤ã—ã¦ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã«ã™ã‚‹\n",
        "print(f'R.shape:{R.shape}')        # ç¢ºèªç”¨ ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã®ã‚µã‚¤ã‚º\n",
        "print(f'ç›¸é–¢ä¿‚æ•°è¡Œåˆ— R:\\n{R[8:15,8:15]}')           # ç¢ºèªç”¨ ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã®æœ€åˆã® 3 è¡Œ 3 åˆ—ã‚’è¡¨ç¤ºã™ã‚‹\n",
        "\n",
        "R2 = np.corrcoef(X.T)              # ä¸Šè¨˜ã‚’ä¸€è¡Œã§è¡Œã† numpy ã‚³ãƒãƒ³ãƒ‰\n",
        "print(f'ç›¸é–¢ä¿‚æ•°è¡Œåˆ— R2:\\n{R2[8:15,8:15]}')         # çµæœã®è¡¨ç¤º"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QXLGyHa8IfjA",
      "metadata": {
        "id": "QXLGyHa8IfjA"
      },
      "outputs": [],
      "source": [
        "# ä¸Šã‚»ãƒ«ã¨ã¯ç•°ãªã‚Šãƒ‡ãƒ¼ã‚¿è¡Œåˆ— X (n è¡Œ m åˆ—) ã®è¡Œæ–¹å‘ n x n ã®ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã‚’æ±‚ã‚ã‚‹\n",
        "# ã“ã®å ´åˆ 400 ç”»åƒ x 400 ç”»åƒ (400 ã¯ 40 äººåˆ†ã®ç”»åƒã§å„äºº 10 æšã®ç”»åƒ)\n",
        "import seaborn as sns  # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ã®ãŸã‚ã«ä½¿ç”¨\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(np.corrcoef(X), center=0, vmin=-1., vmax=1., square=True, cmap='gnuplot2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OTfzM0YmIhzc",
      "metadata": {
        "id": "OTfzM0YmIhzc"
      },
      "outputs": [],
      "source": [
        "# å¹³å‡ç”»åƒã‚’æç”»\n",
        "x_mean = np.mean(X, axis=0)   # å„åˆ—ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
        "print(f'x_mean.min():{x_mean.min():.2f}',\n",
        "      f'x_mean.max():{x_mean.max():.2f}')\n",
        "#print(X.min(), X.max())\n",
        "plt.figure(figsize=(2.5,2.5))\n",
        "plt.title(f'å¹³å‡é¡”, æœ€å°å€¤:{x_mean.min():.2f}, æœ€å¤§å€¤:{x_mean.max():.2f}')\n",
        "plt.imshow(x_mean.reshape(64,64), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8UjDln-yIj8d",
      "metadata": {
        "id": "8UjDln-yIj8d"
      },
      "outputs": [],
      "source": [
        "# ã„ãã¤ã‹ã®ç”»åƒã«å¯¾ã—ã¦ï¼Œå¹³å‡é¡”ã‹ã‚‰ã®å·®åˆ†ã‚’ç”»åƒåŒ–ã—ã¦è¡¨ç¤ºã—ã¦ã¿ã‚‹\n",
        "idxes = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(idxes), figsize=(len(idxes) * 1.2, 1.5))\n",
        "for i in range(0,len(idxes),1):\n",
        "    idx = idxes[i]\n",
        "    #print(f'idx:{idx}')\n",
        "    axes[i].imshow((X[idx] - x_mean).reshape(64,64), cmap='gray')\n",
        "    axes[i].set_title(f'idx:{idx}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OWCmA5hSIo6k",
      "metadata": {
        "id": "OWCmA5hSIo6k"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "class PCA():\n",
        "    def __init__(self, X:np.array, n_dim:int=500):\n",
        "        self.X = X\n",
        "\n",
        "        N, M = X.shape\n",
        "        if N < M:\n",
        "            self.n_dim = N\n",
        "        else:\n",
        "            self.n_dim = M\n",
        "\n",
        "        if self.n_dim > n_dim:\n",
        "            self.n_dim = n_dim\n",
        "        self.mean = np.mean(X, axis=0)          # è¡Œåˆ— X ã®å„åˆ—ã”ã¨ã®å¹³å‡ã‚’æ±‚ã‚ã‚‹\n",
        "        self._X = X - self.mean                 # å„åˆ—ã”ã¨ã®å¹³å‡ã‚’å¼•ã„ãŸ å¹³å‡åå·®è¡Œåˆ— _X\n",
        "        self.Corr = np.dot(self._X.T, self._X)  # å„åˆ—ã”ã¨ã®åˆ†æ•£å…±åˆ†æ•£è¡Œåˆ— Corr\n",
        "        self.Eigenvalues, self.Eigenvectors = np.linalg.eig(self.Corr)  # å›ºæœ‰å€¤å•é¡Œã‚’è§£ã\n",
        "\n",
        "        self.Eigenvalues = self.Eigenvalues[0:self.n_dim].copy()\n",
        "        self.Eigenvectors = self.Eigenvectors[:,0:self.n_dim].copy()\n",
        "        self.projections = np.dot(self._X, self.Eigenvectors)\n",
        "\n",
        "    def _reconstruct(self, start:int=0, end:int=10):\n",
        "        return np.dot(self.projections[:,start:end], self.Eigenvectors[:,start:end].T) + self.mean\n",
        "\n",
        "\n",
        "_PCA = PCA(X=X)\n",
        "Eigenvalues, Eigenvectors, mean = _PCA.Eigenvalues, _PCA.Eigenvectors, _PCA.mean\n",
        "print(f'Eigenvalues.shape:{Eigenvalues.shape}',\n",
        "      f'\\nEigenvectors.shape:{Eigenvectors.shape}',\n",
        "      f'\\nmean.shape:{mean.shape}')\n",
        "\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.title('å¹³å‡é¡”')\n",
        "plt.imshow(mean.reshape(64,64), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tW6rgJ1AayMm",
      "metadata": {
        "id": "tW6rgJ1AayMm"
      },
      "outputs": [],
      "source": [
        "# ã„ãã¤ã‹ã®ç”»åƒã«å¯¾ã—ã¦ï¼Œå¹³å‡é¡”ã‹ã‚‰ã®å·®åˆ†ã‚’ç”»åƒåŒ–ã—ã¦è¡¨ç¤ºã—ã¦ã¿ã‚‹\n",
        "idxes = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n",
        "n_idx = 12\n",
        "idxes = sorted(np.random.permutation(np.arange(X.shape[0]))[:n_idx])\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(idxes), figsize=(len(idxes) * 1.2, 1.5))\n",
        "for i in range(0,len(idxes),1):\n",
        "    idx = idxes[i]\n",
        "    #print(f'idx:{idx}')\n",
        "    axes[i].imshow((X[idx] - x_mean).reshape(64,64), cmap='gray')\n",
        "    axes[i].set_title(f'idx:{idx}', size=8)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('ã„ãã¤ã‹ã®ç”»åƒã«å¯¾ã—ã¦ï¼Œå¹³å‡é¡”ã‹ã‚‰ã®å·®åˆ†ã‚’ç”»åƒåŒ–', size=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v397odVca3Nh",
      "metadata": {
        "id": "v397odVca3Nh"
      },
      "source": [
        "## å›ºæœ‰å€¤ã®ãƒ—ãƒ­ãƒƒãƒˆ (é™é †)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hy9T4qNka5Zs",
      "metadata": {
        "id": "Hy9T4qNka5Zs"
      },
      "outputs": [],
      "source": [
        "# å¤§ãã„æ–¹ã‹ã‚‰ä¸Šä½ N å€‹ã®å›ºæœ‰å€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã‚‹\n",
        "N = 200\n",
        "plt.plot(Eigenvalues[:N])\n",
        "plt.title(f'å›ºæœ‰å€¤ã®ãƒ—ãƒ­ãƒƒãƒˆ: max:{_PCA.n_dim}')\n",
        "\n",
        "\n",
        "N_pca = 10\n",
        "fix, axes = plt.subplots(nrows=1, ncols=N_pca, figsize=(N_pca * 1.4, 1.4))\n",
        "for i in range(N_pca):\n",
        "    axes[i].imshow(_PCA.Eigenvectors[:,i].reshape(64,64), cmap=\"gray\")\n",
        "    axes[i].set_title(f'ç¬¬ {i+1} å›ºæœ‰å€¤ã«å¯¾å¿œ\\nã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«', size=7)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j5H07ld6bJ89",
      "metadata": {
        "id": "j5H07ld6bJ89"
      },
      "source": [
        "## å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã‚’å¯è¦–åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lPLgr3S5bMXd",
      "metadata": {
        "id": "lPLgr3S5bMXd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(np.corrcoef(_PCA.Eigenvectors.T)[:400,:400], center=0, vmin=-1., vmax=1., square=True, cmap='gnuplot2')\n",
        "plt.title('å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®ç›¸é–¢ä¿‚æ•°è¡Œåˆ—ã‚’å¯è¦–åŒ–')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478fe75c-d353-4877-8c12-8776a5f0c3c7",
      "metadata": {
        "id": "478fe75c-d353-4877-8c12-8776a5f0c3c7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(_PCA.Eigenvectors.T @ _PCA.Eigenvectors, center=0, vmin=-1., vmax=1., square=True, cmap='gnuplot2')\n",
        "plt.title('å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®ç©ã‚’å¯è¦–åŒ–')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2211703-9e22-42cb-8d37-1371b3f446c6",
      "metadata": {
        "id": "b2211703-9e22-42cb-8d37-1371b3f446c6"
      },
      "outputs": [],
      "source": [
        "print('å›ºæœ‰å€¤ã¯å„è¡Œã”ã¨ã«æ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚ã‹ã¤ï¼Œå„å›ºæœ‰å€¤ã¯ç›´äº¤ã—ã¦ã„ã‚‹ã€‚ã™ãªã‚ã¡å¯¾è§’è¦ç´ ãŒ 1 ã§éå¯¾è§’è¦ç´ ã¯ 0 ã§ã‚ã‚‹')\n",
        "print((_PCA.Eigenvectors.T @ _PCA.Eigenvectors)[:5,:5])\n",
        "\n",
        "print('å›ºæœ‰å€¤ã¯å„åˆ—ã”ã¨ã§ã¯ãªã„ã€‚ãã®è¨¼æ‹ ã«åˆ—ã”ã¨ã«è¨ˆç®—ã—ã¦ã¿ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹')\n",
        "print((_PCA.Eigenvectors @ _PCA.Eigenvectors.T)[:5,:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Soz_jNBbYq8",
      "metadata": {
        "id": "_Soz_jNBbYq8"
      },
      "source": [
        "## PCA ã«ã‚ˆã‚‹æ¬¡å…ƒåœ§ç¸®ã®çµæœã‚’è¡¨ç¤º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NdAvqkHJIrR-",
      "metadata": {
        "id": "NdAvqkHJIrR-"
      },
      "outputs": [],
      "source": [
        "# PCA ã«ã‚ˆã‚‹æ¬¡å…ƒåœ§ç¸®ã®çµæœã‚’è¡¨ç¤º\n",
        "\n",
        "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(14,4))\n",
        "axes[0].scatter(_PCA.projections[:,0], _PCA.projections[:,1], s=1)\n",
        "axes[0].set_xlabel('ç¬¬ 1 ä¸»æˆåˆ†')\n",
        "axes[0].set_ylabel('ç¬¬ 2 ä¸»æˆåˆ†')\n",
        "for i in range(_PCA.projections.shape[0]):\n",
        "    axes[0].annotate(str(y[i]), (_PCA.projections[i,0], _PCA.projections[i,1]))\n",
        "\n",
        "axes[1].scatter(_PCA.projections[:,1], _PCA.projections[:,2], s=1)\n",
        "axes[1].set_xlabel('ç¬¬ 2 ä¸»æˆåˆ†')\n",
        "axes[1].set_ylabel('ç¬¬ 3 ä¸»æˆåˆ†')\n",
        "for i in range(_PCA.projections.shape[0]):\n",
        "    axes[1].annotate(str(y[i]), (_PCA.projections[i,1], _PCA.projections[i,2]))\n",
        "\n",
        "axes[2].scatter(_PCA.projections[:,0], _PCA.projections[:,3], s=1)\n",
        "axes[2].set_xlabel('ç¬¬ 1 ä¸»æˆåˆ†')\n",
        "axes[2].set_ylabel('ç¬¬ 4 ä¸»æˆåˆ†')\n",
        "for i in range(_PCA.projections.shape[0]):\n",
        "    axes[2].annotate(str(y[i]), (_PCA.projections[i,0], _PCA.projections[i,3]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GC82DIa6be_A",
      "metadata": {
        "id": "GC82DIa6be_A"
      },
      "source": [
        "## PCA ã«ã‚ˆã‚‹å›ºæœ‰é¡”ã®å†æ§‹æˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pM6gioH1bgfZ",
      "metadata": {
        "id": "pM6gioH1bgfZ"
      },
      "outputs": [],
      "source": [
        "# è¡¨ç¤ºã™ã‚‹ç”»åƒç•ªå·ãƒªã‚¹ãƒˆã‚’ idexes ã§å®£è¨€\n",
        "idxes = [0, 10, 20, 40, 60, 70]\n",
        "idxes = [70, 71, 72, 73, 74, 75]\n",
        "\n",
        "# ä¹±æ•°ã‚’ç™ºç”Ÿã•ã›ã¦ n_idx å€‹ã®ãƒ‡ãƒ¼ã‚¿ç”»åƒã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã§ idxes ã«æ ¼ç´ã™ã‚‹\n",
        "n_idx = 10\n",
        "idxes = sorted(np.random.permutation(np.arange(X.shape[0]))[:n_idx])\n",
        "\n",
        "# PCA ã«ã‚ˆã‚‹ç”»åƒã®å†æ§‹æˆæ™‚ã«ä½•å€‹ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’é‡ã­ã‚‹ã‚’æŒ‡å®šã™ã‚‹ãƒªã‚¹ãƒˆ ends\n",
        "ends = [1, 10, 20, 40, 80, 160, 320]\n",
        "ends = [1, 10, 20, 30, 40, 50, 100, 150, 200, 250, 300]\n",
        "\n",
        "# è¡¨ç¤ºã™ã‚‹ç”»åƒã‚µã‚¤ã‚ºã®å®£è¨€ figsize ã®å¹…ã¨é«˜ã•ã‚’æŒ‡å®š,å˜ä½ã¯æ­´å²çš„çµŒç·¯ã‹ã‚‰ã‚¤ãƒ³ãƒ\n",
        "fig, axes = plt.subplots(nrows=len(idxes), ncols=len(ends), figsize=(1.4 * len(ends), 1.6 * len(idxes)))\n",
        "for i, idx in enumerate(idxes):\n",
        "    for j, end in enumerate(ends):\n",
        "        axes[i][j].axis('off')\n",
        "        if j == 0:\n",
        "            axes[i][j].set_title(f'ç”»åƒç•ªå·:{idx}')\n",
        "        else:\n",
        "            axes[i][j].set_title(f'{end} æ¬¡å…ƒã¾ã§')\n",
        "        axes[i][j].imshow((_PCA.projections[idx,0:end] @ _PCA.Eigenvectors[:,0:end].T + _PCA.mean).reshape(64,64), cmap='gray')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wsBJNZHPbj1v",
      "metadata": {
        "id": "wsBJNZHPbj1v"
      },
      "source": [
        "## å›ºæœ‰é¡” (PCA) ã‚’ç”¨ã„ãŸåˆ†é¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MYuE8TJWbkmQ",
      "metadata": {
        "id": "MYuE8TJWbkmQ"
      },
      "outputs": [],
      "source": [
        "# å›ºæœ‰é¡” (PCA) ã‚’ç”¨ã„ã¦ 40 åã®é¡”èªè­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¡ã„ã¦ 3 æ‰‹æ³•ã‚’æ¯”è¼ƒ\n",
        "X_train, X_test, y_train, y_test=train_test_split(_PCA.projections, y, test_size=split_ratio, stratify=y, random_state=42)\n",
        "#X_train, X_test, y_train, y_test=train_test_split(_PCA.projections, y_sex, test_size=split_ratio, stratify=y, random_state=42)\n",
        "\n",
        "# å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "params = {'max_iter':10 ** 4,\n",
        "          #'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "# 3 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©\n",
        "svc_model = SVC(**params)\n",
        "logistic_model = LogisticRegression(**params)\n",
        "lda_model = LinearDiscriminantAnalysis()\n",
        "\n",
        "# å‡ºåŠ›ã™ã‚‹å›³ã®ã‚µã‚¤ã‚ºã‚’å®šç¾©\n",
        "fig, _axes = plt.subplots(ncols=3, nrows=1, figsize=(12,4), constrained_layout=True)\n",
        "\n",
        "# 3 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãã‚Œãã‚Œå®Ÿè¡Œã—ã¦çµæœã‚’æç”»\n",
        "for i, (model_name, model) in enumerate([('ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒã‚·ãƒ³',svc_model),\n",
        "                                         ('ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°',logistic_model),\n",
        "                                         ('ç·šå½¢åˆ¤åˆ¥åˆ†æ',lda_model)]):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_hat = model.predict(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦äºˆæ¸¬ã‚’è¡Œã„çµæœã‚’ y_hat ã«æ ¼ç´\n",
        "\n",
        "    _axes[i].imshow(metrics.confusion_matrix(y_test,y_hat), cmap='gray')\n",
        "    _axes[i].set_title(f'{model_name}:åˆ†é¡ç²¾åº¦:{metrics.accuracy_score(y_test,y_hat):.2f}')\n",
        "    _axes[i].axis('off')\n",
        "\n",
        "    print(model_name, '\\n', metrics.confusion_matrix(y_test,y_hat))\n",
        "    print(model_name, '\\n', metrics.classification_report(y_test,y_hat))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wP9bW8YMbpeJ",
      "metadata": {
        "id": "wP9bW8YMbpeJ"
      },
      "source": [
        "## tSNE ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒƒãƒˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "okTvz_DpbrFQ",
      "metadata": {
        "id": "okTvz_DpbrFQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_tsne = TSNE(n_components=2, learning_rate='auto',\n",
        "              init='random', perplexity=3).fit_transform(X)\n",
        "\n",
        "PCA_tsne = TSNE(n_components=2, learning_rate='auto',\n",
        "              init='random', perplexity=3).fit_transform(_PCA.projections)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t45EfhY4byMB",
      "metadata": {
        "id": "t45EfhY4byMB"
      },
      "outputs": [],
      "source": [
        "#plt.figure(figsize=(12,6))\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
        "\n",
        "axes[0].scatter(X_tsne[:,0], X_tsne[:,1], s=1)\n",
        "for i in range(X_tsne.shape[0]):\n",
        "    axes[0].annotate(str(y[i]), (X_tsne[i,0], X_tsne[i,1]))\n",
        "axes[0].set_title('tSNE ãƒ—ãƒ­ãƒƒãƒˆ')\n",
        "\n",
        "axes[1].scatter(PCA_tsne[:,0], PCA_tsne[:,1], s=1)\n",
        "for i in range(PCA_tsne.shape[0]):\n",
        "    axes[1].annotate(str(y[i]), (X_tsne[i,0], X_tsne[i,1]))\n",
        "axes[1].set_title('tSNE ãƒ—ãƒ­ãƒƒãƒˆ')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RdAMVKdQb145",
      "metadata": {
        "id": "RdAMVKdQb145"
      },
      "source": [
        "# ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼é¡” Fisherfaces\n",
        "\n",
        "ç·šå½¢åˆ¤åˆ¥åˆ†æã¯ã€R.A.ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ å¿ã«ã‚ˆã£ã¦ç™ºæ˜ã•ã‚ŒãŸã€‚\n",
        "ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ã¯ 1936 å¹´ã®è«–æ–‡ \"The use of multiple measurements in classificationonomic problems\" ã®ä¸­ã§ã€ã‚¢ãƒ¤ãƒ¡ã®åˆ†é¡ã«ç”¨ã„ã‚‹ã“ã¨ã«æˆåŠŸã—ãŸã€‚\n",
        "<!-- ã—ã‹ã—ã€ä¸»æˆåˆ†åˆ†æ (PCA) ãŒã“ã‚Œã»ã©ã†ã¾ãã„ã£ãŸã®ã«ã€ãªãœåˆ¥ã®æ¬¡å…ƒå‰Šæ¸›æ³•ãŒå¿…è¦ãªã®ã ã‚ã†ã‹ï¼Ÿ-->\n",
        "PCA ã¯ãƒ‡ãƒ¼ã‚¿ã®å…¨åˆ†æ•£ã‚’æœ€å¤§åŒ–ã™ã‚‹ç‰¹å¾´ã®ç·šå½¢çµåˆã‚’è¦‹ã¤ã‘ã‚‹ã€‚\n",
        "ã“ã‚Œã¯å¼·åŠ›ãªæ–¹æ³•ã ãŒã€ã‚¯ãƒ©ã‚¹ã‚’è€ƒæ…®ã—ãªã„ãŸã‚ã€æˆåˆ†ã‚’æ¨ã¦ã‚‹ã¨ãã«å¤šãã®è­˜åˆ¥æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚\n",
        "åˆ†æ•£ãŒå¤–éƒ¨ã‚½ãƒ¼ã‚¹ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã‚‹çŠ¶æ³ã‚’æƒ³åƒã—ã¦ã¿ã‚ˆã†ã€‚\n",
        "PCA ã«ã‚ˆã£ã¦åŒå®šã•ã‚ŒãŸæˆåˆ†ã¯ã€å¿…ãšã—ã‚‚è­˜åˆ¥æƒ…å ±ã‚’å…¨ãå«ã¾ãªã„ã®ã§ã€æŠ•å½±ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã¯äº’ã„ã«æ··ã–ã‚Šåˆã„ã€åˆ†é¡ã¯ä¸å¯èƒ½ã«ãªã‚‹ã€‚\n",
        "ã‚¯ãƒ©ã‚¹é–“ã‚’æœ€ã‚‚ã‚ˆãåˆ†é›¢ã™ã‚‹ç‰¹å¾´ã®çµ„ã¿åˆã‚ã›ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ã€ç·šå½¢åˆ¤åˆ¥åˆ†æã¯ã‚¯ãƒ©ã‚¹é–“ã®ã°ã‚‰ã¤ãã¨ã‚¯ãƒ©ã‚¹å†…ã®ã°ã‚‰ã¤ãã®æ¯”ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚\n",
        "è€ƒãˆæ–¹ã¯å˜ç´”ã§ã€åŒã˜ã‚¯ãƒ©ã‚¹ã¯äº’ã„ã«å¯†ã«é›†ã¾ã‚Šã€ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ã¯ã§ãã‚‹ã ã‘é›¢ã‚Œã‚‹ã¹ãã§ã‚ã‚‹ã€‚\n",
        "ã“ã®ã“ã¨ã¯ Belhumeur_Hespanha_Kriegman ã‚‚èªè­˜ã—ã¦ãŠã‚Šã€å½¼ã‚‰ã¯ [3] ã§é¡”èªè­˜ã«åˆ¤åˆ¥åˆ†æã‚’é©ç”¨ã—ãŸã€‚\n",
        "<!-- The Linear Discriminant Analysis was invented by the great statistician Sir R. A. Fisher, who successfully used it for classifying owers in his 1936 paper The use of multiple measurements in taxonomic problems [8].\n",
        "But why do we need another dimensionality reduction method, if the Principal Component Analysis (PCA) did such a good job?\n",
        "The PCA â€€finds a linear combination of features that maximizes the total variance in data.\n",
        "While this is clearly a powerful way to represuccsent data, it doesn't consider any classes and so a lot of discriminative information may be lost when throwing components away.\n",
        "Imagine a situation where the variance is generated by an external source, let it be the light.\n",
        "The components identified by a PCA do not necessarily contain any discriminative information at all, so the projected samples are smeared together and a classification becomes impossible.\n",
        "In order to â€€nd the combination of features that separates best between classes the Linear Discriminant Analysis maximizes the ratio of between-classes to within-classes scatter.\n",
        "The idea is simple: same classes should cluster tightly together, while different classes are as far away as possible from each other.\n",
        "This was also recognized by Belhumeur, Hespanha and Kriegman and so they applied a Discriminant Analysis to face recognition in [3]. -->\n",
        "\n",
        "* [3] Belhumeur, P. N., Hespanha, J., and Kriegman, D. Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection. IEEE Transactions on Pattern Analysis and Machine Intelligence 19, 7 (1997), 711-720.\n",
        "* [4] Brunelli, R., and Poggio, T. Face recognition through geometrical features. In European Conference on Computer Vision (ECCV) (1992), pp. 792-800.\n",
        "* [5] Cardinaux, F., Sanderson, C., and Bengio, S. User authentication via adapted statistical models of face images. IEEE Transactions on Signal Processing 54 (January 2006), 361-373.\n",
        "* [6] Chiara Turati, Viola Macchi Cassia, F. S., and Leo, I. Newborns face recognition: Role of inner and outer facial features. Child Development 77, 2 (2006), 297-311.\n",
        "* [7] Duda, R. O., Hart, P. E., and Stork, D. G. Pattern Classification (2nd Edition), 2 ed. November 2001.\n",
        "* [8] Fisher, R. A. The use of multiple measurements in taxonomic problems. Annals Eugen. 7 (1936), 179-188.\n",
        "\n",
        "### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èª¬æ˜\n",
        "<!--2.3.1 Algorithmic Description-->\n",
        "\n",
        "$X$ ã‚’ $c$ å€‹ã®ã‚¯ãƒ©ã‚¹ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ã¨ã™ã‚‹ï¼š\n",
        "<!--Let $X$ be a random vector with samples drawn from c classes: -->\n",
        "$$\\tag{8}\n",
        "X = \\left\\{X_1,X_2,\\ldots,X_{c}\\right\\}\n",
        "$$\n",
        "$$\\tag{9}\n",
        "X_i = \\left\\{x_1, x_2,\\ldots, x_{n}\\right\\}\n",
        "$$\n",
        "\n",
        "åˆ†æ•£è¡Œåˆ— $S_B$ (ç´šé–“åˆ†æ•£) ã¨ $S_W$ (ç´šå†…åˆ†æ•£) ã¯æ¬¡å¼ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ï¼š\n",
        "<!-- The scatter matrices $S_B$ and $S_W$ are defined as: -->\n",
        "$$\\tag{10}\n",
        "S_B = \\sum_{i=1}^{c} N_i (\\mu_i - \\mu)(\\mu_i - \\mu)^{\\top},\n",
        "$$\n",
        "$$\\tag{11}\n",
        "S_W = \\sum_{i=1}^{c} \\sum_{x \\in X_i} (x - \\mu_i)(x - \\mu_i)^{\\top},\n",
        "$$\n",
        "ã“ã“ã§ $\\mu$ ã¯å…¨å¹³å‡:<!--where $\\mu$ is the total mean:-->\n",
        "$$\\tag{12}\n",
        "\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i,\n",
        "$$\n",
        "ã•ã‚‰ã« $\\mu_i$ ã¯ç¾¤å†…å¹³å‡:<!--And $\\mu_i$ is the mean of class $i\\in\\left\\{1,\\ldots,C\\right\\}$:-->\n",
        "$$\\tag{13}\n",
        "\\mu_i = \\frac{1}{\\left|X_i\\right|} \\sum_{x_{j} \\in X_{i}} x_{j}.\n",
        "$$\n",
        "ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ã®å¤å…¸çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã‚¯ãƒ©ã‚¹åˆ†é›¢å¯èƒ½æ€§åŸºæº–ã‚’æœ€å¤§åŒ–ã™ã‚‹å°„å½±è¡Œåˆ— $W$ ã‚’æ¢ã™ï¼š\n",
        "<!-- Fisher's classic algorithm now looks for a projection matrix $W$, that maximizes the class separability criterion: -->\n",
        "$$\\tag{14}\n",
        "W_{opt}=\\arg\\max_{W} \\frac{\\left|W^{\\top} S_B W\\right|}{\\left|W^{\\top} S_W W\\right|},\n",
        "$$\n",
        "[3] ã«å¾“ã£ã¦ã€ã“ã®æœ€é©åŒ–å•é¡Œã®è§£ã¯ã€ä¸€èˆ¬å›ºæœ‰å€¤å•é¡Œã‚’è§£ãã“ã¨ã«ã‚ˆã£ã¦ä¸ãˆã‚‰ã‚Œã‚‹ï¼š\n",
        "<!-- Following[3], a solution for this optimization problem is given by solving the Genral Eigenvalue Problem: -->\n",
        "$$\\tag{15}\\begin{aligned}\n",
        "S_{B}\\nu_i &= \\lambda_i S_{W}\\nu_i,\\\\\n",
        "S_{W}^{-1} S_{B} \\nu_i &= \\lambda_i \\nu_i,\n",
        "\\end{aligned}$$\n",
        "\n",
        "è§£æ±ºã™ã¹ãå•é¡ŒãŒ 1 ã¤æ®‹ã£ã¦ã„ã‚‹ï¼š\n",
        "$S_W$ ã®ãƒ©ãƒ³ã‚¯ã¯æœ€å¤§ã§ã‚‚ $(N-c)$ ã§ã‚ã‚Šï¼Œ$N$ å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã¨ $c$ å€‹ã®ã‚¯ãƒ©ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã€‚\n",
        "ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜å•é¡Œã§ã¯ï¼Œã‚µãƒ³ãƒ—ãƒ«æ•° $N$ ã¯å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒ (ç”»ç´ æ•°) ã‚ˆã‚Šå°ã•ã„ã“ã¨ãŒã»ã¨ã‚“ã©ãªã®ã§ï¼Œåˆ†æ•£è¡Œåˆ— $S_W$ ã¯ç‰¹ç•°è¡Œåˆ—ã«ãªã‚‹ ([2]ã‚’å‚ç…§)ã€‚\n",
        "[3] ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ä¸»æˆåˆ†åˆ†æã‚’å®Ÿè¡Œã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚’ (N-c) æ¬¡å…ƒç©ºé–“ã«å°„å½±ã™ã‚‹ã“ã¨ã§ã“ã‚Œã‚’è§£æ±ºã—ãŸã€‚\n",
        "$S_W$ ãŒç‰¹ç•°ã§ãªããªã£ãŸã®ã§ã€ç·šå½¢åˆ¤åˆ¥åˆ†æãŒç¸®å°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ã€‚\n",
        "æœ€é©åŒ–å•é¡Œã¯æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‰ã‚Œã‚‹:\n",
        "<!-- There's one problem left to solve:\n",
        "The rank of S_W is at most (N-c), with N samples and c classes.\n",
        "In pattern recognition problems the number of samples N is almost always smaller than the dimension of the input data (the number of pixels), so the scatter matrix SW becomes singular (see [2]).\n",
        "In [3] this was solved by performing a Principal Component Analysis on the data and projecting the samples int the (N-c)-dimensional space.\n",
        "A Linear Discriminant Analysis is then performed on the reduced data, because S_W is not singular anymore.\n",
        "The optimization problem is can be rewritten as: -->\n",
        "$$\\tag{16}\n",
        "W_{pca}=\\arg\\max_{W} \\left|W^{\\top} S_T W\\right|\n",
        "$$\n",
        "\n",
        "$$\\tag{17}\n",
        "W_{fld} = \\arg\\max_W \\frac{\\left|W^{\\top} W_{pca}^{\\top} S_B W_{pca}W\\right|}{\\left|W^{\\top} W_{pca}^{\\top}S_W W_{pca}W\\right|},\n",
        "$$\n",
        "ã‚µãƒ³ãƒ—ãƒ«ã‚’ (c-1) æ¬¡å…ƒç©ºé–“ã«æŠ•å½±ã™ã‚‹å¤‰æ›è¡Œåˆ— $W$ ã¯æ¬¡ã®ã‚ˆã†ã«ä¸ãˆã‚‰ã‚Œã‚‹ï¼š\n",
        "<!-- The transformation matrix $W$, that projects a sample into the (c-1)-dimensional space is then given by:-->\n",
        "$$\\tag{18}\n",
        "W = W_{fld}^{\\top}W_{pca}^{\\top}.\n",
        "$$\n",
        "\n",
        "æœ€å¾Œã«æ³¨æ„ç‚¹ï¼š\n",
        "$S_W$ ã¨ $S_B$ ã¨ã¯å¯¾ç§°è¡Œåˆ—ã§ã‚ã‚‹ãŒï¼Œå¯¾ç§°è¡Œåˆ—åŒå£«ã®ç©ã¯å¿…ãšã—ã‚‚å¯¾ç§°ã§ã¯ãªã„ã®ã§ï¼Œä¸€èˆ¬è¡Œåˆ—ç”¨ã®å›ºæœ‰å€¤ã‚½ãƒ«ãƒã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã€‚\n",
        "OpenCV ã® `cv:eigen` ã¯ï¼Œç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯å¯¾ç§°è¡Œåˆ—ã«å¯¾ã—ã¦ã®ã¿å‹•ä½œã™ã‚‹ã€‚\n",
        "éå¯¾ç§°è¡Œåˆ—ã«å¯¾ã—ã¦ã¯ï¼Œç‰¹ç•°å€¤ã®å›ºæœ‰å€¤ã¯ç­‰ä¾¡ã§ã¯ãªã„ã®ã§ï¼Œç‰¹ç•°å€¤åˆ†è§£ï¼ˆSVDï¼‰ã‚½ãƒ«ãƒãƒ¼ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã¯ã§ããªã„ï¼\n",
        "<!-- one final note:\n",
        "Although $S_W$ and $S_B$ are symmetric matrices, the product of two symmetric matrices is not necessarily symmetric; thus, so you have to use an eigenvalue solver for general matrices.\n",
        "OpenCV's `cv:eigen` only works for symmetric matrices in it scurrent version; since eigenvlues of singular values are not equivalent for non-symmetric matrices you can not use a Singular Value Decomposition (SVD) eigher. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a8c08d-0222-45e7-ac36-ee675d0e306a",
      "metadata": {
        "id": "44a8c08d-0222-45e7-ac36-ee675d0e306a"
      },
      "source": [
        "\n",
        "## å›ºæœ‰é¡” Eigenfaces\n",
        "\n",
        "ä¸ãˆã‚‰ã‚ŒãŸç”»åƒè¡¨ç¾ã®å•é¡Œã¯ã€ãã®é«˜æ¬¡å…ƒæ€§ã§ã‚ã‚‹ã€‚\n",
        "2æ¬¡å…ƒã® $p\\times q$ ã®æ¿ƒæ·¡ç”»åƒã¯ $m=pq$ æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«ã¾ãŸãŒã‚‹ã®ã§ã€$100\\times100$ ç”»ç´ ã®ç”»åƒã¯ã™ã§ã« 10,000 æ¬¡å…ƒã®ç”»åƒç©ºé–“ã«ã‚ã‚‹ã€‚\n",
        "ã“ã‚Œã¯ã©ã‚“ãªè¨ˆç®—ã‚’ã™ã‚‹ã«ã‚‚å¤šã™ãã‚‹ãŒã€ã™ã¹ã¦ã®æ¬¡å…ƒãŒæœ¬å½“ã«æˆ‘ã€…ã«ã¨ã£ã¦æœ‰ç”¨ãªã®ã ã‚ã†ã‹ï¼Ÿ\n",
        "æˆ‘ã€…ã¯ãƒ‡ãƒ¼ã‚¿ã«åˆ†æ•£ãŒã‚ã‚‹å ´åˆã«ã®ã¿åˆ¤æ–­ã‚’ä¸‹ã™ã“ã¨ãŒã§ãã‚‹ã®ã§ã€æˆ‘ã€…ãŒæ¢ã—ã¦ã„ã‚‹ã®ã¯æƒ…å ±ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã‚‹æˆåˆ†ã§ã‚ã‚‹ã€‚\n",
        "ä¸»æˆåˆ†åˆ†æ (PCA) ã¯ã€ã‚«ãƒ¼ãƒ«ãƒ»ãƒ”ã‚¢ã‚½ãƒ³(1901) ã¨ãƒãƒ­ãƒ«ãƒ‰ãƒ»ãƒ›ãƒ†ãƒªãƒ³ã‚° (1933) ã«ã‚ˆã£ã¦ç‹¬è‡ªã«ææ¡ˆã•ã‚ŒãŸã‚‚ã®ã§ï¼Œç›¸é–¢ã—ã¦ã„ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å¤‰æ•°ã®é›†åˆã‚’ï¼Œã‚ˆã‚Šå°ã•ãªç›¸é–¢ã—ã¦ã„ãªã„å¤‰æ•°ã®é›†åˆã«å¤‰ãˆã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚\n",
        "ã“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€é«˜æ¬¡å…ƒã®ãƒ‡ãƒ¼ã‚¿é›†åˆã¯ã€ã—ã°ã—ã°ç›¸é–¢ã®ã‚ã‚‹å¤‰æ•°ã«ã‚ˆã£ã¦è¨˜è¿°ã•ã‚Œã€ã—ãŸãŒã£ã¦ã€ã„ãã¤ã‹ã®æ„å‘³ã®ã‚ã‚‹æ¬¡å…ƒã ã‘ãŒæƒ…å ±ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã‚ã‚‹ã€‚\n",
        "PCA ã¯ã€ä¸»æˆåˆ†ã¨å‘¼ã°ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ä¸­ã®æœ€å¤§ã®åˆ†æ•£ã‚’æŒã¤æ–¹å‘ã‚’è¦‹ã¤ã‘ã‚‹ã€‚\n",
        "<!-- The problem with the image representation we are given is its high dimensionality.\n",
        "Two-dimensional pxq grayscale images span a m=pq-dimensional vector space, so an image with 100x100 pixels lies in a 10,000-dimensional image space already.\n",
        "That's way too much for any computations, but are all dimensions really useful for us?\n",
        "We can only make a decision if there's any variance in data, so what we are looking for are the components that account for most of the information.\n",
        "The Principal Component Analysis (PCA) was independently proposed by Karl Pearson (1901) and Harold Hotelling (1933) to turn a set of possibly correlated variables into a smaller set of uncorrelated variables.\n",
        "The idea is that a high-dimensional dataset is often described by correlated variables and therefore only a few meaningful dimensions account for most of the information.\n",
        "The PCA method finds the directions with the greatest variance in the data, called principal components. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf5815a",
      "metadata": {
        "id": "faf5815a"
      },
      "source": [
        "### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ <!--Algorithmic Description-->\n",
        "\n",
        "$X=\\left\\{x_1,x_2,\\ldots,x_n\\right\\}$ ã¯ $x_i\\in\\mathbb{R}^{d}$ ã‹ã‚‰ã®ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã¨ã™ã‚‹:\n",
        "<!-- Let $X=\\left\\{x_1,x_2,\\ldots,x_n\\right\\}$ be a random vector with observation $x_i\\in\\mathbb{R}^{d}$. -->\n",
        "\n",
        "1. Compute the mean $\\mu$\n",
        "$$\\tag{1}\n",
        "\\mu=\\frac{1}{n}\\sum_{i=1}^{n}x_i.\n",
        "$$\n",
        "2. Compute the Covariance matrix $C$\n",
        "$$\\tag{2}\n",
        "C=\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)(x_i-\\mu)^{\\top}.\n",
        "$$\n",
        "3. Compute the eigenvalues $\\lambda_i$ and eigenvectors $\\nu_i$ of $C$\n",
        "$$\\tag{3}\n",
        "C\\nu_i=\\lambda_i\\nu_i, \\text{\\hspace{1cm} $i=1,2,\\ldots,n$}\n",
        "$$\n",
        "4. Order the eigenvalues descending by their eigenvalue.\n",
        "The $k$ principla components are the eigenvectors corresponding to the $k$ largest eigenvalues.\n",
        "\n",
        "è¦³æ¸¬ãƒ™ã‚¯ãƒˆãƒ« $x$ ã® $k$ å€‹ã®ä¸»æˆåˆ†ã¯æ¬¡å¼ã§ä¸ãˆã‚‰ã‚Œã‚‹ï¼š<!-- The k principal components of the observed vector $x$ are then given by: -->\n",
        "\n",
        "$$\\tag{4}\n",
        "y = W^{\\top}(x-\\mu),\n",
        "$$\n",
        "\n",
        "ã“ã“ã§ $W=\\left(v_1,v_2,\\ldots,v_k\\right)$.\n",
        "<!--where $W=left(v_1,v_2,\\ldots,v_k\\right)$.-->\n",
        "PCA åŸºåº•ã‹ã‚‰ã®å†æ§‹æˆã¯æ¬¡å¼:<!--The reconstruction from the PCA basis is given by:-->\n",
        "$$\\tag{5}\n",
        "x= Wy + \\mu.\n",
        "$$\n",
        "ãã—ã¦å›ºæœ‰é¡”æ³•ã¯ã€æ¬¡ã®ã‚ˆã†ã«ã—ã¦é¡”èªè­˜ã‚’è¡Œã†ï¼š<!-- The Eigenfaces method then performs face recognition by: -->\n",
        "1. ã™ã¹ã¦ã®è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã‚’ PCA ã®éƒ¨åˆ†ç©ºé–“ã«å°„å½±ã™ã‚‹  (å¼ (4) )ã€‚\n",
        "2. ã‚¯ã‚¨ãƒªç”»åƒã‚’ PCA éƒ¨åˆ†ç©ºé–“ã«å°„å½±ã™ã‚‹ (ãƒªã‚¹ãƒˆ 5 ã‚’ä½¿ç”¨)ã€‚\n",
        "3. æŠ•å½±ã•ã‚ŒãŸè¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã¨æŠ•å½±ã•ã‚ŒãŸã‚¯ã‚¨ãƒªç”»åƒã®é–“ã®æœ€è¿‘å‚ã‚’è¦‹ã¤ã‘ã‚‹ã€‚\n",
        "\n",
        "<!-- 1. Projecting all training samples into the PCA subspace (using Equation 4).\n",
        "2. Projecting the query image into the PCA subspace (using Listing 5).\n",
        "3. Finding the nearest neighbor between the projected training samples and the projected query image. -->\n",
        "\n",
        "ã¾ã ã€è§£æ±ºã—ãªã‘ã‚Œã°ãªã‚‰ãªã„å•é¡ŒãŒ 1 ã¤æ®‹ã£ã¦ã„ã‚‹ã€‚\n",
        "100x100 ç”»ç´ ã®ç”»åƒãŒ 400 æšä¸ãˆã‚‰ã‚ŒãŸã¨ã™ã‚‹ã€‚\n",
        "ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã¯å…±åˆ†æ•£è¡Œåˆ— $C=XX^{\\top}$ ã‚’è§£ããŒã€ã“ã®ä¾‹ã§ã¯ size(X)=$10000\\times400$ ã§ã‚ã‚‹ã€‚\n",
        "çµå±€ 10000x10000 ã®è¡Œåˆ—ã«ãªã‚Šã€ã–ã£ã¨ 0.8GB ã«ãªã‚‹ã€‚\n",
        "ã“ã®å•é¡Œã‚’è§£ãã®ã¯ç¾å®Ÿçš„ã§ã¯ãªã„ã®ã§ã€ãƒˆãƒªãƒƒã‚¯ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n",
        "ç·šå½¢ä»£æ•°ã®æˆæ¥­ã§ã€$M>N$ ã® $M\\times N$ è¡Œåˆ—ã¯ $N-1$ å€‹ã®éã‚¼ãƒ­å›ºæœ‰å€¤ã—ã‹æŒã¦ãªã„ã“ã¨ã‚’çŸ¥ã£ã¦ã„ã‚‹ã ã‚ã†ã€‚\n",
        "ãã“ã§ã€ä»£ã‚ã‚Šã«ã‚µã‚¤ã‚º $N\\times N$ ã®å›ºæœ‰å€¤åˆ†è§£ $S=X^{\\top}X$ ã‚’å–ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š\n",
        "<!-- Still there is one problem left to solve.\n",
        "Imagin we are given 400 images sized 100x100 pixels.\n",
        "The Principal Component Analysis (PCA) solves the covariance matrix $C=XX^{\\top}$, where size(X)=10000X400 in our example.\n",
        "You would end up with a 10000x10000 matrix, rougly 0.8GB.\n",
        "Solving this  problem is not feasible, so we will need to apply a trick.\n",
        "From your linear algebra lessons you know that a MxN matrix with M>N can only have N-1 non-zero eigenvalues.\n",
        "So it is possible to take the eigenvalue decompostion $S=X^{\\top}X$ of size NxN instead: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qxf4e2iNb7Kp",
      "metadata": {
        "id": "qxf4e2iNb7Kp"
      },
      "outputs": [],
      "source": [
        "def LDA(X, y, num_comp=0):\n",
        "    \"\"\"ç·šå½¢åˆ¤åˆ¥åˆ†æ LDA: Linear Discriminant Analysis\n",
        "        X: ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—\n",
        "        y: X ã®å„è¡Œã«å¯¾å¿œã™ã‚‹ç¾¤ãƒ©ãƒ™ãƒ«ã‚’æ ¼ç´ã—ãŸ numpy.array ãƒªã‚¹ãƒˆ\n",
        "    \"\"\"\n",
        "    #y = np.asarray(y)\n",
        "    [n,d] = X.shape         # ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—ã® è¡Œ:n ã¨ åˆ—:d ã‚’æ±‚ã‚ã‚‹\n",
        "    n_class = np.unique(y)  # ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ç¾¤ã®æ•°\n",
        "    if (num_comp <= 0) or (num_comp > (len(n_class)-1)):\n",
        "        num_comp = (len(n_class)-1)\n",
        "\n",
        "    Total_mean = X.mean(axis=0)             # å…¨å¹³å‡ã®è¨ˆç®—\n",
        "    Sw = np.zeros((d,d), dtype=np.float32)  # ç¾¤å†…åˆ†æ•£ä¿å­˜ç”¨\n",
        "    Sb = np.zeros((d,d), dtype=np.float32)  # ç¾¤é–“åˆ†æ•£ä¿å­˜ç”¨\n",
        "    for i in n_class:                       # å„ç¾¤ã«ã¤ã„ã¦ç¹°ã‚Šè¿”ã™\n",
        "        Xi = X[np.where(y==i)[0],:]\n",
        "        G_mean = Xi.mean(axis=0)                                             # ç¾¤å¹³å‡\n",
        "        Sw = Sw + np.dot((Xi - G_mean).T, (Xi - G_mean))                     # ç¾¤å†…åˆ†æ•£\n",
        "        Sb = Sb + n * np.dot((G_mean - Total_mean).T, (G_mean - Total_mean)) # ç¾¤é–“åˆ†æ•£\n",
        "    Eigenvalues, Eigenvectors = np.linalg.eig(np.linalg.inv(Sw) * Sb)\n",
        "    idx = np.argsort(-Eigenvalues.real)\n",
        "    Eigenvalues, Eigenvectors = Eigenvalues[idx], Eigenvectors[:,idx]\n",
        "    Eigenvalues = np.array(Eigenvalues[0:num_comp].real, dtype=np.float32, copy=True)\n",
        "    Eigenvectors = np.array(Eigenvectors[0:,0:num_comp].real, dtype=np.float32, copy=True)\n",
        "    return Eigenvalues, Eigenvectors\n",
        "\n",
        "\n",
        "def Fisherfaces(X,y,num_comp=0):\n",
        "    #y = np.asarray(y)\n",
        "    [n,d] = X.shape\n",
        "    n_class = len(np.unique(y))\n",
        "    #[Eigenvalues_pca, Eigenvectors_pca, mu_pca] = pca(X, y, (n-n_class))\n",
        "    Eigenvalues_pca, Eigenvectors_pca, mu_pca = _PCA.Eigenvalues, _PCA.Eigenvectors, _PCA.mean\n",
        "    #[Eigenvalues_lda, Eigenvectors_lda] = LDA(project(Eigenvectors_pca, X, mu_pca), y, num_comp)\n",
        "    Eigenvalues_lda, Eigenvectors_lda = LDA(_PCA.projections, y, num_comp)\n",
        "    Eigenvectors = np.dot(Eigenvectors_pca, Eigenvectors_lda)\n",
        "    return Eigenvalues_lda, Eigenvectors #, mu_pca\n",
        "\n",
        "Fisher_Eignevalues, Fisher_Eigenvectors = Fisherfaces(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49KNaJr6b-Gu",
      "metadata": {
        "id": "49KNaJr6b-Gu"
      },
      "source": [
        "## ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼é¡”å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®è¦–è¦šåŒ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GbjuPPF2cAHS",
      "metadata": {
        "id": "GbjuPPF2cAHS"
      },
      "outputs": [],
      "source": [
        "# help(np.set_printoptions)\n",
        "N_pca = 10\n",
        "fix, axes = plt.subplots(nrows=1, ncols=N_pca, figsize=(N_pca * 1.4, 1.4))\n",
        "for i in range(N_pca):\n",
        "    axes[i].imshow(Fisher_Eigenvectors[:,i].reshape(64,64), cmap=\"gray\")\n",
        "    axes[i].set_title(f'ç¬¬ {i+1} å›ºæœ‰å€¤ã«å¯¾å¿œ\\nã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«', size=7)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xk4VsFCVcB8B",
      "metadata": {
        "id": "xk4VsFCVcB8B"
      },
      "outputs": [],
      "source": [
        "Fisher_projections = _PCA._X @ Fisher_Eigenvectors\n",
        "Fisher_reconstruct = Fisher_projections @ Fisher_Eigenvectors.T + _PCA.mean\n",
        "print(Fisher_reconstruct.shape, Fisher_projections.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b81111f-fed8-4adc-8a7f-7eac1ba632c3",
      "metadata": {
        "id": "0b81111f-fed8-4adc-8a7f-7eac1ba632c3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(Fisher_Eigenvectors.T @ Fisher_Eigenvectors, center=0, vmin=-1., vmax=1., square=True, cmap='gnuplot2')\n",
        "plt.title('å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®ç©ã‚’å¯è¦–åŒ–')\n",
        "plt.show()\n",
        "\n",
        "print((Fisher_Eigenvectors.T @ Fisher_Eigenvectors)[:5,:5])\n",
        "print((Fisher_Eigenvectors.T @ Fisher_Eigenvectors)[-10:,-10:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a4b574b-4006-41a9-b224-44461da86fd2",
      "metadata": {
        "id": "9a4b574b-4006-41a9-b224-44461da86fd2"
      },
      "outputs": [],
      "source": [
        "plt.plot(Fisher_Eignevalues[3:])\n",
        "plt.title('Fisher é¡”ã®å›ºæœ‰å€¤ãƒ—ãƒ­ãƒƒãƒˆ')\n",
        "plt.show()\n",
        "print(Fisher_Eignevalues[3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BzdPZLZEcFA3",
      "metadata": {
        "id": "BzdPZLZEcFA3"
      },
      "outputs": [],
      "source": [
        "# å›ºæœ‰é¡” (PCA) ã‚’ç”¨ã„ã¦ 40 åã®é¡”èªè­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¡ã„ã¦ 3 æ‰‹æ³•ã‚’æ¯”è¼ƒ\n",
        "X_train, X_test, y_train, y_test=train_test_split(Fisher_projections, y, test_size=split_ratio, stratify=y, random_state=42)\n",
        "#X_train, X_test, y_train, y_test=train_test_split(_PCA.projections, y_sex, test_size=split_ratio, stratify=y, random_state=42)\n",
        "\n",
        "# å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "params = {'max_iter':10 ** 4,\n",
        "          #'C':1e3,\n",
        "          #'penalty':'l2'\n",
        "         }\n",
        "\n",
        "# 3 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©\n",
        "svc_model = SVC(**params)\n",
        "logistic_model = LogisticRegression(**params)\n",
        "lda_model = LinearDiscriminantAnalysis()\n",
        "\n",
        "# å‡ºåŠ›ã™ã‚‹å›³ã®ã‚µã‚¤ã‚ºã‚’å®šç¾©\n",
        "fig, _axes = plt.subplots(ncols=3, nrows=1, figsize=(12,4), constrained_layout=True)\n",
        "\n",
        "# 3 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãã‚Œãã‚Œå®Ÿè¡Œã—ã¦çµæœã‚’æç”»\n",
        "for i, (model_name, model) in enumerate([('ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒã‚·ãƒ³',svc_model),\n",
        "                                         ('ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°',logistic_model),\n",
        "                                         ('ç·šå½¢åˆ¤åˆ¥åˆ†æ',lda_model)]):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_hat = model.predict(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦äºˆæ¸¬ã‚’è¡Œã„çµæœã‚’ y_hat ã«æ ¼ç´\n",
        "\n",
        "    _axes[i].imshow(metrics.confusion_matrix(y_test,y_hat), cmap='gray')\n",
        "    _axes[i].set_title(f'{model_name}:åˆ†é¡ç²¾åº¦:{metrics.accuracy_score(y_test,y_hat):.2f}')\n",
        "    _axes[i].axis('off')\n",
        "\n",
        "    # print(model_name, '\\n', metrics.confusion_matrix(y_test,y_hat))\n",
        "    # print(model_name, '\\n', metrics.classification_report(y_test,y_hat))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03kLjQfcIcL",
      "metadata": {
        "id": "e03kLjQfcIcL"
      },
      "outputs": [],
      "source": [
        "Fisher_reconstruct.shape\n",
        "plt.figure(figsize=(2.5,2.5))\n",
        "plt.imshow(Fisher_reconstruct[-1].reshape(64,64), cmap='gray')\n",
        "plt.title(f'ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼é¡” å†æ§‹æˆ')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJ8z9kxHcMOE",
      "metadata": {
        "id": "yJ8z9kxHcMOE"
      },
      "outputs": [],
      "source": [
        "# PCA ã«ã‚ˆã‚‹æ¬¡å…ƒåœ§ç¸®ã®çµæœã‚’è¡¨ç¤º\n",
        "\n",
        "XX = Fisher_projections\n",
        "XX = Fisher_reconstruct\n",
        "\n",
        "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(14,4))\n",
        "axes[0].scatter(XX[:,0], XX[:,1], s=1)\n",
        "axes[0].set_xlabel('ç¬¬ 1 ä¸»æˆåˆ†')\n",
        "axes[0].set_ylabel('ç¬¬ 2 ä¸»æˆåˆ†')\n",
        "for i in range(XX.shape[0]):\n",
        "    axes[0].annotate(str(y[i]), (XX[i,0], XX[i,1]))\n",
        "\n",
        "axes[1].scatter(XX[:,1], XX[:,2], s=1)\n",
        "axes[1].set_xlabel('ç¬¬ 2 ä¸»æˆåˆ†')\n",
        "axes[1].set_ylabel('ç¬¬ 3 ä¸»æˆåˆ†')\n",
        "for i in range(XX.shape[0]):\n",
        "    axes[1].annotate(str(y[i]), (XX[i,1], XX[i,2]))\n",
        "\n",
        "axes[2].scatter(XX[:,0], XX[:,3], s=1)\n",
        "axes[2].set_xlabel('ç¬¬ 1 ä¸»æˆåˆ†')\n",
        "axes[2].set_ylabel('ç¬¬ 4 ä¸»æˆåˆ†')\n",
        "for i in range(XX.shape[0]):\n",
        "    axes[2].annotate(str(y[i]), (XX[i,0], XX[i,3]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe102e6-47b5-45b8-bfb9-d14c786ad49d",
      "metadata": {
        "id": "8fe102e6-47b5-45b8-bfb9-d14c786ad49d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89812df-8736-4a5f-afc9-5d1c5dae4a76",
      "metadata": {
        "id": "d89812df-8736-4a5f-afc9-5d1c5dae4a76"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}