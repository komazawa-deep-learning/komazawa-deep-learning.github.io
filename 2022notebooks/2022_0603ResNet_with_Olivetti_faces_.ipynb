{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0603ResNet_with_Olivetti_faces_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22cbe53-8b42-4fcc-947b-6e490e8b2b5a",
      "metadata": {
        "id": "f22cbe53-8b42-4fcc-947b-6e490e8b2b5a"
      },
      "source": [
        "# ResNet 実習\n",
        "\n",
        "<center>\n",
        "<img src='https://komazawa-deep-learning.github.io/assets/ResNet_Fig2.svg' width=\"33%\"><br/>\n",
        "<img src='https://komazawa-deep-learning.github.io/assets/2015ResNet30.svg' width=\"94%\"><br/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "726e991e-daa5-4c4c-991b-dcc506f38abe",
      "metadata": {
        "id": "726e991e-daa5-4c4c-991b-dcc506f38abe"
      },
      "source": [
        "# オリベッティ顔画像データについて\n",
        "\n",
        "40 の異なる被写体について，それぞれ 10 種類の画像がある。\n",
        "被写体によっては 被写体によっては，撮影時刻，照明条件，表情 (開目けている／閉眼、笑っている／笑っていない)，細部 (眼鏡有り／無し) を変えて撮影した。\n",
        "すべての画像は，暗い均質な背景で，被験者は直立し，正面を向いた姿勢で撮影された。 (多少の横の動きは許容している）。\n",
        "\n",
        "*    Classes                                40\n",
        "*    Samples total                         400\n",
        "*    Dimensionality                       4096\n",
        "*    Features            real, between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb12613-eaf9-4d3a-8298-aab1af02c742",
      "metadata": {
        "id": "7bb12613-eaf9-4d3a-8298-aab1af02c742"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f'torch.__version__:{torch.__version__}')\n",
        "\n",
        "# リソースの選択（CPU/GPU/MPS）\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "import os\n",
        "try:\n",
        "    import ipynb_path\n",
        "except ImportError:\n",
        "    !pip install ipynb-path\n",
        "    import ipynb_path\n",
        "__file__ = os.path.basename(ipynb_path.get())\n",
        "print(f'__file__:{__file__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca3f571-57e0-4132-9fc8-2c56b17a1e35",
      "metadata": {
        "id": "8ca3f571-57e0-4132-9fc8-2c56b17a1e35"
      },
      "source": [
        "## `torch.nn.Conv2d` の引数\n",
        "\n",
        "* in_channels: int, 入力特徴数，(チャンネル数)\n",
        "* out_channels: int, 出力特徴数 (チャンネル数)\n",
        "* kernel_size: Union[int, Tuple[int, int]], カーネルサイズ，数字を 1 つだけ与えると 縦横とも同じサイズのカーネル幅になる\n",
        "* stride: Union[int, Tuple[int, int]] = 1,  ストライド，カーネルをスライドさせる幅，数字を 1 だけ与えると縦横とも同サイズの幅となる\n",
        "* padding: Union[str, int, Tuple[int, int]] = 0, 4 角に加える幅，数字を 1 つだけ与えると上下左右とも同サイズの幅にまる。\n",
        "W x H の画像に対して，横幅は，W_pad + W + W_pad となり，縦長は H_pad + H + H_pad となるので，H x W の入力データが (W + 2 W_pad) * (H + 2 H_pad) のサイズとなる\n",
        "* dilation: Union[int, Tuple[int, int]] = 1\n",
        "ダイレーションの幅，畳み込みカーネルの間隙を指定する\n",
        "* groups: int = 1,\n",
        "* bias: bool = True,\n",
        "バイアス項 `X @ w + b` にするときの `b` のこと\n",
        "* padding_mode: str = 'zeros', device=None, dtype=None) -> None        \n",
        "`padding` で指定した 4 角の拡張領域をどのような数値で埋めるかを指定する。デフォルトでは `zeros` すなわち 0 パディングとなる。\n",
        "そのほかに取りうる値は，`reflect`, `replicate`, `circular` である。\n",
        "\n",
        "$$\n",
        "H_{\\text{out}} = \\frac{H_{in} + 2 H_{\\text{pad}} - H_{\\text{dilation}}\\times \\left(H_{\\text{kernel}} -1\\right)+1}{H_{\\text{stride}}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_{\\text{out}} = \\frac{W_{in} + 2 W_{\\text{pad}} - W_{\\text{dilation}}\\times \\left(W_{\\text{kernel}} -1\\right)+1}{W_{\\text{stride}}}\n",
        "$$\n",
        "\n",
        "## `torch.nn.MacPool2d` の引数\n",
        "\n",
        "* kernel_size: Union[int, Tuple[int, ...]],\n",
        "* stride: Union[int, Tuple[int, ...], NoneType] = None,\n",
        "* padding: Union[int, Tuple[int, ...]] = 0,\n",
        "* dilation: Union[int, Tuple[int, ...]] = 1,\n",
        "* return_indices: bool = False,\n",
        "* ceil_mode: bool = False) -> None\n",
        "\n",
        "$$\n",
        "out(N_i,C_j,h,w)= \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1}\n",
        "\\text{input}\\left(N_i, C_j, \\text{stride[0]} \\times h + m, \\text{stride[1]} \\times w + n\\right)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9c74c6ed-5286-4fd3-be20-855a211449ab",
      "metadata": {
        "id": "9c74c6ed-5286-4fd3-be20-855a211449ab"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "def conv3x3(\n",
        "    in_features:int,\n",
        "    out_features:int,\n",
        "    stride:int=1,\n",
        "    groups:int=1,\n",
        "    dilation:int=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_channels=in_features,\n",
        "        out_channels=out_features,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(\n",
        "    in_features:int,\n",
        "    out_features:int,\n",
        "    stride:int=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_channels=in_features,\n",
        "        out_channels=out_features,\n",
        "        kernel_size=1,\n",
        "        stride=stride,\n",
        "        bias=False)\n",
        "\n",
        "\n",
        "class ResNet_BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_features:int,\n",
        "                 out_features:int,\n",
        "                 stride:int=1,\n",
        "                 downsample:bool=None,\n",
        "                 groups:int=1,\n",
        "                 base_width:int=64,\n",
        "                 dilation:int=1,\n",
        "                 norm_layer:bool=None):\n",
        "\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self.conv1 = conv3x3(\n",
        "            in_features=in_features,\n",
        "            out_features=out_features,\n",
        "            stride=stride)\n",
        "        self.bn1 = norm_layer(out_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_features, out_features)\n",
        "        self.bn2 = norm_layer(out_features)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2eefe4fe-822c-4627-8250-35a581d11779",
      "metadata": {
        "id": "2eefe4fe-822c-4627-8250-35a581d11779"
      },
      "outputs": [],
      "source": [
        "class ResNet_Bottleneck(nn.Module):\n",
        "    # ボトルネックは、ダウンサンプリングのストライドを 3x3 convolution(self.conv2) に置くのに対し、\n",
        "    # オリジナルの実装では最初の 1x1 convolution(self.conv1) にしています。\n",
        "    # [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385) によると、\n",
        "    # ダウンサンプリングのストライドを 3x3 convolution(self.conv2) にしています。\n",
        "    # このバージョンは ResNet V1.5 としても知られており、精度が向上しています。\n",
        "    # (https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch)\n",
        "\n",
        "    #expansion: int = 4\n",
        "\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 stride=1,\n",
        "                 downsample=None,\n",
        "                 groups=1,\n",
        "                 base_width=64,\n",
        "                 dilation=1,\n",
        "                 norm_layer= None):\n",
        "\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(out_featuress * (base_width / 64.0)) * groups\n",
        "\n",
        "        self.expansion = 4\n",
        "\n",
        "        self.conv1 = conv1x1(in_features, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, otu_features * self.expansion)\n",
        "        self.bn3 = norm_layer(out_features * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, inp):\n",
        "        identity = inp\n",
        "\n",
        "        out = self.conv1(inp)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 block,\n",
        "                 layers,\n",
        "                 in_channels = 3,\n",
        "                 num_classes = 40, # 1000,\n",
        "                 zero_init_residual=False,\n",
        "                 groups=1,\n",
        "                 width_per_group=64,\n",
        "                 #width_per_group=8,\n",
        "                 norm_layer=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.in_features = 64\n",
        "        self.dilation = 1\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(in_channels, self.in_features, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.in_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # 各残差ブランチの最後のバッチ正規化をゼロ初期化し，残差ブランチがゼロで始まり，各残差ブロックが恒等写像のように振る舞うようにします。\n",
        "        # https://arxiv.org/abs/1706.02677 によると，これによりモデルが 0.2~0.3 %改善されます。\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self,\n",
        "                    block,\n",
        "                    out_features,\n",
        "                    blocks,\n",
        "                    stride=1):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if stride != 1 or self.in_features != out_features * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.in_features, out_features * block.expansion, stride),\n",
        "                norm_layer(out_features * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.in_features, out_features, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.in_features = out_features * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.in_features,\n",
        "                    out_features,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    \"\"\"ResNet-18 model from [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "    \"\"\"\n",
        "    return _resnet(\"resnet18\", ResNet_BasicBlock, [1, 1, 1, 1], **kwargs)\n",
        "    #return _resnet(\"resnet18\", ResNet_BasicBlock, [2, 2, 2, 2], **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81bacbf6-9287-433d-8cc9-dfaaa6c637d0",
      "metadata": {
        "id": "81bacbf6-9287-433d-8cc9-dfaaa6c637d0"
      },
      "outputs": [],
      "source": [
        "in_channels=1\n",
        "model = resnet18(in_channels=in_channels, num_classes=40).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5831e8d8-4f5b-4eb1-b407-aa570db9f876",
      "metadata": {
        "id": "5831e8d8-4f5b-4eb1-b407-aa570db9f876"
      },
      "source": [
        "$$\n",
        "\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
        "l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
        "w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},\n",
        "$$\n",
        "ここで $x$ は入力，$y$ は教師信号，$w$  は結合係数 (重み), $N$ はバッチサイズ。\n",
        "`reduction` が `none` (デフォルト設定では `mean`) でなければ，以下:\n",
        "<!-- where $x$ is the input, $y$ is the target, $w$ is the weight, and $N$ is the batch size.\n",
        "If `reduction` is not `none` (default `mean` ), then -->\n",
        "$$\n",
        "\\ell(x, y) = \\begin{cases}\n",
        "\\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &\n",
        "\\text{if reduction} = \\text{`mean';}\\\\\n",
        "\\sum_{n=1}^N l_n,  &\n",
        "\\text{if reduction} = \\text{`sum'.}\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ec8d2413-2c88-4f68-bd08-1d4f8e923112",
      "metadata": {
        "id": "ec8d2413-2c88-4f68-bd08-1d4f8e923112"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# モデル\n",
        "in_channels=1\n",
        "model = resnet18(in_channels=in_channels, num_classes=40).to(device)\n",
        "\n",
        "#loss_f = nn.NLLLoss()\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "optim_f = optim.Adam(model.parameters(),lr=0.0001, weight_decay=0.001)\n",
        "\n",
        "\n",
        "# 学習・評価\n",
        "def calc_loss(label, pred):\n",
        "    return loss_f(pred, label)\n",
        "\n",
        "def train_step(x, y):\n",
        "    model.train()\n",
        "    preds = model(x)\n",
        "    loss = calc_loss(y, preds)\n",
        "    optim_f.zero_grad()\n",
        "    loss.backward()\n",
        "    optim_f.step()\n",
        "\n",
        "    return loss, preds\n",
        "\n",
        "def test_step(x, y):\n",
        "    model.eval()\n",
        "    preds = model(x)\n",
        "    loss = calc_loss(y, preds)\n",
        "\n",
        "    return loss, preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35000912-ec57-4778-ae33-6c4610eb1d1e",
      "metadata": {
        "id": "35000912-ec57-4778-ae33-6c4610eb1d1e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = fetch_olivetti_faces()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# split_ratio = 0.2 としているので，訓練データ対テストデータが 8:2 になります\n",
        "split_ratio = 0.2\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=split_ratio,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=0)\n",
        "print(f'オリジナル X サイズ:{X.shape}',\n",
        "      f'X_train 訓練画像のサイズ: {X_train.shape}')\n",
        "print(f'オリジナル y サイズ:{y.shape}',\n",
        "      f'y_train 教師信号データのサイズ: {y_train.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrows = 40   # nrows 人分のデータを表示\n",
        "nrows = 18\n",
        "ncols = 10\n",
        "fig, fig_axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 1.4, nrows * 1.4), constrained_layout=True)\n",
        "# constrained_layout は subplot や 凡例やカラーバーなどの装飾を自動的に調整して，\n",
        "# ユーザが要求する論理的なレイアウトをできるだけ維持しながら， 図ウィンドウに収まるようにします。\n",
        "\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        x = i * 10 + j\n",
        "        fig_axes[i][j].imshow(X[x].reshape(64,64), cmap='gray')\n",
        "        fig_axes[i][j].axis('off')\n",
        "        fig_axes[i][j].set_title(f'num:{x}, ID:{y[x]}')"
      ],
      "metadata": {
        "id": "kRUinsf-JKd0"
      },
      "id": "kRUinsf-JKd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f15dd4d5-d65a-4f41-a1c1-bb577d2f7181",
      "metadata": {
        "id": "f15dd4d5-d65a-4f41-a1c1-bb577d2f7181"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 乱数シード固定（再現性の担保）\n",
        "def fix_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "\n",
        "# データローダの下位過程の乱数 seed 固定\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "\n",
        "for i in range(8):\n",
        "    worker_init_fn(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d2a9c691-d44c-48e5-b3c0-c9a652077de7",
      "metadata": {
        "id": "d2a9c691-d44c-48e5-b3c0-c9a652077de7"
      },
      "outputs": [],
      "source": [
        "# データセットの作成\n",
        "class _dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super().__init__()\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        feature = self.X[index]\n",
        "        label = self.y[index]\n",
        "        return feature, label\n",
        "\n",
        "\n",
        "X_ = torch.tensor(X_train).float().reshape(-1,1,64,64)\n",
        "#X_ = torch.reshape(torch.tensor(X_train).float(), (-1,1,64,64))\n",
        "y_ = torch.tensor(y_train).long()\n",
        "Xtest_ = torch.tensor(X_test).float().reshape(-1,1,64,64)\n",
        "ytest_ = torch.tensor(y_test).long()\n",
        "\n",
        "train_dataset = _dataset(X_, y_)\n",
        "test_dataset = _dataset(Xtest_, ytest_)\n",
        "\n",
        "# データローダーの作成\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=64,    # バッチサイズ\n",
        "                                               shuffle=True,     # データシャッフル\n",
        "                                               num_workers=0,    # 高速化\n",
        "                                               pin_memory=True,  # 高速化\n",
        "                                               worker_init_fn=worker_init_fn\n",
        "                                              )\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                              batch_size=64,\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=0,\n",
        "                                              pin_memory=True,\n",
        "                                              worker_init_fn=worker_init_fn\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ab3d50-0df9-47e2-aaea-181a12ef3b2a",
      "metadata": {
        "id": "80ab3d50-0df9-47e2-aaea-181a12ef3b2a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "for i in range(8):\n",
        "    worker_init_fn(i)\n",
        "\n",
        "epochs = 20\n",
        "train_losses, test_losses, train_accs, test_accs = [], [], [], []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss, test_loss, train_acc, test_acc = 0., 0., 0., 0.\n",
        "    for (x, y) in train_dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        loss, preds = train_step(x, y)\n",
        "        train_loss += loss.item()\n",
        "        train_acc += accuracy_score(y.tolist(), preds.argmax(dim=-1).tolist())\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_losses.append(train_loss)\n",
        "    train_acc /= len(train_dataloader)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    for (x, y) in test_dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        loss, preds = test_step(x, y)\n",
        "        test_loss += loss.item()\n",
        "        test_acc += accuracy_score(y.tolist(), preds.argmax(dim=-1).tolist())\n",
        "\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_acc /= len(test_dataloader)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    print(f'エポック: {epoch + 1:2d},',\n",
        "          f'訓練損失: {train_loss:.3f}',\n",
        "          f'テスト損失: {test_loss:.3f},',\n",
        "          f'訓練精度:{train_acc:.3f}',\n",
        "          f'検証精度: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2d4a59-c4f6-4910-b7ab-4f40947dbfd9",
      "metadata": {
        "id": "9c2d4a59-c4f6-4910-b7ab-4f40947dbfd9"
      },
      "outputs": [],
      "source": [
        "# 学習進行状況の描画\n",
        "plt.plot(train_losses, label='訓練損失', c='red')\n",
        "plt.plot(test_losses, label='テスト損失', c='cyan')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_accs, label=\"訓練精度\", c='red')\n",
        "plt.plot(test_accs, label=\"テスト精度\", c='cyan')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdaafa61-1333-42c0-82ce-6b6b83620483",
      "metadata": {
        "id": "bdaafa61-1333-42c0-82ce-6b6b83620483"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2021_1028Olivetti_face_ResNet_from_Pytorch_source.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}