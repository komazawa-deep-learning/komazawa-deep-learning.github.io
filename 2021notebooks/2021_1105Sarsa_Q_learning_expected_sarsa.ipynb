{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105Sarsa_Q_learning_expected_sarsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4xBE-UFfaP8"
      },
      "source": [
        "# TD (時間差)学習, SARSA, 期待 SARSA, Q 学習 と Python 実装\n",
        "\n",
        "<!-- date: 2020-0519-->\n",
        "- title: Reinforcement learning: Temporal-Difference, SARSA, Q-Learning & Expected SARSA in python\n",
        "- author: Vaibhav Kumar\n",
        "- Date: May 9, 2019\n",
        "- Original: <https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e>\n",
        "\n",
        "<!--\n",
        "# TD, SARSA, Q-Learning and Expected SARSA along with their python implementation and comparison\n",
        "-->\n",
        "\n",
        "<!--\n",
        "> If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-diff\n",
        "erence (TD) learning. — Andrew Barto and Richard S. Sutton\n",
        "-->\n",
        "\n",
        "> 強化学習の中心的で斬新なアイデアを一つ挙げるとすれば、それは間違いなく時間差（TD）学習であろう。<br/>\n",
        ">    -- アンドリュー・バルト， リチャード・S・サットン\n",
        "\n",
        "<!-- # Pre-requisites\n",
        "- Basics of Reinforcement learning\n",
        "- Markov chains, Markov Decision Process (MDPs)\n",
        "- Bellman equation\n",
        "- Value, policy functions and iterations\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06IN-g2JSBS5"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "\n",
        "# if isColab:\n",
        "#     !sudo apt-get update\n",
        "#     !sudo apt-get dist-upgrade\n",
        "#     !sudo apt install xorg-dev libx11-dev libgl1-mesa-glx\n",
        "#     !pip install gym[toy_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1vuW4H0SkXx"
      },
      "outputs": [],
      "source": [
        "!pip install \"gymnasium[classic-control]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfeZ4ohUoPO8"
      },
      "source": [
        "## 0.1. 前提知識\n",
        "\n",
        "- 強化学習の基本\n",
        "- マルコフ連鎖, マルコフ決定プロセス(MDP)\n",
        "- ベルマン方程式\n",
        "- 価値，方針関数，反復\n",
        "\n",
        "<!-- # Model-dependent and model-free reinforcement learning\n",
        "Model-dependent RL algorithms (namely value and policy iterations) work with the help of a transition table. A transiti\n",
        "on table can be thought of as a life hack book which has all the knowledge the agent needs to be successful in the worl\n",
        "d it exists in. Naturally, writing such a book is very tedious and impossible in most cases which is why model dependen\n",
        "t learning algorithms have little practical use.\n",
        "\n",
        "Temporal Difference is a model-free reinforcement learning algorithm. This means that the agent learns through actual e\n",
        "xperience rather than through a readily available all-knowing-hack-book (transition table). This enables us to introduc\n",
        "e stochastic elements and large sequences of state-action pairs. The agent has no idea about the reward and transition\n",
        "systems. It does not know what will happen on taking an arbitrary action at an arbitrary state. The agent has to intera\n",
        "ct with “world” or “environment” and find out for itself.-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeURXFYXoRoZ"
      },
      "source": [
        "## 0.2 強化学習におけるモデル依存学習とモデル自由学習\n",
        "\n",
        "モデル依存 強化学習 (RL) アルゴリズム（すなわち，価値と方策反復）は，遷移表に基づいて動作する。遷移表は，動作主 (エージェント) が存在する世界で成功するために必要なすべての知識が書かれたライフハック本と考えることができる。当然ながら，そのような本を書くのは非常に面倒で，ほとんどの場合不可能である。\n",
        "\n",
        "TD (Temporal Difference) はモデルフリー型の強化学習アルゴリズムである。TD 学習，動作主 (エージェント) が即時に入手可能な全知全能のハックブック (遷移表) ではなく，実際の経験を通して学習することを意味する。これにより，確率的な要素と状態と行動との対の大規模な系列を導入することが可能となる。\n",
        "\n",
        "* 動作主 (エージェント) は，報酬システムと遷移システムについては何も知らない。\n",
        "* 動作主 (エージェント) は，任意の状態で任意の行動をとった場合に何が起こるかを知らない。\n",
        "* 動作主 (エージェント) は，「世界」や「環境」と相互作用し、自分自身で見つけ出さなければならない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSE2US7Ml-9A"
      },
      "source": [
        "\n",
        "## 0.3 TD 学習 (時差学習)\n",
        "\n",
        "TD 時間差学習アルゴリズムは、動作主が取る一つ一つの行動を通して学習することを可能にする。TD 学習では、エピソード（ゴールや終了状態に到達する）ごとではなく、タイムステップ（行動）ごとにエージェントの知識を更新する。\n",
        "\n",
        "$$\n",
        "\\text{新しい状態評価} \\leftarrow \\text{古い状態評価} + \\text{ステップサイズ}\\left[\\text{目標} - \\text{古い状態評価}\\right]\n",
        "$$\n",
        "\n",
        "ターゲット誤差と呼ばれる値は上式では，「$\\text{目標} - \\text{古い状態評価}$」の部分である。「ステップサイズ」は 通常 α で表され、学習率とも呼ばれる。その値は 0 から 1 の間にある。\n",
        "<!-- The value Target-OldEstimate is called the target error. StepSize is usually denoted by α is also called the learning rate. Its value lies between 0 and 1.-->\n",
        "\n",
        "上式は、各時間ステップで更新を行うことで**目標**を達成するのに役立つ。ターゲットとは状態の効用である。効用が高い状態ほど、動作主が移行すべき優れた状態を意味する。本稿の簡潔さを考慮し、読者が[ベルマン方程式](https://en.wikipedia.org/wiki/Bellman_equation)を理解していることを前提とする。これによれば、状態の効用は割引報酬の期待値として以下のように定義される：\n",
        "<!--The equation above helps us achieve **Target** by making updates at every timestep. Target is the utility of a state. Higher utility means a better state for the agent to transition into. For the sake of brevity of this post, I have assumed the readers know about the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation). According to it, the utility of a state is the expected value of the discounted reward as follows:-->\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{目標}=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+k+1}\\right]\n",
        "$$\n",
        "\n",
        "平たく言えば、動作主を世界に自由に活動させる。動作主は、状態、報酬、遷移についての知識を持っていない。動作主は環境と相互作用し（ランダムな行動や情報に基づいた行動をとる）、すべての行動をとった後に既存の知識を継続的に更新することで、 新たな推定値（状態と行動の対の値）を学習する。\n",
        "<!-- In layman terms, we are letting an agent run free into a world. The agent has no knowledge of the state, the rewar\n",
        "ds and transitions. It interacts with the environment (make random or informed actions) and learns new estimates (value\n",
        "s of state-action pairs) by updating it’s existing knowledge continuously after taking every action.-->\n",
        "\n",
        "これまでの議論で、次のようないくつかの疑問が生じる。動作主はどのように環境と相互作用するのか？動作主はどのように行動を選択するのか、すなわち 特定の状態（方針）でどのような行動をとるのか？\n",
        "<!--The discussion till now shall give rise to several questions such as — What is an environment? How will the agent inter\n",
        "act with the environment? How will the agent choose actions i.e what action will the agent take in a particular state (\n",
        "policy)?-->\n",
        "\n",
        "ここで SARSA と Q-学習 の出番である。これらは、環境の中で動作主を誘導し、興味深いことを学ぶことを可能にする ２ つの 制御方針である。これらを説明する前に、環境とは何かを議論しなければならない。\n",
        "<!--This is where SARSA and Q-Learning come in. These are the two control policies that will guide our agent in an environm\n",
        "ent and enable it to learn interesting things. But before that, we shall discuss what is the environment.-->\n",
        "\n",
        "\n",
        "\n",
        "# 環境\n",
        "\n",
        "環境とは、動作主が離散的な状態を観察し、行動をとり、その行動をとることで報酬を観察することができるミニ世界と考えることができる。ビデオゲームは環境であり、自分自身がエージェントであると考えてください。\n",
        "ゲーム「ドゥーム」では、エージェントであるあなたは、状態（画面のフレーム）を観察し、アクション（前進、後退、ジャンプ、シュートなどのキーを押す）を行い、報酬を観察する。敵を殺せば喜び (効用) が得られ、前進している間はプラスの報酬が得られ、あまり報酬は得らないが、将来の報酬(敵を見つけて殺す) を得るために ゲームをしたいと思うだろう。このような環境を作るのは面倒で大変な作業である ([7人のチームが 1 年以上かけて Doom を開発](https://en.wikipedia.org/wiki/Development_of_Doom))。\n",
        "<!--An environment can be thought of as a mini-world where an agent can observe discrete states, take actions and observe rewards by taking those actions. Think of a video game as an environment and yourself as the agent. In the game Doom, you as an agent will observe the states (screen frames) and take actions (press keys like Forward, backward, jump, shoot etc) and observe rewards. Killing an enemy would yield you pleasure (utility) and a positive reward while moving ahead won’t yield you much reward but you would still want to do that to get future rewards (find and then kill the enemy). Creating such environments can be tedious and hard ([a team of 7 people worked for more than a year to develop Doom](https://en.wikipedia.org/wiki/Development_of_Doom)).-->\n",
        "\n",
        "OpenAI gym が登場したことは福音だ。gym は 様々な強化学習アルゴリズムをテストできる環境が組み込まれている Python ライブラリ である。結果を共有したり、分析したり、比較したりするための学術的な標準環境としての地位を確立している。Gym は [ドキュメント](https://gym.openai.com/docs/)が整備されていて、使いやすい。ドキュメントを読んで慣れておく必要がある。\n",
        "<!--OpenAI gym comes to the rescue! gym is a python library that has several in-built environments on which you can test various reinforcement learning algorithms. It has established itself as an academic standard to share, analyze and compare results. Gym is very well [documented](https://gym.openai.com/docs/) and super easy to use. You must read the documents and familiarize yourself with it before proceeding further.-->\n",
        "\n",
        "強化学習の応用のためには、自分で環境を作る必要がある。常に gym 互換の環境を参考にして書いて、誰もが使えるように公開しておくことを推奨する。Gym の ソースコードを読めば、公開ができるようになる。面倒だけど楽しい！と思っている人にはおすすめである。\n",
        "<!--For novel applications of reinforcement learning, you will have to create your own environments. It’s advised to always refer and write gym compatible environments and release them publicly so that everyone can use them. Reading the gym’s source code will help you do that. It is tedious but fun!-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2voyWwZ5mIkO"
      },
      "source": [
        "# 1. SARSA\n",
        "\n",
        "> SARSA とは，State-Action-Reward-State-Action の省略形\n",
        "\n",
        "<!-- > SARSA is acronym for State-Action-Reward-State-Action-->\n",
        "\n",
        "SARSA とははオンポリシーな 時間差制御方式 である。方針(ポリシー) とは 状態 と 動作(行動) との対のことである。python では 状態 を キー、動作(行動) を 値 とする 辞書 dict と考えることができる。方針（ポリシー）は各状態で取るべき 動作（行動）をマッピングする。オンポリシー制御では、学習中にある 方針 (大抵は方針反復 のように自分自身で評価しているもの) に従うことで、 状態ごとに 動作 (行動) を選択する。\n",
        "我々の目的は、現在の 方策 $\\pi$ と全ての 状態行動 $(s-a)$ の対について、 $Q \\pi(s,a)$ を推定することである。これは、ある 状態-動作(行動) の対 から 別の 状態-動作(行動) の対 に 動作主を遷移させることで、タイムステップ ごとに 適用される TD 更新規則 を用いて行う (状態 から 別の 状態に 動作主を遷移させる モデル依存型 強化学習技法とは異なる)。\n",
        "<!-- SARSA is an on-policy TD control method. A policy is a state-action pair tuple. In python, you can think of it as a dictionary with keys as the state and values as the action. Policy maps the action to be taken at each state. An on-policy control method chooses the action for each state during learning by following a certain policy (mostly the one it is evaluating itself, like in policy iteration). Our aim is to estimate $QQ\\pi(s,a)$ for the current policy $\\pi$ and all state-action $(s-a)$ pairs. We do this using TD update rule applied at every timestep by letting the agent transition from one state-action pair to another state-action pair (unlike model dependent RL techniques where the agent transitions from a state to another state).-->\n",
        "\n",
        "**Q-値** 状態の効用値については Q-値 も同じである。Q-値 は、状態と行動の対とその効用を表す実数とのマッピングです。Q-学習 と SARSA とは、すべての 行動-状態 の対に対して 最適な Q-値 を評価する 方針制御手法である。\n",
        "<!-- **Q-value** You must be already familiar with the utility value of a state, Q-value is the same with the only difference of being defined over the state-action pair rather than just the state. It’s a mapping between state-action pair and a real number denoting its utility. Q-learning and SARSA are both policy control methods which work on evaluatingthe optimal Q-value for all action-state pairs.-->\n",
        "\n",
        "\n",
        "SARSA の更新則は:<!-- The update rule for SARSA is:-->\n",
        "$$\n",
        "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha\\left[ R_{t+1}+\\gamma Q(S_{t+1},A_{t+1}-Q(S_t,A_t)\\right]\n",
        "$$\n",
        "![出典: Introduction to Reinforcement learning by Sutton and Barto — 6.7](https://cdn-images-1.medium.com/max/1280/1*3Ul422CbPyIDJI0XJVun3Q.png)\n",
        "\n",
        "ある状態 $S$ が終了した場合 (ゴールに達したり，終了状態に陥った場合)，$Q(S,a)=0\\forall a\\in A$ となる。ここで，$A$ は全行動レパートリーを表す。\n",
        "<!-- If a state $S$ is terminal (goal state or end state) then, $Q(S,a)=0\\forall a\\in A$ where A is the set of all possible actions-->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*fYuXsaJoyCWuIZu49vx_GA.png\" width=\"49%\"><br/>\n",
        "Source: Introduction to Reinforcement learning by Sutton and Barto —Chapter 6\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW5eDNeDSBS9"
      },
      "source": [
        "<center>\n",
        "<div style=\"color:white;background-color:gray;width:49%;\">\n",
        "SARSA (オンポリシー TD 制御) $Q\\approx q_{\\star}$ を推定</div>\n",
        "<div style=\"background-color:whitesmoke;width:49%;text-align:left\">\n",
        "\n",
        "- $Q(s,a)$ を全状態 $s$ と 全動作 $a$ について初期化，$Q(\\text{終了状態},\\cdot)=0$ とする\n",
        "- 各エピソードを繰り返す:\n",
        "    - $S$ を初期化する\n",
        "    - $S$ の中から Q 関数に従って $A$ を選ぶ\n",
        "    - $Q(S,A)\\leftarrow Q(S,A)+\\alpha\\left[R+\\gamma Q(S',S')-Q(S,A)\\right]$\n",
        "    - $S\\leftarrow S'$, $A\\leftarrow A'$\n",
        "- $S$ が収束するまで繰り返す\n",
        "</div></center>\n",
        "\n",
        "## イプシロン ($\\epsilon$) 貪欲(欲張り) 方針<!-- Epsilon-greedy policy is this: -->\n",
        "\n",
        "イプシロン貪欲な方策とは以下のことを言う:\n",
        "\n",
        "1. 0 から 1 の範囲 ($r\\in[0,1]$) の乱数 $r$ を一つ発生させる\n",
        "2. もし $r>\\epsilon$ であれあば，ランダムにある行動を選択する\n",
        "3. そうでなければ (すなわち $r\\le\\epsilon$) $Q$ 値 (最大効用をあたえる) 行動を選択する\n",
        "\n",
        "<!--\n",
        "1. Generate a random number $r\\in[0,1]$\n",
        "2. If $r>\\epsilon$ choose a random action\n",
        "3. Else choose an action derived from the $Q$ values (which yields the maximum utility)-->\n",
        "\n",
        "<!-- It shall become more clear after reading the python code.-->\n",
        "\n",
        "以下に Python コードを示す:\n",
        "<!--It shall become more clear after reading the python code.-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qmVJT8qfmZR"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(\n",
        "    Q,         # 状態 X 行動 から 価値 を与える\n",
        "    epsilon,   # 新規行動の探索率 イプシロン貪欲戦略のために使用\n",
        "    n_actions, # 可能な行動の選択肢数\n",
        "    s,         # 状態数\n",
        "    train=False, # `True` Q 値を最大にする行動が決定論的に選択される\n",
        "):\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HvEc_6VgDVu"
      },
      "source": [
        "$\\epsilon$ の値は **探索と活用のジレンマ** を決定する。英語では exploration-exploitatoin dilemma である。綴が似ているために英語で覚えた方が良い。\n",
        "\n",
        "<!--source: https://gist.githubusercontent.com/TimeTraveller-San/40a7a2655743bf6230706d45d1201b49/raw/25ddaaf54e946d33576e27e6bab45a40f22864a9/epsilon_greedy.py -->\n",
        "\n",
        "- $\\epsilon$ が大きければ、乱数 $r$ が $\\epsilon$ より大きくなることはほとんどなく、 ランダムな行動はほとんど起こらない (探索が少なく、知識利用 が多くなる)。\n",
        "- $\\epsilon$ が小さければ、乱数 $r$ は $\\epsilon$ よりも大きくなることが多いので、 エージェント は より 多くのランダム な 行動を選択することになる。\n",
        "\n",
        "このような確率的な性質を利用して、エージェントは環境をより探索することができるようになる。\n",
        "\n",
        "経験則として、$\\epsilon$ は通常 $0.9$ が選ばれる。だが $\\epsilon$ は 環境タイプに応じて変化させることができる。いくつかのケースでは、より高い探索に続いてより高い知識利用 を可能にするために、時間の経過とともに $\\epsilon$ は 緩和 漸減 される。\n",
        "\n",
        "以下 [OpenAI Gym Taxi-v3 環境](https://gym.openai.com/envs/Taxi-v3/) に適用された SARSA のシンプルな Python コードを示す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-2j4prnnW3I"
      },
      "source": [
        "## Taxi-v3 環境の説明\n",
        "\n",
        "* Taxi-v3 課題は [Dietterich(2000)](https://arxiv.org/pdf/cs/9905014.pdf) で階層型強化学習の問題点を説明するために導入された。\n",
        "* 4 つの場所（異なる文字でラベル付けされている）があり， エージェントの仕事は，ある場所で乗客を拾い，別の場所で降ろすことである。\n",
        "* 送迎が成功すると +20点，時間がかかるごとに 1 点ずつ減ります。\n",
        "* また，違法な乗降行為には 10 ポイントの罰則がある。\n",
        "\n",
        "<!--  This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning.\n",
        "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another.\n",
        "You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWWeQemYSBS-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "s = env.reset()\n",
        "print(f'type(s):{type(s)}, len(s):{len(s)}')\n",
        "print(f's:{s}')\n",
        "\n",
        "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "print(n_states, n_actions)\n",
        "print(dir(env))\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfjOWLv1Sq60"
      },
      "outputs": [],
      "source": [
        "# import gymnasium as gym\n",
        "\n",
        "# # Create our training environment - a cart with a pole that needs balancing\n",
        "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "\n",
        "# # Reset environment to start a new episode\n",
        "# observation, info = env.reset()\n",
        "# # observation: what the agent can \"see\" - cart position, velocity, pole angle, etc.\n",
        "# # info: extra debugging information (usually not needed for basic learning)\n",
        "\n",
        "# print(f\"Starting observation: {observation}\")\n",
        "# # Example output: [ 0.01234567 -0.00987654  0.02345678  0.01456789]\n",
        "# # [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
        "\n",
        "# episode_over = False\n",
        "# total_reward = 0\n",
        "\n",
        "# while not episode_over:\n",
        "#     # Choose an action: 0 = push cart left, 1 = push cart right\n",
        "#     action = env.action_space.sample()  # Random action for now - real agents will be smarter!\n",
        "\n",
        "#     # Take the action and see what happens\n",
        "#     observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "#     # reward: +1 for each step the pole stays upright\n",
        "#     # terminated: True if pole falls too far (agent failed)\n",
        "#     # truncated: True if we hit the time limit (500 steps)\n",
        "\n",
        "#     total_reward += reward\n",
        "#     episode_over = terminated or truncated\n",
        "\n",
        "# print(f\"Episode finished! Total reward: {total_reward}\")\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69RBwjiZSBS-"
      },
      "outputs": [],
      "source": [
        "# import gymnasium as gym # Ensure gymnasium is used\n",
        "# import numpy as np # Ensure numpy is imported for general use\n",
        "\n",
        "# env = gym.make('Taxi-v3') # Now this gym refers to gymnasium\n",
        "# n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "# Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "\n",
        "# s, info_reset = env.reset() # Handle gymnasium's reset return value (state, info)\n",
        "# a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "\n",
        "# s_, reward, terminated, truncated, info_step = env.step(a) # Handle gymnasium's step return values\n",
        "# done = terminated or truncated # Define done for compatibility with older logic if needed\n",
        "# a_ = epsilon_greedy(Q, epsilon, n_actions, s_)\n",
        "# print(f's:{s},a:{a}, s_:{s_}, a_:{a_}, done:{done}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVfYVuu3gCGK"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym # Changed from import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def init_q(s,           # 状態の数\n",
        "           a,           # 行動の数\n",
        "           type=\"ones\"  # `ones` であれば状態遷移表を 1 で初期化，`random` であれば乱数で初期化\n",
        "                        # `zeros` であれば 0 で初期化\n",
        "          ):\n",
        "    \"\"\" Q 表の初期化\"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q,           # 状態 X 行動 から 価値 を与える\n",
        "                   epsilon,     # 新規行動の探索率 イプシロン貪欲戦略のために使用\n",
        "                   n_actions,   # 可能な行動の選択肢数\n",
        "                   s,           # 状態\n",
        "                   train=False, # `True` Q 値を最大にする行動が決定論的に選択される\n",
        "                  ):\n",
        "    \"\"\" イプシロン貪欲に行動を探索する\"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "\n",
        "def sarsa(alpha,         # 学習率\n",
        "          gamma,         # 割引率\n",
        "          epsilon,       # イプシロン貪欲な探索率\n",
        "          episodes,      # 最大エピソード数\n",
        "          max_steps,     # 最大ステップ数\n",
        "          n_tests,       # エピソード数\n",
        "          render=False,\n",
        "          test=False):\n",
        "    \"\"\"\n",
        "    alpha: 学習率\n",
        "    gamma: 割引率\n",
        "    epsilon: イプシロン貪欲な探索率\n",
        "    max_steps: 各エピソードの最大ステップ数\n",
        "    n_tests: number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"エピソード: {episode}\")\n",
        "        total_reward = 0\n",
        "        s, _ = env.reset() # Modified to unpack (observation, info)\n",
        "        # Removed: s = s[0] as it's now handled by unpacking\n",
        "        #a = epsilon_greedy(Q, epsilon, n_actions, s[0])\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium step API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            a_ = epsilon_greedy(Q, epsilon, n_actions, s_)\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"このエピソードでのステップ数 {t}，総報酬:{total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Q 値:\\n{Q}\\nテスト開始:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=0.1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s, _ = env.reset() # Modified to unpack (observation, info)\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            # env.render() # Removed rendering in test_agent as it might cause issues in Colab or if not explicitly configured.\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"状態 {s} で行動 {a} を選択\")\n",
        "            s, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium step API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                print(f\"エピソード総報酬: {total_reward}\")\n",
        "                time.sleep(1)\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYobcf66rh0x",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "alpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 3000\n",
        "max_steps = 2500\n",
        "n_tests = 20\n",
        "timestep_reward = sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "#timestep_reward = qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "print(timestep_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy3pRTdLiNHy"
      },
      "source": [
        "# 3. Q 学習\n",
        "\n",
        "Q-学習 は、方針によらない TD 制御である。SARSA とほぼ同じであり，唯一の違いは、次の 動作(行動) $A$ を見つけるための 方針に従うのではなく、貪欲に 動作(行動)  を選択することである。\n",
        "SARSA と同様に Q 値 を評価することを目的としており、更新則 は次のようになる:\n",
        "\n",
        "$$\n",
        "Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma\\max_{\\alpha} Q(S_{t+1},a)-Q(S_t,A_t)\\right]\n",
        "$$\n",
        "\n",
        "SARSA では、ある方策 に従って 行動 $A'$ を選択していた。これに対し、上式 Q 学習 では 行動 $A'$ ($a$) は、 $Q$ の最大値を取るだけの 欲張り(貪欲, グリーディ)な 方法で選択される。\n",
        "\n",
        "Q 学習のアルゴリズムを以下に示す:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*Rf_H0YXhSPPm-iyBY2Gnjg.png\" width=\"49%\"><br/>\n",
        "\n",
        "Source: [Introduction to Reinforcement learning by Sutton and Barto — Chapter 6](https://cdn-images-1.medium.com/max/1280/1*Rf_H0YXhSPPm-iyBY2Gnjg.png)\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JhWNzdgSBS-"
      },
      "source": [
        "<center>\n",
        "<!-- <div style=\"background-color:lightgray;width:49%;text-align:left\"> -->\n",
        "<div style=\"color:white;background-color:gray;width:49%;align:center\">\n",
        "\n",
        "Q 学習 (オフポリシー TD 制御) for $\\pi\\approx\\pi_{\\star}$\n",
        "</div>\n",
        "<div style=\"background-color:whitesmoke;width:49%;text-align:left\">\n",
        "\n",
        "- $Q(s,a)$ を全状態 $s$ と 全動作 $a$ について初期化，$Q(\\text{終了状態},\\cdot)=0$ とする\n",
        "- 各エピソードを繰り返す:\n",
        "    - $S$ を初期化する\n",
        "    - 各エピソードを繰り返し\n",
        "        - $S$ の中から Q 関数に従って $A$ を選ぶ\n",
        "        - 行動 $A$ を起こし，$R$, $S'$ を観察する\n",
        "        - $Q(S,A)\\leftarrow Q(S,A)+\\alpha\\left[R+\\gamma \\max_{a} Q(S',a)-Q(S,A)\\right]$\n",
        "        - $S\\leftarrow S'$\n",
        "</div></center>\n",
        "\n",
        "Python コードは以下:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-Vu_7d_iR7r"
      },
      "outputs": [],
      "source": [
        "# import gym\n",
        "import numpy as np\n",
        "import time\n",
        "import gymnasium as gym # Added this import\n",
        "\n",
        "\"\"\"\n",
        "Q 学習はオフポリシー学習の python 実装。\n",
        "Sutton and Barto の著書 にある Q 学習アルゴリズムの python 実装\n",
        "SARSA と Q 学習の唯一の違いは、SARSA は現在の方針に基づいて次の行動を取るのに対し，Q 学習は次の状態で最大の効用を持つ行動をとることである。\n",
        "\"\"\"\n",
        "\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\"\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        s, _ = env.reset() # Modified for gymnasium API\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            a_ = np.argmax(Q[s_, :])\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s, _ = env.reset() # Modified for gymnasium API\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            # env.render() # Rendering can cause issues in Colab, keep commented for now.\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"Reached goal!\")\n",
        "                else:\n",
        "                    print(\"Shit! dead x_x\")\n",
        "                time.sleep(3)\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROXygIyTsDnw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "alpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 10000\n",
        "max_steps = 2500\n",
        "n_tests = 2\n",
        "timestep_reward = qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "print(timestep_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6-TQZN2iwTq"
      },
      "source": [
        "source: https://gist.github.com/TimeTraveller-San/9e56f9d09be7d50b795ef2f83be2ba72#file-qlearning-py\n",
        "\n",
        "# 3. 期待 SARSA\n",
        "\n",
        "期待 SARSA とは、その名が示すように、現在の状態で起こりうるすべての行動についての Q値 の期待値(平均値)を取る。ターゲットの更新則を使うと、より明確になる。\n",
        "<!-- Expected SARSA, as the name suggest takes the expectation (mean) of Q values for every possible action in the current state. The target update rule shall make things more clear:-->\n",
        "\n",
        "$$\n",
        "Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha\\left[R_{t+1}+\\gamma\\mathbb{E}\\left[Q(S_{t+1},A_{t+1}\\vert S_{t+1}\\right]-Q(S_t\n",
        ",A_t)\\right]\\\\ \\\\\n",
        "\\hspace{6em}\\leftarrow Q(S_t,A_t)+\\alpha\\left[\n",
        "R_{t+1}+\\gamma\\sum_{\\alpha}\\pi(a\\vert S_{t+1}) Q(S_{t+1},a) - Q(S_t,A_t)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Python コードを以下に示す:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPDLyhHQirtC"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym # Changed from import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\"\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "def expected_sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        total_reward = 0\n",
        "        s, _ = env.reset() # Modified for gymnasium API\n",
        "        t = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "            s_, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium step API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                expected_value = np.mean(Q[s_,:])\n",
        "                # print(Q[s,:], sum(Q[s,:]), len(Q[s,:]), expected_value)\n",
        "                Q[s, a] += alpha * (reward + (gamma * expected_value) - Q[s, a])\n",
        "            s = s_\n",
        "            if done:\n",
        "                if True:\n",
        "                    print(f\"This episode took {t} timesteps and reward {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=0.1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s, _ = env.reset() # Modified for gymnasium API\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            # env.render() # Rendering can cause issues in Colab, keep commented for now.\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium step API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                print(f\"Episode reward: {total_reward}\")\n",
        "                time.sleep(1)\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_W61s7usjdg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.9\n",
        "episodes = 1000\n",
        "max_steps = 2500\n",
        "n_tests = 20\n",
        "timestep_reward = expected_sarsa(\n",
        "    alpha, gamma, epsilon,\n",
        "    episodes, max_steps, n_tests,\n",
        "    render=False, test=True)\n",
        "print(timestep_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXWhV9fnjcMM"
      },
      "source": [
        "source: https://gist.github.com/TimeTraveller-San/5bd93710e118633e0793dc5d0b92b19a#file-expectedsarsa-py\n",
        "\n",
        "# 比較\n",
        "\n",
        "以下のパラメータで 3 つのアルゴリズムを比較した\n",
        "\n",
        "```python\n",
        "lpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 2000\n",
        "max_steps = 2500 # (max number of time steps possible in a single episode)\n",
        "```\n",
        "\n",
        "ここでは、上記の 3 つの方策制御方法の比較 をプロットする。\n",
        "\n",
        "## 収束\n",
        "\n",
        "以下のプロットが示すように、Q 学習 (緑) は SARSA(オレンジ) と 期待 SARSA(青) の両者より先に収束した。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*0LpFOud2FUKciZ9jPd1ZBA.png\" width=\"49%\"><br/>\n",
        "SARSA, Q-learning & Expected SARSA — Convergence comparison\n",
        "</center>\n",
        "\n",
        "## 成績\n",
        "\n",
        "実装した 3 つのアルゴリズムでは、Q-学習 が最も良く、期待 SARSA が最も悪い性能のようである。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*IC0L25IzedYZ_TujMHpRKg.png\"><br/>\n",
        "SARSA, Q-learning & Expected SARSA — performance comparison!\n",
        "</center>\n",
        "\n",
        "# 結語\n",
        "\n",
        "TD 学習 (時間差学習)は 最も重要な強化学習の概念である。DQN や 二重 DQN のような、さらに派生したものは、AI の分野で有名な画期的な結果を達成している。\n",
        "Google の アルファ碁は、囲碁の世界チャンピオンを倒すために、CNN と DQN アルゴリズムを使用した。\n",
        "<!-- Temporal Difference learning is the most important reinforcement learning concept. It’s further derivatives like DQN and double DQN (I may discuss them later in another post) have achieved groundbreaking results renowned in the field of AI. Google’s alpha go used DQN algorithm along with CNNs to defeat the go world champion. You are now equipped with the theoretical and practical knowledge of basic TD, go out and explore! -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ujQHWRcgFqz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}