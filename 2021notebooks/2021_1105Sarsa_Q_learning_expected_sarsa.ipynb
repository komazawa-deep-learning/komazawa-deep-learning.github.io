{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105Sarsa_Q_learning_expected_sarsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4xBE-UFfaP8"
      },
      "source": [
        "# TD (時間差)学習, SARSA, 期待 SARSA, Q 学習 と Python 実装\n",
        "\n",
        "<!-- date: 2020-0519-->\n",
        "- title: Reinforcement learning: Temporal-Difference, SARSA, Q-Learning & Expected SARSA in python\n",
        "- author: Vaibhav Kumar\n",
        "- Date: May 9, 2019\n",
        "- Original: <https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e>\n",
        "\n",
        "<!--\n",
        "# TD, SARSA, Q-Learning and Expected SARSA along with their python implementation and comparison\n",
        "-->\n",
        "\n",
        "<!--\n",
        "> If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-diff\n",
        "erence (TD) learning. — Andrew Barto and Richard S. Sutton\n",
        "-->\n",
        "\n",
        "> 強化学習の中心的で斬新なアイデアを一つ挙げるとすれば、それは間違いなく時間差（TD）学習であろう。<br/>\n",
        ">    -- アンドリュー・バルト， リチャード・S・サットン\n",
        "\n",
        "<!-- # Pre-requisites\n",
        "- Basics of Reinforcement learning\n",
        "- Markov chains, Markov Decision Process (MDPs)\n",
        "- Bellman equation\n",
        "- Value, policy functions and iterations\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06IN-g2JSBS5"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "\n",
        "# if isColab:\n",
        "#     !sudo apt-get update\n",
        "#     !sudo apt-get dist-upgrade\n",
        "#     !sudo apt install xorg-dev libx11-dev libgl1-mesa-glx\n",
        "#     !pip install gym[toy_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1vuW4H0SkXx"
      },
      "outputs": [],
      "source": [
        "!pip install \"gymnasium[classic-control]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfeZ4ohUoPO8"
      },
      "source": [
        "## 0.1. 前提知識\n",
        "\n",
        "- 強化学習の基本\n",
        "- マルコフ連鎖, マルコフ決定プロセス(MDP)\n",
        "- ベルマン方程式\n",
        "- 価値，方針関数，反復\n",
        "\n",
        "<!-- # Model-dependent and model-free reinforcement learning\n",
        "Model-dependent RL algorithms (namely value and policy iterations) work with the help of a transition table. A transiti\n",
        "on table can be thought of as a life hack book which has all the knowledge the agent needs to be successful in the worl\n",
        "d it exists in. Naturally, writing such a book is very tedious and impossible in most cases which is why model dependen\n",
        "t learning algorithms have little practical use.\n",
        "\n",
        "Temporal Difference is a model-free reinforcement learning algorithm. This means that the agent learns through actual e\n",
        "xperience rather than through a readily available all-knowing-hack-book (transition table). This enables us to introduc\n",
        "e stochastic elements and large sequences of state-action pairs. The agent has no idea about the reward and transition\n",
        "systems. It does not know what will happen on taking an arbitrary action at an arbitrary state. The agent has to intera\n",
        "ct with “world” or “environment” and find out for itself.-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeURXFYXoRoZ"
      },
      "source": [
        "## 0.2 強化学習におけるモデル依存学習とモデル自由学習\n",
        "\n",
        "モデル依存 強化学習 (RL) アルゴリズム（すなわち，価値と方策反復）は，遷移表に基づいて動作する。遷移表は，動作主 (エージェント) が存在する世界で成功するために必要なすべての知識が書かれたライフハック本と考えることができる。当然ながら，そのような本を書くのは非常に面倒で，ほとんどの場合不可能である。\n",
        "\n",
        "TD (Temporal Difference) はモデルフリー型の強化学習アルゴリズムである。TD 学習，動作主 (エージェント) が即時に入手可能な全知全能のハックブック (遷移表) ではなく，実際の経験を通して学習することを意味する。これにより，確率的な要素と状態と行動との対の大規模な系列を導入することが可能となる。\n",
        "\n",
        "* 動作主 (エージェント) は，報酬システムと遷移システムについては何も知らない。\n",
        "* 動作主 (エージェント) は，任意の状態で任意の行動をとった場合に何が起こるかを知らない。\n",
        "* 動作主 (エージェント) は，「世界」や「環境」と相互作用し、自分自身で見つけ出さなければならない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSE2US7Ml-9A"
      },
      "source": [
        "\n",
        "## 0.3 TD 学習 (時差学習)\n",
        "\n",
        "TD 時間差学習アルゴリズムは、動作主が取る一つ一つの行動を通して学習することを可能にする。TD 学習では、エピソード（ゴールや終了状態に到達する）ごとではなく、タイムステップ（行動）ごとにエージェントの知識を更新する。\n",
        "\n",
        "$$\n",
        "\\text{新しい状態評価} \\leftarrow \\text{古い状態評価} + \\text{ステップサイズ}\\left[\\text{目標} - \\text{古い状態評価}\\right]\n",
        "$$\n",
        "\n",
        "ターゲット誤差と呼ばれる値は上式では，「$\\text{目標} - \\text{古い状態評価}$」の部分である。「ステップサイズ」は 通常 α で表され、学習率とも呼ばれる。その値は 0 から 1 の間にある。\n",
        "<!-- The value Target-OldEstimate is called the target error. StepSize is usually denoted by α is also called the learning rate. Its value lies between 0 and 1.-->\n",
        "\n",
        "上式は、各時間ステップで更新を行うことで**目標**を達成するのに役立つ。ターゲットとは状態の効用である。効用が高い状態ほど、動作主が移行すべき優れた状態を意味する。本稿の簡潔さを考慮し、読者が[ベルマン方程式](https://en.wikipedia.org/wiki/Bellman_equation)を理解していることを前提とする。これによれば、状態の効用は割引報酬の期待値として以下のように定義される：\n",
        "<!--The equation above helps us achieve **Target** by making updates at every timestep. Target is the utility of a state. Higher utility means a better state for the agent to transition into. For the sake of brevity of this post, I have assumed the readers know about the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation). According to it, the utility of a state is the expected value of the discounted reward as follows:-->\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{目標}=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+k+1}\\right]\n",
        "$$\n",
        "\n",
        "平たく言えば、動作主を世界に自由に活動させる。動作主は、状態、報酬、遷移についての知識を持っていない。動作主は環境と相互作用し（ランダムな行動や情報に基づいた行動をとる）、すべての行動をとった後に既存の知識を継続的に更新することで、 新たな推定値（状態と行動の対の値）を学習する。\n",
        "<!-- In layman terms, we are letting an agent run free into a world. The agent has no knowledge of the state, the rewar\n",
        "ds and transitions. It interacts with the environment (make random or informed actions) and learns new estimates (value\n",
        "s of state-action pairs) by updating it’s existing knowledge continuously after taking every action.-->\n",
        "\n",
        "これまでの議論で、次のようないくつかの疑問が生じる。動作主はどのように環境と相互作用するのか？動作主はどのように行動を選択するのか、すなわち 特定の状態（方針）でどのような行動をとるのか？\n",
        "<!--The discussion till now shall give rise to several questions such as — What is an environment? How will the agent inter\n",
        "act with the environment? How will the agent choose actions i.e what action will the agent take in a particular state (\n",
        "policy)?-->\n",
        "\n",
        "ここで SARSA と Q-学習 の出番である。これらは、環境の中で動作主を誘導し、興味深いことを学ぶことを可能にする ２ つの 制御方針である。これらを説明する前に、環境とは何かを議論しなければならない。\n",
        "<!--This is where SARSA and Q-Learning come in. These are the two control policies that will guide our agent in an environm\n",
        "ent and enable it to learn interesting things. But before that, we shall discuss what is the environment.-->\n",
        "\n",
        "\n",
        "\n",
        "# 環境\n",
        "\n",
        "環境とは、動作主が離散的な状態を観察し、行動をとり、その行動をとることで報酬を観察することができるミニ世界と考えることができる。ビデオゲームは環境であり、自分自身がエージェントであると考えてください。\n",
        "ゲーム「ドゥーム」では、エージェントであるあなたは、状態（画面のフレーム）を観察し、アクション（前進、後退、ジャンプ、シュートなどのキーを押す）を行い、報酬を観察する。敵を殺せば喜び (効用) が得られ、前進している間はプラスの報酬が得られ、あまり報酬は得らないが、将来の報酬(敵を見つけて殺す) を得るために ゲームをしたいと思うだろう。このような環境を作るのは面倒で大変な作業である ([7人のチームが 1 年以上かけて Doom を開発](https://en.wikipedia.org/wiki/Development_of_Doom))。\n",
        "<!--An environment can be thought of as a mini-world where an agent can observe discrete states, take actions and observe rewards by taking those actions. Think of a video game as an environment and yourself as the agent. In the game Doom, you as an agent will observe the states (screen frames) and take actions (press keys like Forward, backward, jump, shoot etc) and observe rewards. Killing an enemy would yield you pleasure (utility) and a positive reward while moving ahead won’t yield you much reward but you would still want to do that to get future rewards (find and then kill the enemy). Creating such environments can be tedious and hard ([a team of 7 people worked for more than a year to develop Doom](https://en.wikipedia.org/wiki/Development_of_Doom)).-->\n",
        "\n",
        "OpenAI gym が登場したことは福音だ。gym は 様々な強化学習アルゴリズムをテストできる環境が組み込まれている Python ライブラリ である。結果を共有したり、分析したり、比較したりするための学術的な標準環境としての地位を確立している。Gym は [ドキュメント](https://gym.openai.com/docs/)が整備されていて、使いやすい。ドキュメントを読んで慣れておく必要がある。\n",
        "<!--OpenAI gym comes to the rescue! gym is a python library that has several in-built environments on which you can test various reinforcement learning algorithms. It has established itself as an academic standard to share, analyze and compare results. Gym is very well [documented](https://gym.openai.com/docs/) and super easy to use. You must read the documents and familiarize yourself with it before proceeding further.-->\n",
        "\n",
        "強化学習の応用のためには、自分で環境を作る必要がある。常に gym 互換の環境を参考にして書いて、誰もが使えるように公開しておくことを推奨する。Gym の ソースコードを読めば、公開ができるようになる。面倒だけど楽しい！と思っている人にはおすすめである。\n",
        "<!--For novel applications of reinforcement learning, you will have to create your own environments. It’s advised to always refer and write gym compatible environments and release them publicly so that everyone can use them. Reading the gym’s source code will help you do that. It is tedious but fun!-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2voyWwZ5mIkO"
      },
      "source": [
        "# 1. SARSA\n",
        "\n",
        "> SARSA とは，State-Action-Reward-State-Action の省略形\n",
        "\n",
        "<!-- > SARSA is acronym for State-Action-Reward-State-Action-->\n",
        "\n",
        "SARSA とははオンポリシーな 時間差制御方式 である。方針(ポリシー) とは 状態 と 動作(行動) との対のことである。python では 状態 を キー、動作(行動) を 値 とする 辞書 dict と考えることができる。方針（ポリシー）は各状態で取るべき 動作（行動）をマッピングする。オンポリシー制御では、学習中にある 方針 (大抵は方針反復 のように自分自身で評価しているもの) に従うことで、 状態ごとに 動作 (行動) を選択する。\n",
        "我々の目的は、現在の 方策 $\\pi$ と全ての 状態行動 $(s-a)$ の対について、 $Q \\pi(s,a)$ を推定することである。これは、ある 状態-動作(行動) の対 から 別の 状態-動作(行動) の対 に 動作主を遷移させることで、タイムステップ ごとに 適用される TD 更新規則 を用いて行う (状態 から 別の 状態に 動作主を遷移させる モデル依存型 強化学習技法とは異なる)。\n",
        "<!-- SARSA is an on-policy TD control method. A policy is a state-action pair tuple. In python, you can think of it as a dictionary with keys as the state and values as the action. Policy maps the action to be taken at each state. An on-policy control method chooses the action for each state during learning by following a certain policy (mostly the one it is evaluating itself, like in policy iteration). Our aim is to estimate $QQ\\pi(s,a)$ for the current policy $\\pi$ and all state-action $(s-a)$ pairs. We do this using TD update rule applied at every timestep by letting the agent transition from one state-action pair to another state-action pair (unlike model dependent RL techniques where the agent transitions from a state to another state).-->\n",
        "\n",
        "**Q-値** 状態の効用値については Q-値 も同じである。Q-値 は、状態と行動の対とその効用を表す実数とのマッピングです。Q-学習 と SARSA とは、すべての 行動-状態 の対に対して 最適な Q-値 を評価する 方針制御手法である。\n",
        "<!-- **Q-value** You must be already familiar with the utility value of a state, Q-value is the same with the only difference of being defined over the state-action pair rather than just the state. It’s a mapping between state-action pair and a real number denoting its utility. Q-learning and SARSA are both policy control methods which work on evaluatingthe optimal Q-value for all action-state pairs.-->\n",
        "\n",
        "\n",
        "SARSA の更新則は:<!-- The update rule for SARSA is:-->\n",
        "$$\n",
        "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha\\left[ R_{t+1}+\\gamma Q(S_{t+1},A_{t+1}-Q(S_t,A_t)\\right]\n",
        "$$\n",
        "![出典: Introduction to Reinforcement learning by Sutton and Barto — 6.7](https://cdn-images-1.medium.com/max/1280/1*3Ul422CbPyIDJI0XJVun3Q.png)\n",
        "\n",
        "ある状態 $S$ が終了した場合 (ゴールに達したり，終了状態に陥った場合)，$Q(S,a)=0\\forall a\\in A$ となる。ここで，$A$ は全行動レパートリーを表す。\n",
        "<!-- If a state $S$ is terminal (goal state or end state) then, $Q(S,a)=0\\forall a\\in A$ where A is the set of all possible actions-->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*fYuXsaJoyCWuIZu49vx_GA.png\" width=\"49%\"><br/>\n",
        "Source: Introduction to Reinforcement learning by Sutton and Barto —Chapter 6\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW5eDNeDSBS9"
      },
      "source": [
        "<center>\n",
        "<div style=\"color:white;background-color:gray;width:49%;\">\n",
        "SARSA (オンポリシー TD 制御) $Q\\approx q_{\\star}$ を推定</div>\n",
        "<div style=\"background-color:whitesmoke;width:49%;text-align:left\">\n",
        "\n",
        "- $Q(s,a)$ を全状態 $s$ と 全動作 $a$ について初期化，$Q(\\text{終了状態},\\cdot)=0$ とする\n",
        "- 各エピソードを繰り返す:\n",
        "    - $S$ を初期化する\n",
        "    - $S$ の中から Q 関数に従って $A$ を選ぶ\n",
        "    - $Q(S,A)\\leftarrow Q(S,A)+\\alpha\\left[R+\\gamma Q(S',S')-Q(S,A)\\right]$\n",
        "    - $S\\leftarrow S'$, $A\\leftarrow A'$\n",
        "- $S$ が収束するまで繰り返す\n",
        "</div></center>\n",
        "\n",
        "## イプシロン ($\\epsilon$) 貪欲(欲張り) 方針<!-- Epsilon-greedy policy is this: -->\n",
        "\n",
        "イプシロン貪欲な方策とは以下のことを言う:\n",
        "\n",
        "1. 0 から 1 の範囲 ($r\\in[0,1]$) の乱数 $r$ を一つ発生させる\n",
        "2. もし $r>\\epsilon$ であれあば，ランダムにある行動を選択する\n",
        "3. そうでなければ (すなわち $r\\le\\epsilon$) $Q$ 値 (最大効用をあたえる) 行動を選択する\n",
        "\n",
        "<!--\n",
        "1. Generate a random number $r\\in[0,1]$\n",
        "2. If $r>\\epsilon$ choose a random action\n",
        "3. Else choose an action derived from the $Q$ values (which yields the maximum utility)-->\n",
        "\n",
        "<!-- It shall become more clear after reading the python code.-->\n",
        "\n",
        "以下に Python コードを示す:\n",
        "<!--It shall become more clear after reading the python code.-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qmVJT8qfmZR"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(\n",
        "    Q,         # 状態 X 行動 から 価値 を与える\n",
        "    epsilon,   # 新規行動の探索率 イプシロン貪欲戦略のために使用\n",
        "    n_actions, # 可能な行動の選択肢数\n",
        "    s,         # 状態数\n",
        "    train=False, # `True` Q 値を最大にする行動が決定論的に選択される\n",
        "):\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HvEc_6VgDVu"
      },
      "source": [
        "$\\epsilon$ の値は **探索と活用のジレンマ** を決定する。英語では exploration-exploitatoin dilemma である。綴が似ているために英語で覚えた方が良い。\n",
        "\n",
        "<!--source: https://gist.githubusercontent.com/TimeTraveller-San/40a7a2655743bf6230706d45d1201b49/raw/25ddaaf54e946d33576e27e6bab45a40f22864a9/epsilon_greedy.py -->\n",
        "\n",
        "- $\\epsilon$ が大きければ、乱数 $r$ が $\\epsilon$ より大きくなることはほとんどなく、 ランダムな行動はほとんど起こらない (探索が少なく、知識利用 が多くなる)。\n",
        "- $\\epsilon$ が小さければ、乱数 $r$ は $\\epsilon$ よりも大きくなることが多いので、 エージェント は より 多くのランダム な 行動を選択することになる。\n",
        "\n",
        "このような確率的な性質を利用して、エージェントは環境をより探索することができるようになる。\n",
        "\n",
        "経験則として、$\\epsilon$ は通常 $0.9$ が選ばれる。だが $\\epsilon$ は 環境タイプに応じて変化させることができる。いくつかのケースでは、より高い探索に続いてより高い知識利用 を可能にするために、時間の経過とともに $\\epsilon$ は 緩和 漸減 される。\n",
        "\n",
        "以下 [OpenAI Gym Taxi-v3 環境](https://gym.openai.com/envs/Taxi-v3/) に適用された SARSA のシンプルな Python コードを示す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-2j4prnnW3I"
      },
      "source": [
        "## Taxi-v3 環境の説明\n",
        "\n",
        "* Taxi-v3 課題は [Dietterich(2000)](https://arxiv.org/pdf/cs/9905014.pdf) で階層型強化学習の問題点を説明するために導入された。\n",
        "* 4 つの場所（異なる文字でラベル付けされている）があり， エージェントの仕事は，ある場所で乗客を拾い，別の場所で降ろすことである。\n",
        "* 送迎が成功すると +20点，時間がかかるごとに 1 点ずつ減ります。\n",
        "* また，違法な乗降行為には 10 ポイントの罰則がある。\n",
        "\n",
        "<!--  This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning.\n",
        "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another.\n",
        "You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!--source: https://gymnasium.farama.org/environments/toy_text/taxi/#taxi_ref-->\n",
        "<img src=\"https://gymnasium.farama.org/_images/taxi.gif\">\n",
        "\n",
        "* Action Space: Discrete(6)\n",
        "* Observation Space: Discrete(500)\n",
        "* `import gymnasium.make(\"Taxi-v3\")`\n",
        "\n",
        "### 説明<!-- ### Description-->\n",
        "\n",
        "* 5x5 のグリッド世界には、4 つの指定乗降場所（赤、緑、黄、青）がある。タクシーはランダムなマスから出発し、乗客は指定場所のいずれかにいる。\n",
        "* 目的は、タクシーを乗客の位置まで移動させ、乗客を乗せ、乗客の希望する目的地まで移動し、乗客を降ろすことだ。乗客を降ろすと、そのエピソードは終了する。\n",
        "* 乗客を正しい場所で無事に降ろせると、プレイヤーはプラスの報酬を得る。乗客の乗降を誤った場合や、報酬を得られなかった移動ステップごとにマイナスの報酬が課される。\n",
        "\n",
        "<!--There are four designated pick-up and drop-off locations (Red, Green, Yellow and Blue) in the 5x5 grid world. The taxi starts off at a random square and the passenger at one of the designated locations.\n",
        "\n",
        "The goal is move the taxi to the passenger’s location, pick up the passenger, move to the passenger’s desired destination, and drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "The player receives positive rewards for successfully dropping-off the passenger at the correct location. Negative rewards for incorrect attempts to pick-up/drop-off passenger and for each step where another reward is not received. -->\n",
        "\n",
        "\n",
        "### マップ<!--Map-->:\n",
        "```\n",
        "    +---------+\n",
        "    |R: | : :G|\n",
        "    | : | : : |\n",
        "    | : : : : |\n",
        "    | | : | : |\n",
        "    |Y| : |B: |\n",
        "    +---------+\n",
        "```\n",
        "\n",
        "Tom Dieeterich (2000) MAX Q 価値関数分解を用いた階層的強化学習\"\n",
        "From “Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition” by Tom Dietterich [Dietterich2000].\n",
        "<!-- T. G. Dietterich, “Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition,” Journal of Artificial Intelligence Research, vol. 13, pp. 227–303, Nov. 2000, doi: 10.1613/jair.639. -->\n",
        "\n",
        "### 行動空間<!--Action Space-->\n",
        "\n",
        "アクションの形状は (1,) であり、範囲{0, 5}でタクシーの移動方向や乗客の乗降を示す。\n",
        "<!-- The action shape is (1,) in the range {0, 5} indicating which direction to move the taxi or to pickup/drop off passengers.-->\n",
        "\n",
        "0: 南（下）へ移動<br/>\n",
        "1: 北（上）へ移動<br/>\n",
        "2: 東（右）へ移動<br/>\n",
        "3: 西（左）へ移動<br/>\n",
        "4: 乗客を乗せる<br/>\n",
        "5: 乗客を降ろす<br/>\n",
        "<!--0: Move south (down)<br/>\n",
        "1: Move north (up)<br/>\n",
        "2: Move east (right)<br/>\n",
        "3: Move west (left)<br/>\n",
        "4: Pickup passenger<br/>\n",
        "5: Drop off passenger<br/> -->\n",
        "\n",
        "### 観察空間<!--Observation Space-->\n",
        "\n",
        "タクシーの位置が 25 箇所、乗客の位置が 5 通り（乗客がタクシー内にいる場合を含む）、目的地が 4 箇所あるため、状態は 500 通りある。<br/>\n",
        "地図上の目的地は、色の頭文字で表される。<br/>\n",
        "<!-- There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
        "Destination on the map are represented with the first letter of the color.-->\n",
        "\n",
        "**乗客の位置**：<br/>\n",
        "0: 赤<br/>\n",
        "1: 緑<br/>\n",
        "2: 黄<br/>\n",
        "3: 青<br/>\n",
        "4: タクシー内<br/>\n",
        "<!-- Passenger locations:\n",
        "0: Red<br/>\n",
        "1: Green<br/>\n",
        "2: Yellow<br/>\n",
        "3: Blue<br/>\n",
        "4: In taxi<br/> -->\n",
        "\n",
        "**目的地**：<br/>\n",
        "0: 赤<br/>\n",
        "1: 緑<br/>\n",
        "2: 黄<br/>\n",
        "3: 青<br/>\n",
        "<!-- Destinations:\n",
        "0: Red<br/>\n",
        "1: Green<br/>\n",
        "2: Yellow<br/>\n",
        "3: Blue<br/> -->\n",
        "\n",
        "観測値は対応する状態を符号化した整数として返される。計算式は ((タクシー行 * 5 + タクシー列) * 5 + 乗客位置) * 4 + 目的地\n",
        "<!-- An observation is returned as an int() that encodes the corresponding state, calculated by ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination -->\n",
        "\n",
        "なお、エピソード中に実際に到達可能な状態は 400 通りである。不足している状態は、乗客が目的地と同じ位置にいる状況に対応する。これは通常、エピソードの終了を示すためである。さらに、乗客とタクシーの両方が目的地に到着した直後、成功したエピソードの直後に観察可能な状態が4つ追加される。これにより、到達可能な離散状態は合計 404 通りとなる。\n",
        "<!--Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode. Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination. This gives a total of 404 reachable discrete states. -->\n",
        "\n",
        "\n",
        "\n",
        "### 開始状態<!-- ### Starting State-->\n",
        "\n",
        "初期状態は、乗客が目的地にもタクシー内にもいない状態から一様分布からサンプリングされる。初期状態は 300 通りある：タクシーの位置 25 通り、乗客の位置（タクシー内を除く）4 通り、目的地（乗客の現在地を除く）3 通り。\n",
        "<!-- The initial state is sampled uniformly from the possible states where the passenger is neither at their destination nor inside the taxi. There are 300 possible initial states: 25 taxi positions, 4 passenger locations (excluding inside the taxi) and 3 destinations (excluding the passenger’s current location). -->\n",
        "\n",
        "### 報酬<!-- ### Rewards -->\n",
        "\n",
        "-1 ステップごとに、他の報酬が発生しない限り減算される。<br/>\n",
        "+20 乗客を目的地まで送り届けた場合。<br/>\n",
        "-10 「乗客の乗車」および「乗客の降車」操作を違法に実行した場合。<br/>\n",
        "<!-- -1 per step unless other reward is triggered.<br/>\n",
        "+20 delivering passenger.<br/>\n",
        "-10 executing “pickup” and “drop-off” actions illegally.<br/> -->\n",
        "\n",
        "壁に衝突するなど、何もしない操作（noop）となった場合、その時間ステップ分のペナルティが発生する。noopは、infoで返されるaction_maskをサンプリングすることで回避できる。\n",
        "<!--An action that results a noop, like moving into a wall, will incur the time step penalty. Noops can be avoided by sampling the action_mask returned in info. -->\n",
        "\n",
        "### エピソード終了<!-- ### Episode End-->\n",
        "\n",
        "以下の場合、エピソードは終了する：<br/>\n",
        "終了：1. タクシーが乗客を降ろした時。<br/>\n",
        "切り詰め（time_limit ラッパー使用時）：1. エピソードの長さが 200 に達した時。<br/>\n",
        "<!--The episode ends if the following happens:<br/>\n",
        "Termination: 1. The taxi drops off the passenger.<br/>\n",
        "Truncation (when using the time_limit wrapper): 1. The length of the episode is 200.<br/> -->\n",
        "\n",
        "### 情報<!-- ### Information-->\n",
        "\n",
        "`step()` と `reset()` は以下のキーを持つ辞書を返す：\n",
        "\n",
        "`p` - 状態の遷移確率。<br/>\n",
        "`action_mask` - アクションが新しい状態への遷移を引き起こすかどうか。<br/>\n",
        "<!-- p - transition probability for the state.<br/>\n",
        "action_mask - if actions will cause a transition to a new state.<br/> -->\n",
        "\n",
        "場合によっては、アクションを実行してもエピソードの状態に影響しないことがある。v0.25.0では、`info[「action_mask」]`には各アクションについて、そのアクションが状態を変更するかどうかを指定する `np.ndarray` が含まれる。\n",
        "<!-- For some cases, taking an action will have no effect on the state of the episode. In v0.25.0, info[\"action_mask\"] contains a np.ndarray for each of the actions specifying if the action will change the state. -->\n",
        "\n",
        "状態を変更するアクションをサンプリングするには、以下の方法がある：\n",
        "`action = env.action_space.sample(info[「action_mask」])` あるいは <br/>\n",
        "Q 値ベースのアルゴリズムを使用する場合：`action = np.argmax(q_values[obs, np.where(info[「action_mask」] == 1)[0]])`\n",
        "<!--To sample a modifying action, use action = env.action_space.sample(info[\"action_mask\"]) Or with a Q-value based algorithm action = np.argmax(q_values[obs, np.where(info[\"action_mask\"] == 1)[0]]). -->\n",
        "\n",
        "<!-- `step()` and `reset()` return a dict with the following keys: -->\n",
        "`step()`と`reset()`は、以下のキーを持つ辞書を返す：\n",
        "\n",
        "### 引数<!--Arguments-->\n",
        "\n",
        "```python\n",
        "import gymnasium as gym\n",
        "gym.make('Taxi-v3')\n",
        "```\n",
        "\n",
        "* `is_raining=False`: True の場合、タクシーは目標方向へ 80% の確率で移動する。それ以外の場合は 目標方向の左右どちらかへ、左右同確率（各10%）で移動する。<br/><!-- is_raining=False: If True the cab will move in intended direction with probability of 80% else will move in either left or right of target direction with equal probability of 10% in both directions.-->\n",
        "* 気まぐれな乗客 `fickle_passenger=False`: `True` の場合、タクシーが乗客の出発地点から 1 マス離れた時点で、乗客が目的地を変更する確率が 30% ある。乗客の気まぐれは最初の乗車時かつ移動成功時のみ発生する。乗客が出発地点で降ろされ、再び乗車した場合、この挙動は再発しない。<!--fickle_passenger=False: If true the passenger has a 30% chance of changing destinations when the cab has moved one square away from the passenger’s source location. Passenger fickleness only happens on the first pickup and successful movement. If the passenger is dropped off at the source location and picked up again, it is not triggered again. -->"
      ],
      "metadata": {
        "id": "lmhmUFwbnxQN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWWeQemYSBS-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "s = env.reset()\n",
        "print(f'type(s):{type(s)}, len(s):{len(s)}')\n",
        "print(f's:{s}')\n",
        "\n",
        "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "print(n_states, n_actions)\n",
        "print(dir(env))\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVfYVuu3gCGK"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym # Changed from import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def init_q(s,           # 状態の数\n",
        "           a,           # 行動の数\n",
        "           type=\"ones\"  # `ones` であれば状態遷移表を 1 で初期化，`random` であれば乱数で初期化\n",
        "                        # `zeros` であれば 0 で初期化\n",
        "          ):\n",
        "    \"\"\" Q 表の初期化\"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q,           # 状態 X 行動 から 価値 を与える\n",
        "                   epsilon,     # 新規行動の探索率 イプシロン貪欲戦略のために使用\n",
        "                   n_actions,   # 可能な行動の選択肢数\n",
        "                   s,           # 状態\n",
        "                   train=False, # `True` Q 値を最大にする行動が決定論的に選択される\n",
        "                  ):\n",
        "    \"\"\" イプシロン貪欲に行動を探索する\"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "\n",
        "def sarsa(alpha,         # 学習率\n",
        "          gamma,         # 割引率\n",
        "          epsilon,       # イプシロン貪欲な探索率\n",
        "          episodes,      # 最大エピソード数\n",
        "          max_steps,     # 最大ステップ数\n",
        "          n_tests,       # エピソード数\n",
        "          render=False,\n",
        "          test=False):\n",
        "    \"\"\"\n",
        "    alpha: 学習率\n",
        "    gamma: 割引率\n",
        "    epsilon: イプシロン貪欲な探索率\n",
        "    max_steps: 各エピソードの最大ステップ数\n",
        "    n_tests: number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"エピソード: {episode}\")\n",
        "        total_reward = 0\n",
        "        s, _ = env.reset() # Modified to unpack (observation, info)\n",
        "        # Removed: s = s[0] as it's now handled by unpacking\n",
        "        #a = epsilon_greedy(Q, epsilon, n_actions, s[0])\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium step API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            a_ = epsilon_greedy(Q, epsilon, n_actions, s_)\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"このエピソードでのステップ数 {t}，総報酬:{total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Q 値:\\n{Q}\\nテスト開始:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=0.1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s, _ = env.reset() # Modified to unpack (observation, info)\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            # env.render() # Removed rendering in test_agent as it might cause issues in Colab or if not explicitly configured.\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"状態 {s} で行動 {a} を選択\")\n",
        "            s, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium step API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                print(f\"エピソード総報酬: {total_reward}\")\n",
        "                time.sleep(1)\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYobcf66rh0x",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "alpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 3000\n",
        "max_steps = 2500\n",
        "n_tests = 20\n",
        "timestep_reward = sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "#timestep_reward = qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "print(timestep_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy3pRTdLiNHy"
      },
      "source": [
        "# 3. Q 学習\n",
        "\n",
        "Q-学習 は、方針によらない TD 制御である。SARSA とほぼ同じであり，唯一の違いは、次の 動作(行動) $A$ を見つけるための 方針に従うのではなく、貪欲に 動作(行動)  を選択することである。\n",
        "SARSA と同様に Q 値 を評価することを目的としており、更新則 は次のようになる:\n",
        "\n",
        "$$\n",
        "Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma\\max_{\\alpha} Q(S_{t+1},a)-Q(S_t,A_t)\\right]\n",
        "$$\n",
        "\n",
        "SARSA では、ある方策 に従って 行動 $A'$ を選択していた。これに対し、上式 Q 学習 では 行動 $A'$ ($a$) は、 $Q$ の最大値を取るだけの 欲張り(貪欲, グリーディ)な 方法で選択される。\n",
        "\n",
        "Q 学習のアルゴリズムを以下に示す:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*Rf_H0YXhSPPm-iyBY2Gnjg.png\" width=\"49%\"><br/>\n",
        "\n",
        "Source: [Introduction to Reinforcement learning by Sutton and Barto — Chapter 6](https://cdn-images-1.medium.com/max/1280/1*Rf_H0YXhSPPm-iyBY2Gnjg.png)\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JhWNzdgSBS-"
      },
      "source": [
        "<center>\n",
        "<!-- <div style=\"background-color:lightgray;width:49%;text-align:left\"> -->\n",
        "<div style=\"color:white;background-color:gray;width:49%;align:center\">\n",
        "\n",
        "Q 学習 (オフポリシー TD 制御) for $\\pi\\approx\\pi_{\\star}$\n",
        "</div>\n",
        "<div style=\"background-color:whitesmoke;width:49%;text-align:left\">\n",
        "\n",
        "- $Q(s,a)$ を全状態 $s$ と 全動作 $a$ について初期化，$Q(\\text{終了状態},\\cdot)=0$ とする\n",
        "- 各エピソードを繰り返す:\n",
        "    - $S$ を初期化する\n",
        "    - 各エピソードを繰り返し\n",
        "        - $S$ の中から Q 関数に従って $A$ を選ぶ\n",
        "        - 行動 $A$ を起こし，$R$, $S'$ を観察する\n",
        "        - $Q(S,A)\\leftarrow Q(S,A)+\\alpha\\left[R+\\gamma \\max_{a} Q(S',a)-Q(S,A)\\right]$\n",
        "        - $S\\leftarrow S'$\n",
        "</div></center>\n",
        "\n",
        "Python コードは以下:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-Vu_7d_iR7r"
      },
      "outputs": [],
      "source": [
        "# import gym\n",
        "import numpy as np\n",
        "import time\n",
        "import gymnasium as gym # Added this import\n",
        "\n",
        "\"\"\"\n",
        "Q 学習はオフポリシー学習の python 実装。\n",
        "Sutton and Barto の著書 にある Q 学習アルゴリズムの python 実装\n",
        "SARSA と Q 学習の唯一の違いは、SARSA は現在の方針に基づいて次の行動を取るのに対し，Q 学習は次の状態で最大の効用を持つ行動をとることである。\n",
        "\"\"\"\n",
        "\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\"\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        s, _ = env.reset() # Modified for gymnasium API\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            a_ = np.argmax(Q[s_, :])\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s, _ = env.reset() # Modified for gymnasium API\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            # env.render() # Rendering can cause issues in Colab, keep commented for now.\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"Reached goal!\")\n",
        "                else:\n",
        "                    print(\"Shit! dead x_x\")\n",
        "                time.sleep(3)\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROXygIyTsDnw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "alpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 10000\n",
        "max_steps = 2500\n",
        "n_tests = 2\n",
        "timestep_reward = qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "print(timestep_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6-TQZN2iwTq"
      },
      "source": [
        "source: https://gist.github.com/TimeTraveller-San/9e56f9d09be7d50b795ef2f83be2ba72#file-qlearning-py\n",
        "\n",
        "# 3. 期待 SARSA\n",
        "\n",
        "期待 SARSA とは、その名が示すように、現在の状態で起こりうるすべての行動についての Q値 の期待値(平均値)を取る。ターゲットの更新則を使うと、より明確になる。\n",
        "<!-- Expected SARSA, as the name suggest takes the expectation (mean) of Q values for every possible action in the current state. The target update rule shall make things more clear:-->\n",
        "\n",
        "$$\n",
        "Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha\\left[R_{t+1}+\\gamma\\mathbb{E}\\left[Q(S_{t+1},A_{t+1}\\vert S_{t+1}\\right]-Q(S_t\n",
        ",A_t)\\right]\\\\ \\\\\n",
        "\\hspace{6em}\\leftarrow Q(S_t,A_t)+\\alpha\\left[\n",
        "R_{t+1}+\\gamma\\sum_{\\alpha}\\pi(a\\vert S_{t+1}) Q(S_{t+1},a) - Q(S_t,A_t)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Python コードを以下に示す:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPDLyhHQirtC"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym # Changed from import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\"\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "def expected_sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        total_reward = 0\n",
        "        s, _ = env.reset() # Modified for gymnasium API\n",
        "        t = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "            s_, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium step API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                expected_value = np.mean(Q[s_,:])\n",
        "                # print(Q[s,:], sum(Q[s,:]), len(Q[s,:]), expected_value)\n",
        "                Q[s, a] += alpha * (reward + (gamma * expected_value) - Q[s, a])\n",
        "            s = s_\n",
        "            if done:\n",
        "                if True:\n",
        "                    print(f\"This episode took {t} timesteps and reward {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=0.1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s, _ = env.reset() # Modified for gymnasium API\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            # env.render() # Rendering can cause issues in Colab, keep commented for now.\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, terminated, truncated, info = env.step(a) # Modified for gymnasium step API\n",
        "            done = terminated or truncated # Defined done for compatibility\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                print(f\"Episode reward: {total_reward}\")\n",
        "                time.sleep(1)\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_W61s7usjdg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.9\n",
        "episodes = 1000\n",
        "max_steps = 2500\n",
        "n_tests = 20\n",
        "timestep_reward = expected_sarsa(\n",
        "    alpha, gamma, epsilon,\n",
        "    episodes, max_steps, n_tests,\n",
        "    render=False, test=True)\n",
        "print(timestep_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXWhV9fnjcMM"
      },
      "source": [
        "source: https://gist.github.com/TimeTraveller-San/5bd93710e118633e0793dc5d0b92b19a#file-expectedsarsa-py\n",
        "\n",
        "# 比較\n",
        "\n",
        "以下のパラメータで 3 つのアルゴリズムを比較した\n",
        "\n",
        "```python\n",
        "lpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 2000\n",
        "max_steps = 2500 # (max number of time steps possible in a single episode)\n",
        "```\n",
        "\n",
        "ここでは、上記の 3 つの方策制御方法の比較 をプロットする。\n",
        "\n",
        "## 収束\n",
        "\n",
        "以下のプロットが示すように、Q 学習 (緑) は SARSA(オレンジ) と 期待 SARSA(青) の両者より先に収束した。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*0LpFOud2FUKciZ9jPd1ZBA.png\" width=\"49%\"><br/>\n",
        "SARSA, Q-learning & Expected SARSA — Convergence comparison\n",
        "</center>\n",
        "\n",
        "## 成績\n",
        "\n",
        "実装した 3 つのアルゴリズムでは、Q-学習 が最も良く、期待 SARSA が最も悪い性能のようである。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*IC0L25IzedYZ_TujMHpRKg.png\"><br/>\n",
        "SARSA, Q-learning & Expected SARSA — performance comparison!\n",
        "</center>\n",
        "\n",
        "# 結語\n",
        "\n",
        "TD 学習 (時間差学習)は 最も重要な強化学習の概念である。DQN や 二重 DQN のような、さらに派生したものは、AI の分野で有名な画期的な結果を達成している。\n",
        "Google の アルファ碁は、囲碁の世界チャンピオンを倒すために、CNN と DQN アルゴリズムを使用した。\n",
        "<!-- Temporal Difference learning is the most important reinforcement learning concept. It’s further derivatives like DQN and double DQN (I may discuss them later in another post) have achieved groundbreaking results renowned in the field of AI. Google’s alpha go used DQN algorithm along with CNNs to defeat the go world champion. You are now equipped with the theoretical and practical knowledge of basic TD, go out and explore! -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ujQHWRcgFqz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}