---
title: "第23回 2025年度開講 駒澤大学 人工知能"
author: "浅川 伸一"
layout: home
---

<div align="right">
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 28/Nov/2025<br/>
Appache 2.0 license<br/>
</div>

<link href="/css/asamarkdown.css" rel="stylesheet">

* [課題提出用フォルダ](https://drive.google.com/drive/u/6/folders/1dWRV6Fdwf5ANqMhE-CF2H0Tow2pI5l0N){:target="_blank"}
* [TD (時間差)学習, SARSA, 期待 SARSA, Q 学習 と Python 実装 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105Sarsa_Q_learning_expected_sarsa.ipynb){:target="_blank"}

# 本日のお品書き

1. 池上桃花さん，特別講義
2. 強化学習 の基本用語

<!-- 
* [実習 オーバーフィッティング，アンダーフィッテング <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_Visit_polynomilal_fittings_demo.ipynb){:target="_blank"}
* [符号化器・復号化器モデル ちはやふる <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}
* [PyTorch による Transfomer 実装 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602Transformer_from_scratch.ipynb){:target="_blank"}
* [chatGPT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0608rinna_chatGPT_demo.ipynb){:target="_blank"} -->

<!-- * WEAVER++, Dell モデルの再現シミュレーション
  - [他言語プライミング課題での事象関連電位 （ERP) のシミュレーション Roelofs, Cortex (2016) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Roelofs_ERP_bilingual_lemret.ipynb){:target="_blank"}
  - [概念バイアス `Conceptual Bias` (Reolofs, 2016) 絵画命名，単語音読，ブロック化，マルチモーダル統合 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Roelofs_Conceptual_bias.ipynb){:target="blank"}
  - [2 ステップ相互活性化モデルデモ (Foygell and Dell, 2000) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020ccap_Foygel_Dell2000_2step_interactive_activaition_model_demo.ipynb){:target="_blank"}
  - [WEVER++ デモ 2020-1205 更新 Reolofs(2019) Anomia cueing <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020ccap_Roelofs2019_Anomia_cueing_demo.ipynb){:target="_blank"} -->

<!-- * [1990 年代の Stroop 効果のシミュレーション<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1110Stroop_1990Cohen_model.ipynb){:target="_blank"}
* [転移学習による Stroop 効果のデモ<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1123Stroop_model.ipynb)

* [画像認識における注意 CAM <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2021_0618CAM_demo.ipynb) -->

<!-- * [リカレントニューラルネットワークによる文処理デモ 青空文庫より，夏目漱石 こころ](https://komazawa-deep-learning.github.io/character_demo.html)
* [CartoonGAN 実習<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0628CartoonGAN_demo.ipynb) -->


<!-- * PyTorch 関連 -->

<!-- * [Pytorch によるニューラルネットワークの構築 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1115PyTorch_buildmodel_tutorial_ja.ipynb){:target="_blank"}
  * [Dataset とカスタマイズと，モデルのチェックポイント，微調整 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0824pytorch_simple_fine_tune_tutorial.ipynb){:target="_blank"}
  * [PyTorch Dataset, DataLoader, Sampler, Transforms の使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0824pytorch_dataset_data_loader_sampler.ipynb){:target="_blank"} -->

<!-- * [オノマトペ関連 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2023_1115onomatope_generator.ipynb){:target="_blank"} -->

<!-- * [Stable-baselines3 を用いた PPO デモ Atari Lunalander 月面着陸 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0619stable_baselines3_demo_LunaLander_V2.ipynb)
* [Stable diffusion デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0707stable_diffusion.ipynb) -->


## 強化学習

未知の環境にある動作主(エージェント)がいて，このエージェントは環境と相互作用することで報酬を得ることができるとする。動作主(エージェント)は，累積報酬を最大化するように行動する必要がある。現実的には，ゲームで高得点を出すロボットや，物理的なアイテムを使って物理的な課題をこなすロボットなどが考えられる。だが，これらに限定されるものではない。
<!-- Say, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment.
The agent ought to take actions so as to maximize cumulative rewards.
In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these. -->

* [REINFORCE.js](https://komazawa-deep-learning.github.io/reinforcejs/){:target="_blank"}

<!-- ## 実習ファイル

- [ランダム探索  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)
- [方策勾配法  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)
- [SARSA  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)
- [Q学習  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb) -->

<!-- 以下のデモは，[OpenAI](https://openai.com/) 提供の強化学習環境 [gym](https://gym.openai.com/) を用いています。 -->

### 強化学習の基本概念<!-- ### Key Concepts -->

<!-- RL の重要な概念を正式に定義しましょう。Now Let's formally define a set of key concepts in RL. -->

* 動作主 (エージェントは) **環境** の中で行動する。
* 環境がある行動に対してどのように反応するかは，我々が知っているかどうかわからない **モデル** によって定義される。
* 動作主 (エージェント) は，環境の多くの **状態**（$s\in\mathcal{S}$）のうちの 1 つに留まることができ，多くの **行動**（$a\in\mathcal{A}$）のうちの 1 つを選択して， ある状態から別の状態に切り替えることができる。
* 動作主 (エージェント) がどの状態に到達するかは，状態間の遷移確率 ($P$) によって決定される。
* 行動を起こすと，環境はフィードバックとして **報酬** ($r\in\mathcal{R}$) を与える。
<!-- The agent is acting in an **environment**. How the environment reacts to certain actions is defined by a **model** which we may or may not know.
The agent can stay in one of many **states** ($s \in \mathcal{S}$) of the environment, and choose to take one of many **actions** ($a \in \mathcal{A}$) to switch from one state to another.
Which state the agent will arrive in is decided by transition probabilities between states ($P$).
Once an action is taken, the environment delivers a **reward** ($r \in \mathcal{R}$) as feedback.-->

* モデルは報酬関数と遷移確率を定義する。
* モデルがどのように動作するかを知っている場合と知らない場合があり，これにより 2 つの状況が区別される。
<!-- The model defines the reward function and transition probabilities.
We may or may not know how the model works and this differentiate two circumstances: -->

  * **モデルベース**
<!-- * **モデルを知る**：完全な情報で計画を立てる、モデルベースの RL を行う。-->
        環境が完全にわかっている場合 [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP) によって最適解を求めることができます。
<!-- アルゴリズム 101 の授業で習った "longest increasing partialence" や "traveling salesmen problem" をまだ覚えていますか？笑
これはこの記事の焦点ではありませんが。-->

  * **モデルフリー**
<!-- * **モデルを知らない**：不完全な情報での学習；モデルフリー RL を行うか，-->
アルゴリズムの一部として明示的にモデルを学習しようとする。
<!-- 以下の内容のほとんどは、モデルがわからない場合のシナリオに対応しています。 -->

<!-- The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP).
Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL.
This is not the focus of this post though.
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm.
Most of the following content serves the scenarios when the model is unknown.-->

* 動作主(エージェント)の **方針(ポリシー)**  $\pi(s)$ は，**総報酬** $G$ を最大化することを目的として，ある状態で取るべき最適な行動のガイドラインを提供する。
* 各状態には，その状態で対応する方針を実行することで得られる将来の報酬の期待値を予測する **価値** 関数 $V(s)$ が関連付けられる。
* 言い換えれば，価値関数は，ある状態がどれだけ良いかを定量化する。
* 強化学習で学習しようとするのは，方針関数 と 価値関数の両方。
<!-- The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **the goal to maximize the total rewards**.
Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy.
In other words, the value function quantifies how good a state is.
Both policy and value functions are what we try to learn in reinforcement learning.-->

<center>
<img src="/assets/RL_algorithm_categorization.png" width="94%"><br/>
<div style="text-align: justify;width:88%;background-color: cornsilk;">
価値，方針(ポリシー)，環境 のいずれをモデル化したいかに基づく強化学習アプローチのまとめ。
(画像出典：[David Silver の強化学習講座](https://youtu.be/2pWv7GOvuf0) より転載)
<!--Fig. 2. Summary of approaches in RL based on whether we want to model the value, policy, or the environment.
(Image source: reproduced from David Silver's RL course [lecture 1](https://youtu.be/2pWv7GOvuf0).)-->
</div>
</center>

* 動作主 (エージェント) と環境の相互作用には $t=1,2,\dots,T$ 時間内の一連の行動と観測された報酬が含まれる。
* この過程で，エージェントは環境に関する知識を蓄積し，最適な政策を学習し，最適な政策を効率的に学習するために，次にどのような行動を取るべきかを決定する。
* 時間ステップ $t$ における状態，行動，報酬をそれぞれ $S_t$, $A_t$, $R_t$ とする。
<!-- このように， インタラクションシーケンスは 1 つの **エピソード** (「試行」または「軌跡」とも呼ばれる) で完全に記述され， 系列は末端の状態 $S_T$ で終了します。 -->
<!-- The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, $t=1, 2, \dots, T$.
During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy.
Let's label the state, action, and reward at time step t as $S_t$, $A_t$, and $R_t$, respectively.
Thus the interaction sequence is fully described by one **episode** (also known as "trial" or "trajectory") and the sequence ends at the terminal state $S_T$:-->

$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$

<!-- RL アルゴリズムの様々なカテゴリーを調べる際によく遭遇する用語。 -->
<!-- Terms you will encounter a lot when diving into different categories of RL algorithms: -->

- **モデルベース**: モデルが既知であるか、アルゴリズムがそれを明示的に学習する。
- **モデルフリー**: 学習時にモデルに依存しない。
- **オンポリシー**<!-- **On-policy** -->: アルゴリズムの学習にターゲットポリシーからの決定論的な結果やサンプルを使用する。
- **オフポリシー**<!-- **Off-policy** -->: ターゲット・方策(ポリシー)ではなく，異なる<!-- ビヘイビア・ -->方策(ポリシー)で生成された上他繊維やエピソード分布で学習する。

<!--- **Model-based**: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.
- **Model-free**: No dependency on the model during learning.
- **On-policy**: Use the deterministic outcomes or samples from the target policy to train the algorithm.
- **Off-policy**: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.-->

#### 遷移と報酬 transition and reward<!-- #### Model: Transition and Reward -->

* モデルとは，環境を記述する実体。
* モデルを用いることで，環境がどのように動作主 (エージェント) と相互作用し，フィードバックを与えるかを学習または推論することができる。
* モデルには，遷移確率関数 $P$ と報酬関数 $R$ の 2 つの主要部分がある。

<!-- The model is a descriptor of the environment.
With the model, we can learn or infer how the environment would interact with and provide feedback to the agent.
The model has two major parts, transition probability function $$P$$ and reward function $R$. -->

* 状態 $s$ にいるとき，次の状態 $s’$ に到達して報酬 $r$ を得るために行動 $a$ をとることを決めたとする。
これは 1 つの **遷移** ステップと呼ばれ， タプル $(s,a,s',r)$ で表される。

<!-- Let's say when we are in state s, we decide to take action a to arrive in the next state s' and obtain reward r.
This is known as one **transition** step, represented by a tuple (s, a, s', r).-->

* 遷移関数 $P$ は，行動 $a$ を起こした後に報酬 $r$ を得て，状態 $s$ から $s’$ に遷移する確率を記録したものである。
ここでは $\mathbb{P}$ を「確率」の記号として用いる。
<!-- The transition function P records the probability of transitioning from state s to s' after taking action a while obtaining reward r.
We use $$\mathbb{P}$$ as a symbol of "probability".-->

$$
P(s', r \vert s, a)  = \mathbb{P} \left[S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a\right]
$$

したがって，状態遷移関数は $P(s', r \vert s, a)$ の関数として定義することができる。
<!-- Thus the state-transition function can be defined as a function of $P(s', r \vert s, a)$: -->

$$
P_{ss'}^a = P(s' \vert s, a) = \mathbb{P} \left[S_{t+1} = s' \vert S_t = s, A_t = a\right] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

報酬関数 $R$ は 1 つの行動 (アクション) によって引き起こされる次の報酬を予測する。
<!-- The reward function R predicts the next reward triggered by one action:-->

$$
R(s,a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s'\in \mathcal{S}} P(s',r|s,a)
$$

#### 方針 (ポリシー) policy<!-- #### Policy -->

* 方策 (ポリシー) とは，動作主 (エージェント) の行動関数 $\pi$ として，状態 $s$ においてどのような行動を取るべきかを示すもの。これは，状態 $s$ から行動 $a$ への写像であり，決定論的なものと確率論的なものがある。
<!-- Policy, as the agent's behavior function $\pi$, tells us which action to take in state s.
It is a mapping from state s to action a and can be either deterministic or stochastic:-->

* 決定論的: $\pi(s) = a$.
* 確率論的: $\pi(a \vert s) = \mathbb{P}_ {\pi} \left[A=a \vert S=s\right]$.

#### 価値関数 value function<!-- #### Value Function -->

* 価値関数は，将来の報酬の予測によって，状態の良さや，状態や行動がどれだけ報われるかを測定するもの。
* 将来の報酬は **リターン** とも呼ばれ，今後の報酬を割引いたものの総和。
* 時刻 $t$ から始まるリターン $G_t$ を計算すると以下のようになる:

<!--Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward.
The future reward, also known as **return**, is a total sum of discounted rewards going forward.
Let's compute the return $G_t$ starting from time t:-->

$$
G_{t} = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

$[0,1]$ の割引係数 $\gamma$  は，将来の報酬に罰則 (ペナルティ) を課すことになる。
<!-- The discounting factor $\gamma \in [0, 1]$ penalize the rewards in the future, because:-->

* 将来の報酬は不確実性が高い可能性がある。
例：株式市場。
* 将来の報酬はすぐに得られるものではない。
例：人間として 5 年後よりも今日楽しいことをしたいと思うかもしれない。
* 割引 (ディスカウント) は数学的な利便性をもたらす。
つまり，リターンを計算するために将来のステップを永遠に追跡する必要がないのです。
* 状態遷移グラフの無限ループを心配する必要はない

<!-- - The future rewards may have higher uncertainty; i.e. stock market.
- The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;).
- Discounting provides mathematical convenience; i.e., we don't need to track future steps forever to compute return.
- We don't need to worry about the infinite loops in the state transition graph.-->

* ある状態 $s$ の **状態値** は， 時間 $t$ でこの状態にある場合の期待リターンであり $S_{t}=s$ である。
<!-- The **state-value** of a state s is the expected return if we are in this state at time t, $S_t = s$: -->

$$
V_{\pi}(s) = \mathbb{E}_ {\pi}[G_t \vert S_t = s]
$$

* 同様に，状態と行動の対としての **行動価値**  $Q$ を以下のように定義する:
<!-- Similarly, we define the **action-value** ("Q-value"; Q as "Quality" I believe?) of a state-action pair as:-->

$$
Q_{\pi}(s, a) = \mathbb{E}_ {\pi}[G_t \vert S_t = s, A_t = a]
$$

* 方針 $pi$ に従うので，可能な行動に対する確率分布と Q 値を利用して，状態値を回復することができる。
<!-- Additionally, since we follow the target policy $\pi$, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value: -->

$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

行動価値と状態価値の際は，**アドバンテージ (advantage)** と呼ばれる
<!-- The difference between action-value and state-value is the action **advantage** function ("A-value"): -->

$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$

#### 最適価値と最適方策<!-- ### Optimal Value and Policy -->

* 最適価値関数は，最大リターンを産む
<!-- The optimal value function produces the maximum return: -->

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

* 最適方策は，最適価値関数によって達成される:
<!-- The optimal policy achieves optimal value functions: -->

$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

* $V_{\pi_{\star}}(s)=V_{\star}(s)$ かつ $Q_{\pi_{\star}}(s,a)=Q_{\star}(s,a)$ である
<!-- And of course, we have $V_{\pi_{\star}}(s)=V_{\star}(s)$ and $Q_{\pi_{\star}}(s, a) = Q_{\star}(s, a)$. -->


#### マルコフ決定過程<!-- ## Markov Decision Processes-->

* 広義には強化学習は，**マルコフ決定過程 (MDP: Markov Decision Process)** の一部である。
    * **マルコフ性 Markov property** とは，将来の状態が現在の状態にのみ依存すること。
    * とりわけ，不確かな状況でのマルコフ決定過程を **POMDP (Partially Observed Markov Decision Process)** と呼ぶ
<!-- In more formal terms, almost all the RL problems can be framed as **Markov Decision Processes** (MDPs).
All states in MDP has "Markov" property, referring to the fact that the future only depends on the current state, not the history: -->

$$
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
$$

逆に言えば，未来と過去とは **条件付き独立** である。
将来の意思決定は現在の状況によって定まる。
<!-- Or in other words, the future and the past are **conditionally independent** given the present, as the current state encapsulates all the statistics we need to decide the future. -->

<center>
<img src="/assets/agent_environment_MDP.png" width="49%"><br/>
Fig. 3. The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton & Barto (2017).
</center>

* マルコフ決定過程 (Markov deicison process) は  $\mathcal{M}=\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, の 5 つの要素で構成される
<!-- A Markov deicison process consists of five elements $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, where the symbols carry the same meanings as key concepts in the [previous](#key-concepts) section, well aligned with RL problem settings:-->

- $\mathcal{S}$: 状態集合<!-- - a set of states; -->
- $\mathcal{A}$: 行動集合 <!-- - a set of actions; -->
- $P$: 遷移関数 <!-- - transition probability function; -->
- $R$: 報酬関数<!-- - reward function; -->
- $\gamma$: 割引率<!--  - discounting factor for future rewards. -->

未知の環境では，$P$ と $R$ とは完全に知ることはできない
<!-- In an unknown environment, we do not have perfect knowledge about $P$ and $R$. -->

<center>
<img src="/assets/mdp_example.jpg" width="66%"><br/>
<div style="text-align: center;width:88%;background-color:cornsilk;">
マルコフ決定過程の例：典型的な仕事の一日
(画像出典: [randomant.net/reinforcement-learning-concepts](https://randomant.net/reinforcement-learning-concepts/))
<!--
Fig. 4. A fun example of Markov decision process: a typical work day.
![MDP example]({{ '/assets/images/mdp_example.jpg' | relative_url }}){: class="center"}
Fig. 4. A fun example of Markov decision process: a typical work day.-->
</div>
</center>

#### ベルマン方程式<!-- ### Bellman Equations -->

ベルマン方程式とは， 価値関数を目先の報酬と割引された将来の価値に分解する一連の方程式を指します。
<!-- Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values. -->

$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

Q 関数とは<!-- Similarly for Q-value, -->

$$
\begin{aligned}
Q(s, a)
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \\
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$

##### 期待ベルマン方程式<!-- ### Bellman Expectation Equations -->

* 再帰的な更新過程は，さらに分解すると，状態値関数と行動値関数の両方で構成される方程式となる。
* 今後の行動ステップを進める際には $\pi$ の方針に沿って $V$ と $Q$ を交互に拡張していく。

<!-- The recursive update process can be further decomposed to be equations built on both state-value and action-value functions.
As we go further in future action steps, we extend V and Q alternatively by following the policy $\pi$.-->

<center>
<img src="/assets/bellman_equation.png" width="60%"><br/>
<div style="text-align: center;width: 88%;background-color: powderblue;">
図 5. ベルマン期待値方程式がどのように状態値関数と行動値関数を更新するかを示す図。
<!--
Fig. 5. Illustration of how Bellman expection equations update state-value and action-value functions.
![Bellman]({{ '/assets/images/bellman_equation.png' | relative_url }}){: style="width: 60%;" class="center"}
-->
</div>
</center>

$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi
} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi
} (s', a')
\end{aligned}
$$

##### 最適ベルマン方程式<!-- ### Bellman Optimality Equations -->

* 方策 (ポリシー) に従った期待値を計算するのではなく，最適値にしか興味がないのであれば，方策 (ポリシー) を使わずに代替更新中の最大リターンにすぐに飛びつくことができる。
<!-- RECAP:  最適値 $V_*$ と $Q_*$ は，得られる最高のリターンであり [ここ](#optimal-value-and-policy) で定義されています。-->
<!-- If we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy.
RECAP: the optimal values $V_*$ and $Q_*$ are the best returns we can obtain, defined [here](#optimal-value-and-policy).-->

$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$

* 上式はベルマンの期待方程式と酷似している。
<!-- Unsurprisingly they look very similar to Bellman expectation equations. -->

* もし環境の完全な情報があれば，この問題は計画問題となり，動的計画法 (DP: dynamic programming) で解くことができる。
* 駄菓子菓子，ほとんどの場合 $P_{ss'}^a$ や $R(s, a)$ が不明であるため，ベルマン方程式を直接適用して MDP を解くことはできない。
* 駄菓子菓子，多くの RL アルゴリズムの理論的基礎を与えている。

<!--
If we have complete information of the environment, this turns into a planning problem, solvable by DP.
Unfortunately, in most scenarios, we do not know $P_{ss'}^a$ or $R(s, a)$, so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms. -->


<!--
```python
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
```

[cartpole 問題](https://www.youtube.com/watch?v=J7E6_my3CHk)を解いてみました

```bash
cd ~/study/2019tensorflow_models.git/research/a3c_blogpost
# python a3c_cartpole.py --train
python a3c_cartpole.py --algorithm=random --max-eps=4000
```
-->


<!-- # [GLUE: General Language Understanding Evaluation](https://gluebenchmark.com/leaderboard){:target="_blank"}

<div class="figure figcenter">
<img src="/2023assets/2019-08-21GLUE_leaderboard.png" width="77%">
<div class="figcaption">

[GLUE: General Language Understanding Evaluation](https://gluebenchmark.com/leaderboard){:target="_blank"}
</div></div>
 -->

<!-- 
### GLUE 下位課題

* CoLA: 入力文が英語として正しいか否かを判定
* SST-2: スタンフォード大による映画レビューの極性判断
* MRPC: マイクロソフトの言い換えコーパス。2 文 が等しいか否かを判定
* STS-B: ニュースの見出し文の類似度を 5 段階で評定
* QQP: 2 つの質問文の意味が等価かを判定
* MNLI: 2 入力文が意味的に含意，矛盾，中立を判定
* QNLI: Q and A
* RTE: MNLI に似た 2 つの入力文の含意を判定
* WNI: ウィノグラッド会話チャレンジ -->

<!-- ### SOTA モデルの特徴

* RoBERTa: BERT の訓練コーパスを巨大 (173GB) にし，ミニバッチサイズを大きした
* XLNet: 順列言語モデル。2 ストリーム注意
* MT-DNN: BERT ベース の転移学習に重きをおいたモデル
* GPT-X: BERT に基づく。人間超えして 2019 年 2 月時点で炎上騒ぎ
* BERT: Transformer に基づく言語モデル。**マスク化言語モデル**  と **次文予測** に基づく **事前訓練**，各下流課題を **ファインチューニング**。
事前訓練されたモデルは一般公開済。
* ELMo: 双方向 RNN による文埋め込み表現
* Transformer: 自己注意に基づく言語モデル。多頭注意，位置符号器.

<img src="/2023assets/2019Liu_mt-dnn.png" width="66%">

<img src="/2023assets/2017Vaswani_Fig2_1ja.svg">
<img src="/2023assets/2017Vaswani_Fig2_2ja.svg">

From Vaswani+2017 transformer Fig. 2
 -->

<!-- # 事前訓練とマルチ課題学習

<center>
<img src="/assets/mt-dnn.png" width="66%"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>

$$
\mathop{attention}\left(Q,K,V\right)=\mathop{dropout}\left(\mathop{softmax}\left(\frac{QK^\top}{\sqrt{d}
}\right)\right)V
$$

<center>
<img src="/assets/2017Vaswani_Fig2_1.svg" width="17%">
<img src="/assets/2017Vaswani_Fig2_2.svg" width="23%"><br/>
From [@2017Vaswani_transformer] Fig. 2
</center>


$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\mathop{head}_ {1},\ldots,\mathop{head}_ {h}\right)W^O
$$

where, $\text{head}_{i} = \text{Attention}\left(QW_i^Q,KW_{i}^K,VW_{i}^V\right)$

The projections are parameter matrices

- $W_i^Q\in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^K \in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$,
- $W^O\in\mathbb{R}^{hd_v\times d_{\mathop{model}}}$. $h=8$
- $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$
\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2
$$

$$
\text{PE}_{(\mathop{pos},2i)} = \sin\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

$$
\text{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$ -->

<!--
# BERT, GPT, ELMo 事前訓練の違い

- BERT:   トランスフォーマー，マスク化言語モデル，次文予測課題
- GPT:   順方向トランスフォーマー
- ELMo:  双方向 RNN による中間層の連結
-->

<!-- # 多言語対応

<center>
<img src="/assets/2019Lample_Fig1.svg" width="88%"><br/>
From [@2019Lample_Cross-lingual] Fig. 1
</center> -->


<!-- # BERT: ファインチューニング手続きによる性能比較

<center>
<img src="/assets/2019Devlin_mask_method21.jpg" width="66%"><br/>
マスク化言語モデルのマスク化割合の違いによる性能比較
</center>

マスク化言語モデルのマスク化割合は マスクトークン:ランダム置換:オリジナル=80:10:10 だけでなく，
他の割合で訓練した場合の 2 種類下流課題，
MNLI と NER で変化するかを下図 \ref{fig:2019devlin_mask_method21} に示した。
80:10:10 の性能が最も高いが大きな違いがあるわけではないようである。 -->

<!-- # BERT モデルサイズ比較
<center>
<img src="./assets/2019Devlin_model_size20.jpg" style="width:69%"><br/>
</center>
 -->

<!-- # BERT: モデルサイズ比較

<center>
<img src="/assets/2019Devlin_model_size20.jpg" width="59%"><br/>
モデルのパラメータ数による性能比較
</center>

パラメータ数を増加させて大きなモデルにすれば精度向上が期待できる。
下図では，横軸にパラメータ数で MNLI は青と MRPC は赤 で描かれている。
パラメータ数増加に伴い精度向上が認められる。
図に描かれた範囲では精度が天井に達している訳ではない。パラメータ数が増加すれば精度は向上していると認められる。 -->


<!-- # BERT: モデル単方向，双方向モデル比較

<center>
<img src="/assets/2019Devlin_directionality19.jpg" width="59%"><br/>
言語モデルの相違による性能比較
</center>

言語モデルをマスク化言語モデルか次単語予測の従来型の言語モデルによるかの相違による性能比較を
下図 \ref{fig:2019devlin_directionality19} に示した。
横軸には訓練ステップである。訓練が進むことでマスク化言語モデルとの差は 2 パーセントではあるが認められるようである。 -->


<!-- # BERT 事前訓練比較
<center>
<img src="/assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:66%"><br/>
</center>
-->

<!-- # BERT: 事前訓練比較

<center>
<img src="/assets/2019Devlin_Effect_of_Pretraining18.jpg" width="59%"><br/>
事前訓練の効果比較
</center>

図には事前訓練の比較を示しされている。
全ての事前訓練を用いた場合が青，次文訓練を除いた場合が赤，従来型言語モデルで次文予測課題をした場合を黄，
従来型言語モデルで次文予測課題なしを緑で描かれている。4 種類の下流課題は MNLI, QNLI, MRPC, SQuAD である。
下流のファインチューニング課題ごとに精度が分かれるようである。 -->

<!--![](2019document/2019Devlin_BERT_slides.pdf)-->
<!--8. [DistilBERT](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)-->

<!-- # under construction 従来モデルの問題点

BERT の意味，文法表現を知るために，從來モデルである word2vec の単語表現概説しておく。
各単語はワンホット onehot 表現からベクトル表現に変換するモデルを単語埋め込みモデル word embedding models あるいはベクト
ル表現モデル vector representation models と呼ぶ。
下図のように各単語を多次元ベクトルとして表現する。-->

<!-- # 単語埋め込みモデルの問題点

単語の意味が一意に定まらない場合，ベクトル表現モデルでは対処が難しい。
とりわけ多義語の意味を定めることは困難である。

下図の単語「アップル」は果物であるか，IT 企業であるかは，その単語を単独で取り出した場合一意に定める事ができない。

<div class="figcenter">

<img src="/2025assets/2019Devlin_BERT02upper.svg" wisth="74%;"><br/>
単語の意味を一意に定めることができない場合
</div>

単語の多義性解消のために，あるいは単語のベクトル表現を超えて，より大きな意味単位である，句，節，文のベクトル表現を得る努力がなされてきた。適切な普遍文表現ベクトルを得ることができれば，翻訳を含む多くの下流課題にとって有効だと考えられる。
seq2seq モデルは RNN の中間層に文情報が表現されることを利用した翻訳モデルであった

<div class="figcenter">

<img src="/2025assets/2019Devlin_BERT03.svg" wisth="74%;"><br/>
[@2014Sutskever_Sequence_to_Sequence] より
</div> -->

<!-- BERT は上述の從來モデルを凌駕する性能を示した。以下では BERT の詳細を見ていくこととする。 -->

<!-- # BERT: 事前訓練とマルチ課題学習

図は事前訓練と GLUE の各課題に対応するためファインチューニングを示している。事前訓練として図中レキシコンエンコーダと表記されている部分は，単語表現，位置符号器，文情報の 3 種類の信号の合成である。合成された入力信号がトランスフォーマーへ入力され事前訓練が行なわれる。事前訓練語，各課題毎にファインチューニングが施される。

<div class="figcenter">

<img src="/2025assets/mt-dnn.png" wisth="74%;"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</div>

 -->
