---
title: "第23回 2025年度開講 駒澤大学 人工知能"
author: "浅川 伸一"
layout: home
codemirror_mode: python
codemirror_mime_type: text/x-cython
---

<div align="right">
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 21/Nov/2025<br/>
Appache 2.0 license<br/>
</div>

<link href="/css/asamarkdown.css" rel="stylesheet">

* [課題提出用フォルダ](https://drive.google.com/drive/u/3/folders/10WUFCkw5Uf-jlfsfVSux09uPQEt6iq1T){:target="_blank"}

---

<!-- 
* [実習 オーバーフィッティング，アンダーフィッテング <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_Visit_polynomilal_fittings_demo.ipynb){:target="_blank"}
* [符号化器・復号化器モデル ちはやふる <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}
* [PyTorch による Transfomer 実装 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602Transformer_from_scratch.ipynb){:target="_blank"}
* [chatGPT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0608rinna_chatGPT_demo.ipynb){:target="_blank"} -->

<!-- * WEAVER++, Dell モデルの再現シミュレーション
  - [他言語プライミング課題での事象関連電位 （ERP) のシミュレーション Roelofs, Cortex (2016) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Roelofs_ERP_bilingual_lemret.ipynb){:target="_blank"}
  - [概念バイアス `Conceptual Bias` (Reolofs, 2016) 絵画命名，単語音読，ブロック化，マルチモーダル統合 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Roelofs_Conceptual_bias.ipynb){:target="blank"}
  - [2 ステップ相互活性化モデルデモ (Foygell and Dell, 2000) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020ccap_Foygel_Dell2000_2step_interactive_activaition_model_demo.ipynb){:target="_blank"}
  - [WEVER++ デモ 2020-1205 更新 Reolofs(2019) Anomia cueing <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020ccap_Roelofs2019_Anomia_cueing_demo.ipynb){:target="_blank"} -->

<!-- * [1990 年代の Stroop 効果のシミュレーション<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1110Stroop_1990Cohen_model.ipynb){:target="_blank"}
* [転移学習による Stroop 効果のデモ<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1123Stroop_model.ipynb)

* [画像認識における注意 CAM <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2021_0618CAM_demo.ipynb) -->

<!-- * [リカレントニューラルネットワークによる文処理デモ 青空文庫より，夏目漱石 こころ](https://komazawa-deep-learning.github.io/character_demo.html)
* [CartoonGAN 実習<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0628CartoonGAN_demo.ipynb) -->


<!-- * PyTorch 関連 -->

<!-- * [Pytorch によるニューラルネットワークの構築 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1115PyTorch_buildmodel_tutorial_ja.ipynb){:target="_blank"}
  * [Dataset とカスタマイズと，モデルのチェックポイント，微調整 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0824pytorch_simple_fine_tune_tutorial.ipynb){:target="_blank"}
  * [PyTorch Dataset, DataLoader, Sampler, Transforms の使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0824pytorch_dataset_data_loader_sampler.ipynb){:target="_blank"} -->

* [オノマトペ関連 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2023_1115onomatope_generator.ipynb){:target="_blank"}

* [Stable-baselines3 を用いた PPO デモ Atari Lunalander 月面着陸 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0619stable_baselines3_demo_LunaLander_V2.ipynb)
* [Stable diffusion デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0707stable_diffusion.ipynb)


# 強化学習，条件付けの古典

<img src="https://www.nobelprize.org/images/pavlov-12840-content-portrait-mobile-tiny.jpg" width="24%">
<a href="https://www.nobelprize.org/prizes/medicine/1904/pavlov/biographical/">Ian Pavlov</a>&nbsp;&nbsp;
<img src="https://www.bfskinner.org/wp-content/gallery/1970s-1990/BFS-IN-THE-OFFICE.jpg" width="24%">
<a href="https://www.bfskinner.org/archives/photos/">Burrhus Frederic Skinner</a>&nbsp;&nbsp;
<img src="/2025assets/sutton_0160594.jpeg" width="24%">
<a href="https://awards.acm.org/award-recipients/sutton_0160594">Richard S. Sutton,</a>&nbsp;&nbsp;
<!-- <img src="http://incompleteideas.net/sutton-head5.jpg" width="24%"> -->
<!-- <img src="https://cloudfront.ualberta.ca/-/media/science/people/rsutton/sutton.jpg" width="24%"> -->
<!-- <a href="http://incompleteideas.net/">Richard S. Sutton,</a>&nbsp;&nbsp; -->
<img src="https://people.cs.umass.edu/~barto/barto2006-lowres.jpg" width="24%">
<a href="https://people.cs.umass.edu/~barto/">Andrew G. Barto</a>



- [パブロフ](https://en.wikipedia.org/wiki/Ivan_Pavlov) (Ivan Petrovich Pavlov; 1849/Sep/14-1936/Feb/27)古典的条件づけ 1904 年ノーベル医学生理学賞
- [スキナー](https://en.wikipedia.org/wiki/B._F._Skinner) (Burrhus Frederic Skinner; 1904/Mar/20-1990/Aug/18) 道具的条件付け， オペラント条件づけ，[スキナー箱, Skinner(1938) Fig.1, page 39 より](./assets/1938Skinner_Fig1_skinnerBOX.jpg)
- [Sutton](http://incompleteideas.net/) and [Barto](http://www-anw.cs.umass.edu/~barto/) の強化学習 [初版 1998年](http://incompleteideas.net/book/first/the-book.html), [第2版 2018年](http://incompleteideas.net/book/the-book-2nd.html), [初版は翻訳あり](https://www.amazon.co.jp/dp/4627826613/)，第2版は pdf ファイルで[ダウンロード可能](http://incompleteideas.net/book/bookdraft2017nov5.pdf)




# 強化学習とは何か？

<center>
<img src="/assets/2018Sutton_Fig3j.svg" style="width:74%"><br/>
<p align="center" style="width:74%">
Sutton & Barto (2018) Fig. 3.2 を改変
</p>
</center>

強化学習という言葉は古い言葉ですが機械学習の文脈では，環境とその環境におかれた動作主（エージェントと言ったり，ロボットシステムだったりします）が，環境と相互作用しながらより良い行動を形成するためのモデルです。
動作主は，環境から受け取った現在の状態を分析して，次にとるべき行動を選択します。このとき将来に渡って報酬が最大となるような行動を学習する手法の一つです。

2015 年には，Google 傘下のデープマインドというスタートアップチームが開発した囲碁プログラム AlphaGo がプロ棋士のイ・セドル氏に勝利し話題になりました。
AlphaGo は強化学習を基本技術の一つとして用いています。

1. [強化学習(1): 基礎](https://komazawa-deep-learning.github.io/rl01_elements.pdf)
2. [強化学習(2): エージェントと環境](https://komazawa-deep-learning.github.io/rl02_agentAndEnv.pdf)
3. [強化学習(3): 目標と報酬](https://komazawa-deep-learning.github.io/rl03_goalAndReward.pdf)
4. [強化学習(4): マルコフ決定過程](https://komazawa-deep-learning.github.io/rl04_mdp.pdf)
5. [強化学習(5): 価値反復，方策反復](https://komazawa-deep-learning.github.io/rl05_vi.pdf)
6. [強化学習(6): ](https://komazawa-deep-learning.github.io/rl06_advanced.pdf)
6. [強化学習(7): ](https://komazawa-deep-learning.github.io/rl07_robotics.pdf)

<!-- - エージェントと環境，マルコフ決定過程 MDP，POMDP，効用関数，ベルマン方程式，探索と利用のジレンマ，SARSA:
- 価値，方策，Q 学習，モデルベース対モデルフリー，アクター=クリティック:
- 深層 Q 学習:
- ゲーム AI へ (AlphaGo，AlphaGoZero，OpenAI five):
- セルフプレイ:
- 最近の発展 A3C，Rainbow，RDT，World model: -->

<!--
### 方策，報酬，価値観数，(環境)モデル

- $s,s'$: 状態 state
- $a$: 行動，行為 action
- $r$: 報酬 reward
- $t$: 時間 (離散時間 $t=1,2,\ldots,T$)
- $p(s',r\vert s, a)$: 状態 $s$ で行為 $a$ を行ったとき，報酬 $r$ を受け取って 状態 $s'$ に遷移する確率
- $p(s'\vert s, a)$: 状態 $s$ で行為 $a$ を行った場合，状態 $s'$ へ遷移する確率
- $r(s,a)$: 状態 $s$ で行為 $a$ を行った場合の即時報酬 immediate reward の期待値
- $r(s,a,s')$: 行為 $a$ を行った場合状態が $s$ から $s'$ へ変化したときの即時報酬の期待値
- $v_\pi(s)$: 方策 $\pi$ での状態 $s$ の価値 (期待報酬)
- $v_*(s)$: 最適方策化での状態 $s$ の価値
- $q_\pi(s,a)$: 方策 $\pi$ のもとで状態 $s$ で行為 $a$ をおこなた場合の価値
- $q_*(a)$: 行動 $a$ を行った場合の期待報酬(真の報酬) <!-- true value (expected reward) of action $a$-->

<!--
- $Q_t(a)$: 時刻 $t$ での $q_*(a)$ の期待値 qestimate at time $t$ of $q_*(a)$
- $N_t(a)$: 時刻 $t$ で行為 $a$ を行った回数 number of times action $a$ has been selected up prior to time $t$
- $H_t(a$) 時刻 $t$ で行為 $a$ を行う傾向(選好 preference) learned preference for selecting action a at time $t$
- $\pi_t(a)$: 時刻 $t$ で行為 $a$ を選択する確率 probability of selecting action a at time $t$
- $R_t$: $\pi_t$ が与えられた場合，時刻 $t$ における の期待報酬 estimate at time $t$ of the expected reward given $\pi_t$
--->

### [Richard Sutton, ACM Turing Award 2024](https://awards.acm.org/award-recipients/sutton_0160594)

<div class="figcenter">
<img src="https://awards.acm.org/binaries/content/gallery/acm/awards/photo/s/sutton_0160594" width="24%"><br/>
Award Recipient: Dr. Richard Sutton, ACM A. M. Turing Award (2024)
</div>

Andrew Barto と Richard Sutton が、2024 年度 ACM A.M.チューリング賞を受賞した。1980 年代に始まった一連の論文において、Barto と Sutton は、知能システムを構築するための最も重要なアプローチの一つである強化学習の主要なアイデアを提示し、数学的基礎を構築し、重要なアルゴリズムを開発した。Barto はマサチューセッツ大学 Amherst 校のコンピュータサイエンスの名誉教授であり、Sutton は Alberta 大学のコンピュータサイエンスの教授であり、Keen Technologies キーン・テクノロジーズの研究者。
<!-- Andrew Barto and Richard Sutton are the recipients of the 2024 ACM A.M. Turing Award. In a series of papers beginning in the 1980s, Barto and Sutton introduced the main ideas, constructed the mathematical foundations, and developed important algorithms for reinforcement learning, one of the most important approaches for creating intelligent systems. Barto is Professor Emeritus of Computer Science at the University of Massachusetts, Amherst; Sutton is Professor of Computing Science at the University of Alberta and Research Scientist at Keen Technologies. -->

人工知能（AI）の分野は、一般的にエージェント、つまり知覚し行動する存在の構築に取り組んでいる。より知的なエージェントとは、より良い行動方針を選択するエージェントのことである。したがって、ある行動方針が他の行動方針よりも優れているという概念は、AI の中心的な概念である。
<!-- The field of artificial intelligence (AI) is generally concerned with constructing agents-that is, entities that perceive and act. More intelligent agents are those that choose better courses of action. Thus, the notion that some courses of action are better than others is central to AI.-->

報酬 Reward（心理学と神経科学から借用された用語）とは、エージェントの行動の質に関連してエージェントに与えられる信号を指す。強化学習（RL: Reinforcement Learning）とは、この信号を与えられてより成功する行動をとるように学習する過程である。
<!-- Reward-a term borrowed from psychology and neuroscience-denotes a signal provided to an agent related to the quality of its behavior; and reinforcement learning (RL) is the process of learning to behave more successfully given this signal. -->

報酬から学習するという考え方は、動物の調教師には数千年もの間、よく知られていた。Alan Turing 自身も、1950 年の論文「計算機械と知能」の中で、「報酬と罰」に基づく機械学習のアプローチを提案し、このアプローチを用いていくつかの初期実験を行ったと報告している。Arthur Samuel が 1956 年にテレビで実演した自己学習型チェッカープログラムは、おそらく強化学習の最初の成功例と言えるだろう。ただし、それがなぜ機能するのか、あるいは機能するかどうかについて、いかなる根拠もなかった。
<!-- The idea of learning from reward has been familiar to animal trainers for thousands of years. Alan Turing himself, in his 1950 paper "Computing Machinery and Intelligence," proposed an approach to machine learning based on "rewards and punishments" and reported having conducted some initial experiments with this approach. Arthur Samuel's self-learning checker-playing program, demonstrated on television in 1956, was perhaps the first successful example of reinforcement learning-although it lacked any form of justification as to why or whether it would work. -->

AI 分野において、この点でのさらなる進歩はほとんど見られなかったが、1980 年代初頭、Barto と博士課程の学生 Sutton が心理学の観察に着想を得て、強化学習を一般的な問題枠組みとして定式化し始めた。彼らは、マルコフ決定過程（MDP: Markov Decision Process）の数学的基盤を利用した。MDP では、エージェントは確率的な環境において意思決定を行い、各遷移ごとに報酬信号を受け取り、長期的な累積報酬の最大化を目指す。標準的な MDP 理論では、エージェントが MDP に関するすべての情報を知っていると想定されているが、RL の枠組みでは、環境と報酬が未知であっても構わない。RL の最小限の情報要件と MDP の枠組みの汎用性を組み合わせることで、RL アルゴリズムを幅広い問題に適用することが可能になる。詳細は後述する。
<!-- Within the field of AI, little further progress occurred in this vein until the early 1980s, when Barto and his Ph.D. student Sutton, motivated by observations from psychology, began to formulate reinforcement learning as a general problem framework. They drew on the mathematical foundation provided by Markov decision processes (MDPs), wherein an agent makes decisions in a stochastic environment, receiving a reward signal after each transition and aiming to maximize its long-term cumulative reward. Whereas standard MDP theory assumes that everything about the MDP is known to the agent, the RL framework allows for the environment and the rewards to be unknown. The minimal information requirements of RL, combined with the generality of the MDP framework, allows RL algorithms to be applied to a vast range of problems, as explained further below. -->

Barto と Sutton は、共同で、また他の研究者と共同で、時間差分学習 Temporal Difference、方針勾配法 Policy Gradient、学習済み関数を表現するツールとしてのニューラルネットワークの利用など、RL の基本的なアルゴリズム的アプローチの多くを開発した。彼らはまた、学習と計画を組み合わせたエージェント設計を提案し、計画の基礎として環境に関する知識を獲得することの価値を示した。同様に重要なのは、この分野の標準的な参考文献であり、75,000 回以上引用されている教科書『強化学習入門』（1998年）であろう。この本によって、何千人もの研究者がこの新興分野を理解し、貢献することができた。その結果、強化学習は今日のコンピュータサイエンスにおいて最も活発な研究分野の一つとなっている。
<!-- Barto and Sutton, jointly and with other authors, developed many of the basic algorithmic approaches for RL, including temporal difference learning, policy-gradient methods, and the use of neural networks as a tool to represent learned functions. They also proposed agent designs that combined learning and planning, demonstrating the value of acquiring knowledge of the environment as a basis for planning. Perhaps equally important was the textbook, Reinforcement Learning: An Introduction (1998), which is the standard reference in the field and has been cited over 75,000 times. It allowed thousands of researchers to understand and contribute to this emerging field. As a result, RL is among the most active research areas in computer science today. -->

近年の強化学習の最も顕著な例は、2016 年と 2017 年に AlphaGo が人間の最強囲碁プレイヤーに勝利したことである。しかし、強化学習はロボットの運動技能学習、ネットワーク輻輳制御、チップ設計、インターネット広告、最適化、グローバルサプライチェーンの最適化、チャットボットの行動と推論能力の向上、さらにはコンピュータサイエンスにおける最も古い問題の一つである行列乗算のアルゴリズムの改善など、多くの分野で成功を収めている。ついに、神経科学に部分的にヒントを得た技術が恩返しをした。Barto の研究を含む最近の研究では、AI で開発された特定の RL アルゴリズムが、脳内のドーパミン系に関するさまざまな発見を最もよく説明することが示されている。
<!--The most prominent example of RL in recent years was the victory by AlphaGo over the best human Go players in 2016 and 2017; but RL has achieved success in many areas including robot motor skill learning, network congestion control, chip design, internet advertising, optimization, global supply chain optimization, improving the behavior and reasoning capabilities of chatbots, and even improving algorithms for one of the oldest problems in computer science, matrix multiplication. Finally, a technology that was partly inspired by neuroscience has returned the favor: recent research, including work by Barto, has shown that specific RL algorithms developed in AI provide the best explanations for a wide range of findings concerning the dopamine system in the brain. -->

<div class="figcenter">

<iframe width="705" height="397" src="https://www.youtube.com/embed/sNtuVLO3yaE" title="2024 ACM A.M. Turing Award" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<!-- <video src="/2025assets/2024ACM_Turing_Award.mp4" type="video/mp4" autoplay muted playsinline loop style="margin:auto; width:44%;" ></video> -->
</div>

ACM は、強化学習の概念的およびアルゴリズム的基盤の構築に尽力した功績により、[Andrew G. Barto](https://awards.acm.org/award-recipients/barto_9471663){:target="_blank"} と [Richard S. Sutton](https://awards.acm.org/award-recipients/sutton_0160594){:target="_blank"} を 2024 年度 ACM A.M. Turing 賞の受賞者に選出した。1980 年代に始まった一連の論文において、Burto と Sutton は、強化学習の主要なアイデアを提示し、数学的基盤を構築し、重要なアルゴリズムを開発しました。強化学習は、知能システムを構築するための最も重要なアプローチの 1 つである。

Burto は、マサチューセッツ大学 Amherst 校の情報科学・コンピュータサイエンス科の名誉教授です。Sutton は、Alberta 大学のコンピュータサイエンス科教授、Keen Technologies の研究者、そして Amii（アルバータ機械知能研究所）のフェローである。

ACM A.M.チューリング賞は「コンピューター界のノーベル賞」とも呼ばれ、Google 社による資金援助を受けて 100 万ドルの賞金が授与される。この賞は、コンピューターの数学的基礎を明確にしたイギリスの数学者、アラン・M・Turing にちなんで名付けられた。

<!-- ACM has named Andrew G. Barto and Richard S. Sutton as the recipients of the 2024 ACM A.M. Turing Award for developing the conceptual and algorithmic foundations of reinforcement learning. In a series of papers beginning in the 1980s, Barto and Sutton introduced the main ideas, constructed the mathematical foundations, and developed important algorithms for reinforcement learning—one of the most important approaches for creating intelligent systems.

Barto is Professor Emeritus of Information and Computer Sciences at the University of Massachusetts, Amherst. Sutton is a Professor of Computer Science at the University of Alberta, a Research Scientist at Keen Technologies, and a Fellow at Amii (Alberta Machine Intelligence Institute).

The ACM A.M. Turing Award, often referred to as the “Nobel Prize in Computing,” carries a $1 million prize with financial support provided by Google, Inc. The award is named for Alan M. Turing, the British mathematician who articulated the mathematical foundations of computing. -->

### 強化学習とは？<!-- ### What is Reinforcement Learning?-->

人工知能（AI）の分野は、一般的にエージェント、つまり知覚し行動する存在の構築に取り組んでいる。より知的なエージェントとは、より良い行動方針を選択するエージェントのことである。したがって、ある行動方針が他の行動方針よりも優れているという概念は AI の中心的な概念である。報酬（心理学と神経科学から借用された用語）とは、エージェントの行動の質に関連してエージェントに与えられる信号を指す。強化学習（RL）とは、この信号を与えられたときに、より効果的な行動をとるように学習過程である。
<!-- The field of artificial intelligence (AI) is generally concerned with constructing agents—that is, entities that perceive and act. More intelligent agents are those that choose better courses of action. Therefore, the notion that some courses of action are better than others is central to AI. Reward—a term borrowed from psychology and neuroscience—denotes a signal provided to an agent related to the quality of its behavior. Reinforcement learning (RL) is the process of learning to behave more successfully given this signal. -->

報酬から学習するという考え方は、動物の調教師にとって数千年もの間馴染み深いものであった。その後、Alan Turing が 1950 年に発表した論文「計算機械と知能」は、「機械は考えることができるか？」という問いに答え、報酬と罰に基づく機械学習へのアプローチを提案した。
<!-- The idea of learning from reward has been familiar to animal trainers for thousands of years. Later, Alan Turing’s 1950 paper “Computing Machinery and Intelligence,” addressed the question “Can machines think?” and proposed an approach to machine learning based on rewards and punishments. -->

Turing はこのアプローチを用いた初期の実験を行ったと報告し、Arther Samuel は 1950 年代後半に自己対戦から学習するチェッカー・プログラムを開発したが、その後数十年間、この分野の AI はほとんど進歩しなかった。1980 年代初頭、心理学の観察に触発され、Burto と博士課程の学生 Sutton は、強化学習を一般的な問題枠組みとして定式化し始めた。
<!-- While Turing reported having conducted some initial experiments with this approach and Arthur Samuel developed a checker-playing program in the late 1950s that learned from self-play, little further progress occurred in this vein of AI in the following decades. In the early 1980s, motivated by observations from psychology, Barto and his PhD student Sutton began to formulate reinforcement learning as a general problem framework. -->

彼らは、Markov 決定過程（MDP）が提供する数学的基盤を利用した。MDP では、エージェントは確率的（ランダムに決定される）環境において意思決定を行い、各遷移後に報酬信号を受け取り、長期的な累積報酬の最大化を目指す。標準的な MDP 理論では、エージェントが MDP に関するすべての情報を知っていると想定されるが、強化学習（RL）の枠組みでは、環境と報酬が未知であっても構わない。強化学習の最小限の情報要件と MDP の枠組みの汎用性を組み合わせることで、強化学習アルゴリズムを幅広い問題に適用することが可能になる。これについては後述する。
<!-- They drew on the mathematical foundation provided by Markov decision processes (MDPs), wherein an agent makes decisions in a stochastic (randomly determined) environment, receiving a reward signal after each transition and aiming to maximize its long-term cumulative reward. Whereas standard MDP theory assumes that everything about the MDP is known to the agent, the RL framework allows for the environment and the rewards to be unknown. The minimal information requirements of RL, combined with the generality of the MDP framework, allows RL algorithms to be applied to a vast range of problems, as explained further below. -->

Barto と Sutton は、他の研究者と共同で、強化学習（RL）の基本的なアルゴリズム的アプローチの多くを開発した。これには、報酬予測問題の解決に重要な進歩をもたらした時間差分学習（TDP）の最も重要な貢献、方針勾配法、そして学習済み関数を表現するツールとしてのニューラルネットワークの利用が含まれる。また、学習と計画を組み合わせたエージェント設計を提案し、計画の基礎として環境に関する知識を獲得することの価値を示した。
<!--Barto and Sutton, jointly and with others, developed many of the basic algorithmic approaches for RL. These include their foremost contribution, temporal difference learning, which made an important advance in solving reward prediction problems, as well as policy-gradient methods and the use of neural networks as a tool to represent learned functions. They also proposed agent designs that combined learning and planning, demonstrating the value of acquiring knowledge of the environment as a basis for planning. -->

おそらく同等に影響力があったのは、彼らの教科書『強化学習入門』（1998年）である。これは今なおこの分野の標準的な参考文献であり、75,000 回以上引用されている。この本によって数千人の研究者がこの新興分野を理解し貢献することが可能となり、今日でもコンピュータサイエンスにおける多くの重要な研究活動の原動力となっている。
<!-- Perhaps equally influential was their textbook, Reinforcement Learning: An Introduction (1998), which is still the standard reference in the field and has been cited over 75,000 times. It allowed thousands of researchers to understand and contribute to this emerging field and continues to inspire much significant research activity in computer science today.-->

Barto と Sutton のアルゴリズムは数十年前に開発されたが、強化学習の実用的な応用における大きな進歩は、過去 15 年間に強化学習と深層学習アルゴリズム（2018 年チューリング賞受賞者 Bengio, Hinton, LeCun が先駆けた）を融合させることで実現した。これが深層強化学習という技術につながった。
<!-- Although Barto and Sutton’s algorithms were developed decades ago, major advances in the practical applications of RL came about in the past fifteen years by merging RL with deep learning algorithms (pioneered by 2018 Turing Awardees Bengio, Hinton, and LeCun). This led to the technique of deep reinforcement learning. -->

強化学習の最も顕著な事例は、2016 年と 2017 年に AlphaGo が人間のトップ囲碁プレイヤーを破った勝利だ。最近のもう一つの大きな成果は、チャットボット ChatGPT の開発である。ChatGPT は大規模言語モデル（LLM）であり、二段階の訓練を経ており、第二段階では人間の期待値を捉えるために「人間からのフィードバックによる強化学習（RLHF）」と呼ばれる技術が用いられている。
<!-- The most prominent example of RL was the victory by the AlphaGo computer program over the best human Go players in 2016 and 2017. Another major achievement recently has been the development of the chatbot ChatGPT. ChatGPT is a large language model (LLM) trained in two phases, the second of which employs a technique called reinforcement learning from human feedback (RLHF), to capture human expectations. -->

強化学習は他の多くの分野でも成功を収めている。注目すべき研究例として、ロボットの運動技能学習が挙げられる。具体的には、ロボットによる物理的操作（ルービックキューブ）の解決において、強化学習の全工程をシミュレーション環境で実施しながらも、最終的には大きく異なる実世界での成功を可能にした。
<!-- RL has achieved success in many other areas as well. A high-profile research example is robot motor skill learning in the in-hand robotic manipulation and solution of a physical (Rubik’s Cube), which showed it possible to do all the reinforcement learning in simulation yet ultimately be successful in the significantly different real world. -->

その他の応用分野には、ネットワーク輻輳制御、チップ設計、インターネット広告、最適化、グローバルサプライチェーン最適化、チャットボットの行動・推論能力向上、さらにはコンピュータサイエンスにおける最も古い問題の一つである行列乗算のアルゴリズム改善まで含まれる。
<!-- Other areas include network congestion control, chip design, internet advertising, optimization, global supply chain optimization, improving the behavior and reasoning capabilities of chatbots, and even improving algorithms for one of the oldest problems in computer science, matrix multiplication. -->

最後に、神経科学から部分的に着想を得た技術が恩返しをした。Barto らの研究を含む最近の知見は、AI で開発された特定の強化学習アルゴリズムが、ヒト脳のドーパミン系に関する広範な発見を最もよく説明することを示している。
<!--Finally, a technology that was partly inspired by neuroscience has returned the favor. Recent research, including work by Barto, has shown that specific RL algorithms developed in AI provide the best explanations for a wide range of findings concerning the dopamine system in the human brain. -->

* [スットン 苦い教訓 Bitter Lesson](/2021cogpsy/2019Sutton_Bitter_Lesson.pdf){:target="_blank"}


## 複雑な状況をどう理解して解決するのか？

- 強化学習というニューラルネットワークモデルがあるわけではない
- 動的で複雑な環境に対処 $\rightarrow$ **強化学習** + DL $\rightarrow$ 一般人工知能への礎

- DQN ATARIのビデオゲーム, [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236){:target="_blank"}
- AlphaGo 囲碁, [https://www.nature.com/articles/nature16961](https://www.nature.com/articles/nature16961){:target="_blank"}
- AlphaGoZero 囲碁, [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270){:target="_blank"}

# Deep Q Network

<center>

<img src="/assets/2015DQNFig1.svg" width="66%"><br/>
DQN の模式図, 原著論文より
</center>

<!-- <video src="/assets/MOV_0013s.mp4" type="video/mp4" autoplay muted playsinline loop style="margin:auto; width:44%;" ></video>
<video src="/assets/MOV_0071s.mp4" type="video/mp4" autoplay muted playsinline loop style="margin:auto; width:44%;" ></video> -->

<!-- - [ギャラガ1](/assets/MOV_0013s.mp4)
- [ギャラガ2](/assets/MOV_0071s.mp4) -->

- **Q 学習** Q learning に DNN を採用
- CNN が LeNet, [@1998LeCun] そうであったように，強化学習 RL も昔からの技術 [@Sutton_and_Barto1998]
- ではなぜ，今になって囲碁や自動運転に応用できるようになったのか？
  - $\Rightarrow$ コンピュータの能力, データ規模，アルゴリズムの改良, エコシステム(ArXiv, Linux, Git, ROS, AMT, TensorFlow)

- 強化学習 $\Rightarrow$ 意思決定
  - **エージェント** agent が **行動**(行為) action をする
  - 行動によって **状態** が変化する
  - **環境** から与えられる **報酬** によって**目標**が決定
- 深層学習: $\Rightarrow$ 表現，表象
  - 教師信号として目標が与えられる
  - 目標を達成するために外部状況の **表現** を獲得

<center>
<font size="+2" color="Green">強化学習 + 深層学習 = 人工知能</font>
</center>

  - 強化学習 $\Rightarrow$ 目標の設定
  - 深層学習 $\Rightarrow$ 内部表象の獲得機構を提供

# 用語の整理

- 教師信号なし **報酬信号** reward signal
- 遅延フィードバック

- **価値** Value
- **行為** Action
- **状態** State
- TD 学習
  - **Sarsa**
  - **Q 学習**
  - **アクタークリティック**
- 報酬 $R_t$: **スカラ値**
  - 時刻 $t$ でエージェントのとった行動を評価する指標
  - エージェントは**累積報酬** cumulative reward の最大化する
  - 報酬仮説: **目標は累積期待報酬の最大化として記述可能**

## DQN 結果

<center>

<img src="/assets/2015Mnih_DQNFig.png" style="width:39%"><br/>
</center>


## YouTube 上でのデモ動画

* ブロック崩し: [https://www.youtube.com/watch?v=V1eYniJ0Rnk](https://www.youtube.com/watch?v=V1eYniJ0Rnk)
* スペースインベーダー: [https://www.youtube.com/watch?v=W2CAghUiofY](https://www.youtube.com/watch?v=W2CAghUiofY)

<!--- packman: [https://www.youtube.com/watch?v=r3pb-ZDEKVg](https://www.youtube.com/watch?v=r3pb-ZDEKVg)
- OpenMind selfplay: [https://www.youtube.com/watch?v=OBcjhp4KSgQ](https://www.youtube.com/watch?v=OBcjhp4KSgQ)

-->
<!--
* DQN の動画 スペースインベーダー

<center>

<video style="width:33%" controls src="/assets/2015Mnih_DQN-Nature_Video1.mp4" type="video/mp4" >
</center>

* DQN の動画 ブロック崩し

<center>

<div>
<video style="width:33%" controls src="/assets/2015Mnih_DQN-Nature_Video2.mp4" type="video/mp4" >
</div>
</center>
-->

<!-- ## なぜ DQN には難しいのか？

<center>
<div>
<video style="width:74%" controls src="/assets/Montezuma.mp4" type="video/mp4" /></br>
**Montezuma**
</div>
</center>

<center>
<video style="width:74%" controls src="/assets/PrivateEye.mp4" type="video/mp4" /></br>
**Private Eye**
</center>

<center>
<video width="39%" autoplay loop markdown="0" controls muted>
  <source src="./assets/Montezuma.mp4">
</video>
<video width="39%" autoplay loop markdown="0" controls muted>
  <source src="./assets/privateEye.mp4">
</video>
 -->

## 人間にはできて強化学習には難しいこと

- Montenzuma's Revenge の動画 [https://www.youtube.com/watch?v=Klxxg9JM5tY](https://www.youtube.com/watch?v=Klxxg9JM5tY)
- Private Eys の動画 [https://www.youtube.com/watch?v=OfyS-Wj1M78](https://www.youtube.com/watch?v=OfyS-Wj1M78)

<!---
## エージェントと環境

- At each step $t$ the agent:
  - Executes action $A_t$
  - Receives observation $O_t$
  - Receives scalar reward $R_t$
- The environment:
  - Receives action $A_t$
  - Emits observation $O_{t+1}$
  - Emits scalar reward $R_{t+1}$
- $t$ increments at env. step
-->

<!--
- **エージェント**: 学習と意思決定を行う主体
  1. **行動** action **$A_t$** を行い
  1. 環境の **観察** observation **$O_t$** を行う
  1. 環境からスカラ値の **報酬** reward **$R_t$** を受け取る
- **環境**: エージェント外部の全て
  1. エージェントから **行為** $A_t$ を受け取り
  1. エージェントに **観察** $O_{t+1}$ を与え
  1. エージェントへ **報酬** $R_{t+1}$ を与える

## エージェントの要素

- **方策** Policy
- **価値関数** Value function
- **モデル** エージェントが持つ環境の表象

## 方策 policy

- **方策** : エージェントの行為
- 決定論的方策: $a=\pi(S)$
- 確率論的方策: $\pi(a|s)=p(A_{t=a}|S_{t=s})$

## 価値関数
- 将来の報酬予測
- 状態評価(良/悪)
- 行為の選択
$$
v_\pi(S)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma R_{t+2} + \gamma^2R_{t+3}+\ldots|S_{t=s}\right\}
$$

## 強化学習のモデル
- 価値ベース
  - 方策:なし
  - 価値関数:あり
- 方策ベース
  - 方策:あり
  - 価値関数:なし
- アクター=クリティック Actor Critic
  - 方策: あり
  - 価値関数: あり

- モデルフリー
  - 方策，価値関数: あり
  - モデル: なし
- モデルベース
  - 方策，価値関数: あり
  - モデル: あり

## 探索と利用のジレンマ Exploration and exploitaion dilemma
- 過去の経験から，一番良いと思う行動ばかりをしていると，さらに良い選択肢を見つけ出すことができない **探索不足**
- 更に良い選択肢ばかり探していると過去の経験が活かせない **過去の経験の利用不足**

## 目標，収益，報酬

- エージェントの目標は累積報酬を最大化すること (報酬仮説)
  - **報酬仮説** Reward Hypothesis
  - 目標: 期待報酬の最大化

- 時刻 $t$ における報酬 $R_t$ : **スカラ値**
- 時刻 $t$ におけるエージェント行為の評価

## 逐次的意思決定 Sequential Decision Making
- 目標 Goal: 総収益を最大化する行動を選択すること
- 行為，行動 Actions は長期的結果
- 収益は遅延することも有る
- 直近の報酬を選ぶよりも，長期的な報酬を考えた方が良い場合がある


## 収益 Return
- **収益** return $G_t$: 割引付き収益
$$
G_t=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}
$$

- 割引率 The discount $\gamma\in\left\{0,1\right\}$ : 現時点から見た将来の報酬を計算するため

- **遅延報酬** delayed reward の評価
- $0$ に近ければ __近視眼的__ 評価
- $1$ に近ければ __将来を見通した__ 評価

## 価値関数 Value Function

- **状態価値関数 $v$ ** と **行動価値関数 $q$ **

- **価値関数** $v(s)$: gives the long-term value of state $s$
- **状態価値関数** $v(s)$ of an MRP is the expected return starting from state $s$
$$
v(s)=\mathbb{E}\left\{G_t|S_{t=2}\right\}
$$

- **状態価値関数** state-value function:
$$
v_\pi(s)=\mathbb{E}_\pi\left\{G_t|S_{t=s}\right\}
$$

# ベルマン期待期待 Bellman Expectation Equation
- **状態価値関数** : 即時報酬と後続状態の割引付き報酬の和に分解できる

$$
v_\pi(s)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma v_\pi(S_{t+1}|S_t=s)\right\}
$$

- **行動価値関数** action-value function:
$$
q_\pi(s,a)=\mathbb{E}_\pi\left\{G_t|S_{t}=s,A_{t}=a\right\}
$$

- **行動価値関数** 同じく分解可能 The action-value function can similarly be decomposed,
$$
q_\pi(s,a)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a\right\}
$$


## 最適価値関数 Optimal Value Function
- 最適状態価値関数
$$
v_{*}(s) = \max_{\pi} v_{\pi}(s)
$$
- 最適行動価値関数
$$
q_{*}(s,a)=\max q_{\pi}(s,a)
$$

- ベルマン方程式 一般に非線形になるので難しい
-->

<!--
- No closed form solution (in general)
- Many iterative solution methods
- 幾つかの解法:
  - 価値反復
  - 方策反復
  - Q 学習
  - sarsa
-->

<!--
  - Value Iteration
  - Policy Iteration
  - Q-learning
  - Sarsa
-->

<!--
# Markov Reward Process
A Markov reward process is a Markov chain with values.

- A Markov Reward Process is a tuple $<S,P,R,\gamma>$
  - $S$ is a finite set of states
  - $P$ is a state transition probability matrix,
  - $P_{ss'}=P\of{S_{t+1}=s'\given{S_t=s}}$
  - $R$ is a reward function, $R_s=\mathbb{E}\BRc{R_{t+1}\given{S_t=s}}$
  - $\gamma$ is a discount factor

# Bellman Equation for MRPs
The value function can be decomposed into two parts:
  - immediate reward $R_{t+1}$
  - discounted value of successor state $\gamma v\of{S_{t+1}}$

$$
\begin{array}{lll}
v\of{s}&=&\mathbb{E}\BRc{G_t\given{S_t=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma\Brc{R_{t+2}+\gamma R_{t+3}+\ldots}\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma G_{t+1}\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma v\of{S_{t+1}\given{S_{t}=s}}}
\end{array}
$$

# Bellman Equation for MRPs

$$
v\of{s}=\mathbb{E}\BRc{R_{t+1}+\gamma v\of{S_{t+1}}\given{S_t=s}}
$$


$$
v\of{s}=R_s +\gamma\sum_{s'\in S} P_{ss'}v\of{s'}
$$

# Solving the Bellman Equation
- The Bellman equation is a linear equation
- It can be solved directly:
$$
\begin{array}{lll}
v &=& R +\gamma Pv\\
\Brc{I - \gamma P}v &=& R\\
v &=& \Brc{I-\gamma P}^{-1}R
\end{array}
$$

- Computational complexity is $O(n^3)$ for $n$ states
- Direct solution only possible for small MRPs
- There are many iterative methods for large MRPs, e.g.
  - Dynamic programming
  - Monte-Carlo evaluation
  - Temporal-Difference learning
-->


<!-- # 価値関数 Value Function -->
<!-- - 将来の報酬予測の関数 -->
<!--   - ある状態である行動を起こすとどれほどの報酬が得られるか -->
<!-- - **Q-値関数** Q-value function : 総期待報酬を得る関数<\!--gives expected total reward-\-> -->
<!--   - 方策 $\pi$ のもとで -->
<!--   - 状態 $s$ で行動 $a$ を行ったとき -->
<!-- $$ -->
<!--   Q^\pi\of{s,a}=\mathbb{E}\BRc{r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots\given{s,a}} -->
<!-- $$   -->

<!--
# 最適価値関数 Optimal Value Functions
- 最大の価値を与える関数
$$
Q^*(s,a)=\max_{\pi}Q^\pi(s,a)=Q^{\pi^*}(s,a)
$$
- 最適価値関数 $Q^*$ が得られれば最適方策 $\pi^*$ を求めることができる
$$
\pi^*(s)=\operatorname{argmax}_aQ^*(s,a)
$$

- 全ての意思決定における最適価値:
$$
\begin{array}{lll}
Q^*(s,a)&=&r_{t+1}+\gamma\max_{a_{t+1}}r_{t+2}+\gamma^2\max_{a_{t+2}}r_{t+3}+\ldots\\
       &=&r_{t+1}+\gamma\max_{a_{t+1}}Q^*(s_{t+1},a_{t+1})
\end{array}
$$
-->

<!-- from sliver (2016) icml lecture -->
<!--
- **ベルマン方程式** Bellman equation:
$$
Q^*(s,a)=\mathbb{E}_{s'}\left\{r+\gamma\max_{a'}Q^*(s',a')|s,a\right\}.
$$
-->

<!--
# 報酬(収益) Rewards
- 時刻 $t$ における報酬 $R_t$ : **スカラ値**
- 時刻 $t$ におけるエージェント行為の評価Indicates how well agent is doing at step $t$

# Sequential Decision Making
- 目標 Goal: select actions to maximise total future reward
- 行為 Actions はmay have long term consequences
- 収益は遅延することも有る
- 直近の報酬を選ぶよりも，長期的な報酬を考えた方が良い場合がある It may be better to sacrifice immediate reward to gain more long-term reward-
- Examples:
  - A financial investment (may take months to mature)
  - Refuelling a helicopter (might prevent a crash in several hours)
  - Blocking opponent moves (might help winning chances many moves from now)
-->

---


# [GLUE: General Language Understanding Evaluation](https://gluebenchmark.com/leaderboard){:target="_blank"}

<div class="figure figcenter">
<img src="/2023assets/2019-08-21GLUE_leaderboard.png" width="77%">
<div class="figcaption">

[GLUE: General Language Understanding Evaluation](https://gluebenchmark.com/leaderboard){:target="_blank"}
</div></div>


### GLUE 下位課題

* CoLA: 入力文が英語として正しいか否かを判定
* SST-2: スタンフォード大による映画レビューの極性判断
* MRPC: マイクロソフトの言い換えコーパス。2 文 が等しいか否かを判定
* STS-B: ニュースの見出し文の類似度を 5 段階で評定
* QQP: 2 つの質問文の意味が等価かを判定
* MNLI: 2 入力文が意味的に含意，矛盾，中立を判定
* QNLI: Q and A
* RTE: MNLI に似た 2 つの入力文の含意を判定
* WNI: ウィノグラッド会話チャレンジ

### SOTA モデルの特徴

* RoBERTa: BERT の訓練コーパスを巨大 (173GB) にし，ミニバッチサイズを大きした
* XLNet: 順列言語モデル。2 ストリーム注意
* MT-DNN: BERT ベース の転移学習に重きをおいたモデル
* GPT-X: BERT に基づく。人間超えして 2019 年 2 月時点で炎上騒ぎ
* BERT: Transformer に基づく言語モデル。**マスク化言語モデル**  と **次文予測** に基づく **事前訓練**，各下流課題を **ファインチューニング**。
事前訓練されたモデルは一般公開済。
* ELMo: 双方向 RNN による文埋め込み表現
* Transformer: 自己注意に基づく言語モデル。多頭注意，位置符号器.

<img src="/2023assets/2019Liu_mt-dnn.png" width="66%">

<img src="/2023assets/2017Vaswani_Fig2_1ja.svg">
<img src="/2023assets/2017Vaswani_Fig2_2ja.svg">

From Vaswani+2017 transformer Fig. 2


# 事前訓練とマルチ課題学習

<center>
<img src="/assets/mt-dnn.png" width="66%"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>

<!--
# Transformer: Attention is all you need
-->

$$
\mathop{attention}\left(Q,K,V\right)=\mathop{dropout}\left(\mathop{softmax}\left(\frac{QK^\top}{\sqrt{d}
}\right)\right)V
$$

<center>
<img src="/assets/2017Vaswani_Fig2_1.svg" width="17%">
<img src="/assets/2017Vaswani_Fig2_2.svg" width="23%"><br/>
From [@2017Vaswani_transformer] Fig. 2
</center>


$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\mathop{head}_ {1},\ldots,\mathop{head}_ {h}\right)W^O
$$

where, $\text{head}_{i} = \text{Attention}\left(QW_i^Q,KW_{i}^K,VW_{i}^V\right)$

The projections are parameter matrices

- $W_i^Q\in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^K \in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$,
- $W^O\in\mathbb{R}^{hd_v\times d_{\mathop{model}}}$. $h=8$
- $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$
\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2
$$

$$
\text{PE}_{(\mathop{pos},2i)} = \sin\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

$$
\text{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

<!--
# BERT, GPT, ELMo 事前訓練の違い

- BERT:   トランスフォーマー，マスク化言語モデル，次文予測課題
- GPT:   順方向トランスフォーマー
- ELMo:  双方向 RNN による中間層の連結
-->

# 多言語対応

<center>
<img src="/assets/2019Lample_Fig1.svg" width="88%"><br/>
From [@2019Lample_Cross-lingual] Fig. 1
</center>


# BERT: ファインチューニング手続きによる性能比較

<center>
<img src="/assets/2019Devlin_mask_method21.jpg" width="66%"><br/>
マスク化言語モデルのマスク化割合の違いによる性能比較
</center>

マスク化言語モデルのマスク化割合は マスクトークン:ランダム置換:オリジナル=80:10:10 だけでなく，
他の割合で訓練した場合の 2 種類下流課題，
MNLI と NER で変化するかを下図 \ref{fig:2019devlin_mask_method21} に示した。
80:10:10 の性能が最も高いが大きな違いがあるわけではないようである。

<!-- # BERT モデルサイズ比較
<center>
<img src="./assets/2019Devlin_model_size20.jpg" style="width:69%"><br/>
</center>
 -->

# BERT: モデルサイズ比較

<center>
<img src="/assets/2019Devlin_model_size20.jpg" width="59%"><br/>
モデルのパラメータ数による性能比較
</center>

パラメータ数を増加させて大きなモデルにすれば精度向上が期待できる。
下図では，横軸にパラメータ数で MNLI は青と MRPC は赤 で描かれている。
パラメータ数増加に伴い精度向上が認められる。
図に描かれた範囲では精度が天井に達している訳ではない。パラメータ数が増加すれば精度は向上していると認められる。


# BERT: モデル単方向，双方向モデル比較

<center>
<img src="/assets/2019Devlin_directionality19.jpg" width="59%"><br/>
言語モデルの相違による性能比較
</center>

言語モデルをマスク化言語モデルか次単語予測の従来型の言語モデルによるかの相違による性能比較を
下図 \ref{fig:2019devlin_directionality19} に示した。
横軸には訓練ステップである。訓練が進むことでマスク化言語モデルとの差は 2 パーセントではあるが認められるようである。


<!-- # BERT 事前訓練比較
<center>
<img src="/assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:66%"><br/>
</center>
-->

# BERT: 事前訓練比較

<center>
<img src="/assets/2019Devlin_Effect_of_Pretraining18.jpg" width="59%"><br/>
事前訓練の効果比較
</center>

図には事前訓練の比較を示しされている。
全ての事前訓練を用いた場合が青，次文訓練を除いた場合が赤，従来型言語モデルで次文予測課題をした場合を黄，
従来型言語モデルで次文予測課題なしを緑で描かれている。4 種類の下流課題は MNLI, QNLI, MRPC, SQuAD である。
下流のファインチューニング課題ごとに精度が分かれるようである。

<!--![](2019document/2019Devlin_BERT_slides.pdf)-->
<!--8. [DistilBERT](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)-->

<!-- # under construction 従来モデルの問題点

BERT の意味，文法表現を知るために，從來モデルである word2vec の単語表現概説しておく。
各単語はワンホット onehot 表現からベクトル表現に変換するモデルを単語埋め込みモデル word embedding models あるいはベクト
ル表現モデル vector representation models と呼ぶ。
下図のように各単語を多次元ベクトルとして表現する。-->

# 単語埋め込みモデルの問題点

単語の意味が一意に定まらない場合，ベクトル表現モデルでは対処が難しい。
とりわけ多義語の意味を定めることは困難である。

下図の単語「アップル」は果物であるか，IT 企業であるかは，その単語を単独で取り出した場合一意に定める事ができない。

<div class="figcenter">

<img src="/2025assets/2019Devlin_BERT02upper.svg" wisth="74%;"><br/>
単語の意味を一意に定めることができない場合
</div>

単語の多義性解消のために，あるいは単語のベクトル表現を超えて，より大きな意味単位である，句，節，文のベクトル表現を得る努力がなされてきた。適切な普遍文表現ベクトルを得ることができれば，翻訳を含む多くの下流課題にとって有効だと考えられる。
seq2seq モデルは RNN の中間層に文情報が表現されることを利用した翻訳モデルであった

<div class="figcenter">

<img src="/2025assets/2019Devlin_BERT03.svg" wisth="74%;"><br/>
[@2014Sutskever_Sequence_to_Sequence] より
</div>

BERT は上述の從來モデルを凌駕する性能を示した。以下では BERT の詳細を見ていくこととする。

# BERT: 事前訓練とマルチ課題学習

図は事前訓練と GLUE の各課題に対応するためファインチューニングを示している。事前訓練として図中レキシコンエンコーダと表記されている部分は，単語表現，位置符号器，文情報の 3 種類の信号の合成である。合成された入力信号がトランスフォーマーへ入力され事前訓練が行なわれる。事前訓練語，各課題毎にファインチューニングが施される。

<div class="figcenter">

<img src="/2025assets/mt-dnn.png" wisth="74%;"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</div>


