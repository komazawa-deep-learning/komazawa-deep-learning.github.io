---
title: "第22回 2025年度開講 駒澤大学 人工知能"
author: "浅川 伸一"
layout: home
codemirror_mode: python
codemirror_mime_type: text/x-cython
---

<div align="right">
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 14/Nov/2025<br/>
Appache 2.0 license<br/>
</div>

<link href="/css/asamarkdown.css" rel="stylesheet">

* [課題提出用フォルダ](https://drive.google.com/drive/u/6/folders/1paq7yQ2abtWxxpZZJXgVhm9Ji53_aKqC){:target="_blank"}

## 実習

* [実習 オーバーフィッティング，アンダーフィッテング <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_Visit_polynomilal_fittings_demo.ipynb){:target="_blank"}
* [符号化器・復号化器モデル ちはやふる <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}
* [PyTorch による Transfomer 実装 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602Transformer_from_scratch.ipynb){:target="_blank"}
* [chatGPT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0608rinna_chatGPT_demo.ipynb){:target="_blank"}

* WEAVER++, Dell モデルの再現シミュレーション
  - [他言語プライミング課題での事象関連電位 （ERP) のシミュレーション Roelofs, Cortex (2016) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Roelofs_ERP_bilingual_lemret.ipynb){:target="_blank"}
  - [概念バイアス `Conceptual Bias` (Reolofs, 2016) 絵画命名，単語音読，ブロック化，マルチモーダル統合 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Roelofs_Conceptual_bias.ipynb){:target="blank"}
  - [2 ステップ相互活性化モデルデモ (Foygell and Dell, 2000) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020ccap_Foygel_Dell2000_2step_interactive_activaition_model_demo.ipynb){:target="_blank"}
  - [WEVER++ デモ 2020-1205 更新 Reolofs(2019) Anomia cueing <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020ccap_Roelofs2019_Anomia_cueing_demo.ipynb){:target="_blank"}


* [BERT の微調整 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0623BERT_SNOW_training.ipynb)
* [BERT のマルチヘッド注意の視覚化 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1007BERT_head_view.ipynb)
* [日本語 BERT 2 つの文の距離を求めるデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0624BERTja_test.ipynb)
* [sentenceBERT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602minimam_sentenceBERT.ipynb)

* [1990 年代の Stroop 効果のシミュレーション<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1110Stroop_1990Cohen_model.ipynb){:target="_blank"}
* [転移学習による Stroop 効果のデモ<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1123Stroop_model.ipynb)
* [chatGPT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0608rinna_chatGPT_demo.ipynb)
* [Stable-baselines3 を用いた PPO デモ Atari Lunalander 月面着陸 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0619stable_baselines3_demo_LunaLander_V2.ipynb)
* [画像認識における注意 CAM <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2021_0618CAM_demo.ipynb)
* [Stable diffusion デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0707stable_diffusion.ipynb)
* [リカレントニューラルネットワークによる文処理デモ 青空文庫より，夏目漱石 こころ](https://komazawa-deep-learning.github.io/character_demo.html)
* [CartoonGAN 実習<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0628CartoonGAN_demo.ipynb)
* [加算型注意 (Bahdanu) と 内積型注意 (Loung) の実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb)

* Huggingface RTL
  * [TRL - Transformer Reinforcement Learning](https://github.com/huggingface/trl/tree/main)


* PyTorch 関連

  * [Pytorch によるニューラルネットワークの構築 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1115PyTorch_buildmodel_tutorial_ja.ipynb){:target="_blank"}
  * [Dataset とカスタマイズと，モデルのチェックポイント，微調整 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0824pytorch_simple_fine_tune_tutorial.ipynb){:target="_blank"}
  * [PyTorch Dataset, DataLoader, Sampler, Transforms の使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0824pytorch_dataset_data_loader_sampler.ipynb){:target="_blank"}

<!-- ### tSNE

* [kmnist による PCA と tSNE の比較 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazaawa_kmnist_pca_tsne.ipynb)
* [効率よく t-SNE を使う方法](https://project-ccap.github.io/misread-tsne/) -->


# [Transformer とは何か](https://poloclub.github.io/transformer-explainer/)

Transformer は人工知能へのアプローチを根本的に変えたニューラルネットワークのアーキテクチャである。Transformer は、画期的な論文 [Attention is All You Need](https://dl.acm.org/doi/10.5555/3295222.3295349 "ACM Digital Library") で初めて導入され、それ以来、深層学習モデルの定番アーキテクチャとなった。OpenAI の **GPT**、Meta の **Llama**、Google の **Gemini** といったテキスト生成モデルを支えている。テキスト以外にも、Transformer は[音声合成](https://huggingface.co/learn/audio-course/en/chapter3/introduction "Hugging Face")にも応用されている。[画像認識](https://huggingface.co/learn/computer-vision-course/unit3/vision-transformers/vision-transformers-for-image-classification "Hugging Face")、[タンパク質構造予測](https://elifesciences.org/articles/82819 "eLife")、さらには[ゲームプレイ](https://www.deeplearning.ai/the-batch/reinforcement-learning-plus-transformers-equals-efficiency/ "Deep Learning AI")まで、様々な分野でその汎用性を示している。

基本的に、テキスト生成型 Transformer モデルは**次単語予測**原理で動作する。ユーザからのテキストプロンプトが与えられた時、この入力に続く*最も確率の高い次単語*は何か？
Transformer の中核的な革新性と強みは、自己注意機構の活用にある。これにより、従来のアーキテクチャよりも効果的に系列全体を処理し、長距離依存関係を捕捉できるのだ。

GPT-2 ファミリーのモデルは、テキスト生成型 Transformer の代表例である。Transformer Explainer は、1 億 2400 万のパラメータを持つ [GPT-2](https://huggingface.co/openai-community/gpt2)(small) モデルを基盤としている。最新かつ最強の Transformer モデルではないが、現在の最先端モデルと多くの共通したアーキテクチャ構成要素や原理を備えているため、基礎を理解する理想的な出発点となる。



最低限のニューラルネットワーク

<!-- <div style="background-color:#f7f6f1;width:77%"> -->

```python
import numpy as np                                 # numpy ライブラリの輸入<br/>
np.set_printoptions(precision=2)                   # 表示桁数の設定<br/>
np.random.seed(42)                                 # 乱数系列の初期化<br/>
X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])<br/>
y = np.array([[0,1,1,0]]).T<br/>
w0 = 2 * np.random.random((3,4)) - 1<br/>
w1 = 2 * np.random.random((4,1)) - 1<br/>
for i in range(300):<br/>
　　l1 = np.tanh(np.dot(X,w0))                     # tanh<br/>
　　#l1 = 1/(1+np.exp(-(np.dot(X,w0))))            # sigmoid<br/>
　　l2 = 1/(1+np.exp(-(np.dot(l1,w1))))<br/>
　　dl2 = (y - l2) * (l2 * (1 - l2))<br/>
　　dl1 = dl2.dot(w1.T) * (1 - l1 ** 2)            # tanh<br/>
　　#dl1 = dl2.dot(w1.T) * (11 * (1-l1))           # sigmoid<br/>
　　w1 += np.dot(l1.T, dl2)<br/>
　　w0 += np.dot(X.T, dl1)<br/>
　　print(l2.T) if i % 100 == 0 else None<br/>
```
<!-- </div> -->



## 微調整 (fine-tunig) 以外の課題調整法

<div class="figure figcenter">
<img src="/2023assets/2022Quyang_instructGPT_fig2ja.svg" width="99%">
<div class="figcaption">

instructGPT の概要 [2022Quyang+](https://arxiv.org/abs/2203.02155) Fig.2 を改変

</div></div>


# Transformer

* Transformer: マルチヘッド注意機構，位置符号化器，埋め込み表現，ソフトマックス関数
* 事前訓練とファインチューニング: マスク言語モデル，次文予測  <!--Masked Language model, next sentence prediction--->
* GTP-4: 符号化器・復号化器モデルを用いた画像と言語との融合
* プロンプトエンジニアリングと RLHF (人間のフィードバックによる強化学習): 報酬モデルと代理方針最適化 (Proximal Policy Optimization)

<div class="figcenter">
<img src="/assets/2017Vaswani_Fig2_1.svg" width="09%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/2017Vaswani_Fig2_2.svg" width="19%">&nbsp;&nbsp;&nbsp;
<img src="/assets/2017Vaswani_Fig1.svg" width="29%">
<div class="figcaption">

Transformer ([2017Vaswani++](https://arxiv.org/abs/1706.03762)) Fig.2 を改変
</div></div>



上図で，`matmul` は行列の積，`scale` は，平均 0 分散 1 への標準化，`mask` は 0 と 1 とで，データを制限すること，`softmax` はソフトマックス関数である。

トランスフォーマーの注意とは，このソフトマックス関数である。

<!-- <div class="figure figcenter">
<img src="figures/2017Vaswani_Fig2_1.svg" width="19%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="figures/2017Vaswani_Fig2_2.svg" width="29%">&nbsp;&nbsp;&nbsp;
<img src="figures/2017Vaswani_Fig1.svg" width="39%"> -->

<!-- # Transformer(2): Attention is all you need -->


$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\text{head}_1,\ldots,\text{head}_{h}\right)W^O
$$
where, $\text{head}_i =\text{Attention}\left(QW_i^Q,KW_i^K,VW_i^V\right)$

The projections are parameter matrices $W_i^Q\in\mathbb{R}^{d_{\text{model}}\times d_k}$, $W_i^K \in\mathbb{R}^{d_{\text{model}}\times d_k}$,
$W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$, and $W^O\in\mathbb{R} ^{hd_v\times d_{\mathop{model}}}$.
$h=8$, $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$
\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2
$$

$$
\text{PE}_{(\text{pos},2i)} = \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

$$
\mathop{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

<!-- # Transformer(3): Attention is all you need -->

<div class="figcenter">
<img src="/assets/2017Vaswani_Fig1.svg">
<div class="figcaption" style="width:33%">
[Vaswani+2017](https://arxiv.org/abs/1706.03762) Fig. 1 より
</div></div>

<!-- # Sejnowski2022

<div class="figcenter">
<img src="figures/2022Sejnowski_fig5.jpg" width="66%">
<div class="figcaption">

トランスフォーマーのループと皮質-基底核のループの比較。
(左) トランスフォーマーは，出力を入力とループさせて単語系列を生成するフィードフォワード自己回帰型アーキテクチャを持つ (Vaswani+2017)。
示された単一のエンコーダ/デコーダモジュールは N 層 深く (Nx) 積み重ねることができる。
(右) 位相的に写像された運動皮質は大脳基底核に投射し，大脳皮質にループバックして，話し言葉の単語列のような一連の動作を生成する。
大脳皮質のすべての部分が大脳基底核に投影され，前頭前野と大脳基底核の間の同様のループによって，思考の系列が生み出される。

thalamus: 視床，
putamen: 被殻
SMA: supplementary motor area 補足運動野
PM: premotor cortex 運動前野
STN: subthalamic nucleus 視床下核
GPi: internal segment of globus pallidus 淡蒼球内節
GP: globus pallidus     淡蒼球
M1: 一次運動野
</div></div>
-->

## 1 位置符号器 Position encoders

 Transformer の入力には，上述の単語表現に加えて，位置符号器からの信号も重ね合わされる。
位置 $i$ の信号は次式で周波数領域へと変換される:

$$
\text{PE}_{(\text{pos},2i)} = \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\\
\text{PE}_{(\text{pos},2i+1)} = \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$

位置符号器による位置表現は，$i$ 番目の位置情報をワンホット表現するのではなく，
周波数領域に変換することで周期情報を表現する試みと見なし得るだろう。

<div class="figcenter">
<img src="/assets/PE_example.svg" width="66%">
<div class="figcaption">
位置符号化に用いられる符号化
</div></div>

# BERT

## BERT の入力表現

<div class="figure figcenter">
<img src="/assets/2018Devlin_BERT_Fig2.svg" width="49%">
<div class="figcaption">
埋め込みトークンの総和，位置符号器，分離埋め込みの 3 者
</div></div>

<!-- <div class="figure figcenter">
<img src="figures/2018Devlin_BERT_Fig2.svg">
</div> -->

## BERT の事前訓練: マスク化言語モデル

全入力系列のうち 15% をランダムに [MASK] トークンで置き換える

* 入力はオリジナル系列を [MASK] トークンで置き換えた系列
* ラベル: オリジナル系列の [MASK] 部分にの正しいラベルを予測

%Rather than always replacing the chosen words with [MASK], the date generator will do the following:

* 80%: オリジナル入力系列を [MASK] で置換
y $\rightarrow$ my dog is  [MASK].
* 10%: [MASK] の位置の単語をランダムな無関連語で置き換える
my dog is hairy $\rightarrow$ my dog is apple
* 10%: オリジナル系列

## BERT の事前訓練: 次文予測課題

言語モデルの欠点を補完する目的，次の文を予測

[SEP] トークンで区切られた 2 文入力
* 入力: the man went to the store [SEP] he bought a gallon of milk.
* ラベル: IsNext
* 入力: the man went to the store [SEP] penguins are flightless  birds.
* ラベル: NotNext

### BERT のファインチューニング

<div class="figcenter">
<img src="/2023assets/2019Liu_mt-dnn.png" width="66%">
<div class="figcaption">

[Liu+2019](https://arxiv.org/abs/1901.11504) Fig. 1
</div></div>

<!-- <div class="figcenter">
<img src="figures/2017Vaswani_Fig2_1ja.svg" width="22%">
<img src="figures/2017Vaswani_Fig2_2ja.svg" width="22%">
<div class="figcaption">
From Vaswani+2017 transformer Fig. 2
</div></div> -->

<!-- # BERT: ファインチューニング (1) -->

<div class="figcenter">
<img src="/assets/2018Devlin_BERT_Fig3.svg">
<div class="figcaption">

(a), (b) は文レベル課題，(c),(d)はトークンレベル課題, E: 入力埋め込み表現,
$T_i$: トークン $i$ の文脈表象。[CLS]: 分類出力記号, [SEP]:文分離記号
</div></div>

# GPT-4
加えて，chatGPT の後続モデルである GPT-4 では，マルチモーダル，すなわち，視覚と言語の統合が進んだ。

<div class="figcenter">
<img src="/2023assets/2023kosmos_coverpage.png" width="49%">
<div class="figcaption">

[Kosmos-1 の概念図](https://arXiv.org/abs/2302.14045)
</div></div>

<!-- まず第一に，大規模ではない，言語モデルについて考えます。
言語モデルは，機械翻訳などでも使われる技術です。
ですから，DeepL や Google 翻訳で，使っている方もいることでしょう。

chatGPT を使える方は，上記太字のキーワードについて，chatGPT に質問してみることをお勧めします。
とりわけ 注意 については，認知，視覚，心理学との関連も深く，注意の障害は，臨床，教育，発達などの分野と関係するでしょう。 -->


# BERT: 埋め込みモデルによる構文解析

BERT の構文解析能力を下図示した。
各単語の共通空間に射影し，単語間の距離を計算することにより構文解析木と同等の表現を得ることができることが報告されている [@2019HewittManning_structural]。

<div class="figure figcenter">
<img src="/assets/2019hewitt-header.jpg" width="44%">
<img src="/assets/2019HewittManning_blogFig1.jpg" width="22%">
<img src="/assets/2019HewittManning_blogFig2.jpg" width="22%">
<div class="figcaption">
BERT による構文解析木を再現する射影空間
From `https://github.com/john-hewitt/structural-probes``
</div></div>

word2vec において単語間の距離は内積で定義されていた。
このことから，文章を構成する単語で張られる線形内積空間内の距離が構文解析木を与えると見なすことは不自然ではない。

そこで構文解析木を再現するような射影変換を見つけることができれば BERT を用いて構文解析が可能となる。
例えば上図における chef と store と was の距離を解析木を反映するような空間を見つけ出すことに相当する。
2 つの単語 $w_i$, $w_j$ とし単語間の距離を $d\left(w_i,w_j\right)$ とする。 適当な変換を施した後の座標を $h_i$, $h_j$ とすれば，求める変換 $B$ は次式のような変換を行なうことに相当する:

$$
\min_{B}\sum_l\frac{1}{\left|s_\ell\right|^2}\sum_{i,j}\left(d\left(w_i,w_j\right)-\left\|B\left(h_i-h_j\right)\right\|^2\right)
$$

ここで $\ell$ は文 s の訓練文のインデックスであり，各文の長さで規格化することを意味している。

# StableDiffusion と LangChain

<div class="figure figcenter">
<img src="/2023assets/2022patrickvonplaten_scientific_images_stable_diffusion.png" width="34%">
<img src="/2023assets/2023Polzer_LLM_app_ja_fig1.webp" width="44%">
<div class="figcaption">
左: `https://github.com/patrickvonplaten/scientific_images/tree/master`，
右: [Polzer2023](https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac) より
</div></div>

<!-- * [StableDiffusion <img src="figures/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0714stable_diffusion.ipynb) -->


## [チューリングテストと知能の概念の変遷](https://www.science.org/doi/10.1126/science.adq9356)<!--The Turing Test and our shifting conceptions of intelligence-->

「機械は考えることができるか？」と、アラン・チューリングは 1950 年の論文「計算機械と知能」の中で問いかけた。チューリングはすぐに、思考を定義することの難しさを考えると、この問いは「議論に値しないほど無意味だ」と指摘しました。哲学的な議論でよくあるように、彼はこの問いを別の問いに置き換えることを提案しました。チューリングは「模倣ゲーム」を構想した。これは、人間の審査員がコンピュータと人間（「フォイル」）の両方と会話し、それぞれが審査員に自分が人間であることを納得させようと競い合うゲームである。重要なのは、コンピュータ、フォイル、そして審査員は互いに顔を合わせず、完全にテキストでコミュニケーションをとることである。各候補者と会話した後、審査員は誰が本物の人間かを推測する。チューリングの新たな問いは、「模倣ゲームで優れた成績を収めるデジタルコンピュータは想像できるか？」であった。
<!-- “Can machines think?” So asked Alan Turing in his 1950 paper, “Computing Machinery and Intelligence.” Turing quickly noted that, given the difficulty of defining thinking, the question is “too meaningless to deserve discussion.” As is often done in philosophical debates, he proposed replacing it with a different question. Turing imagined an “imitation game,” in which a human judge converses with both a computer and a human (a “foil”), each of which vies to convince the judge that they are the human. Importantly, the computer, foil, and judge do not see one another; they communicate entirely through text. After conversing with each candidate, the judge guesses which one is the real human. Turing’s new question was, “Are there imaginable digital computers which would do well in the imitation game?” -->

現在チューリングテストとして知られるこのゲームは、コンピューターは機械的な性質ゆえに、原理的にさえ思考できないという広く信じられている直感に対抗するために、チューリングによって提案された。チューリングの主張は、コンピュータが（外見やその他の身体的特徴を除けば）人間と区別がつかないのであれば、なぜ思考する存在と見なすべきではないのか、というものであった。なぜ「思考する」というステータスを人間（あるいはより一般的には、生物細胞で構成された存在）だけに限定すべきなのだろうか？コンピュータ科学者の Scott Aaronson スコット・アーロンソンが述べたように、チューリングの提案は「肉食至上主義への嘆願」です。
<!-- This game, now known as the Turing Test, was proposed by Turing to combat the widespread intuition that computers, by virtue of their mechanical nature, cannot think, even in principle. Turing’s point was that if a computer seems indistinguishable from a human (aside from its appearance and other physical characteristics), why shouldn’t we consider it to be a thinking entity? Why should we restrict “thinking” status only to humans (or more generally, entities made of biological cells)? As the computer scientist Scott Aaronson described it, Turing’s proposal is “a plea against meat chauvinism.” -->

チューリングは、機械の知能を測る実用的な方法としてではなく、哲学的な思考実験としてこのテストを提示した。しかしながら、チューリングテストは、人工知能（AI）の究極のマイルストーン、つまり汎用機械知能の到来を判断するための主要な指標として、人々の心に象徴的な地位を獲得した。そして今、75 年近くが経ち、AI に関する報道は、OpenAI の ChatGPT や Anthropic の Claude といったチャットボットがついにチューリングテストに合格したという宣言で溢れている。昨年、OpenAI の CEO Sam Altman サム・アルトマンは、「技術革新に直面した人々の回復力と適応力にとって良い兆候だ。チューリングテストはあっさりと通過し、人々はほぼ日常生活を送っていた」と投稿した。様々なメディアの見出しも同様の主張をしており、例えばある新聞は「ChatGPT が有名な『チューリングテスト』に合格。この AI ボットは人間と同等の知能を持っていることを示唆している」と報じた。
<!--Turing offered his test as a philosophical thought experiment, not as a practical way to gauge a machine’s intelligence. However, the Turing Test has taken on iconic status in the public’s mind as the ultimate milestone of artificial intelligence (AI)—the chief metric to determine if general machine intelligence has arrived. And now, nearly 75 years later, the reporting on AI is full of pronouncements that the Turing Test has finally been passed by chatbots such as OpenAI’s ChatGPT and Anthropic’s Claude. Last year, OpenAI’s CEO Sam Altman posted, “[G]ood sign for the resilience and adaptability of people in the face of technological change: the [T]uring test went whooshing by and everyone mostly went about their lives.” Various media headlines have made similar claims, such as one newspaper’s report that “ChatGPT passes the famous ‘Turing test’—suggesting the AI bot has intelligence equivalent to a human.” -->

現代のチャットボットは実際にチューリングテストに合格したのだろうか？もし合格したとしたら、チューリングが提唱したように、思考状態を与えるべきなのだろうか？驚くべきことに、チューリングテストの広範な文化的重要性を考えると、AI コミュニティでは合格基準についてほとんど合意が得られておらず、人間を騙せる会話スキルを持つことが、システムの根底にある知性、つまり「思考状態」について何かを明らかにできるかどうかについても、多くの疑問が持たれている。
<!-- Have modern chatbots actually passed the Turing Test? And if so, should we grant them thinking status, as Turing proposed? Surprisingly, given the Turing Test’s broad cultural importance, there’s little agreement in the AI community on the criteria for passing, and much doubt about whether having conversational skills that can fool a human reveals anything about a system’s underlying intelligence or “thinking status.”-->

チューリングは実践的なテストを提案していなかったため、模倣ゲームに関する記述には詳細な点が欠けていた。テストはどれくらいの時間続くべきか？どのような種類の質問が許されるのか？審査員や対戦相手として行動するために人間にはどのような資質が必要なのか？チューリングはそのような細かい点については言及しなかった。彼は一つだけ具体的な予測を立てた。「約 50 年後には、コンピュータをプログラムして模倣ゲームを非常に上手にプレイさせ、平均的な尋問者が5分間の質問で正しく識別できる確率が 70% 以下になるようにするだろう」つまり、5 分間の会話で、平均的な審査員は 30% の確率で騙されるということである。
<!-- Because he was not proposing a practical test, Turing’s description of the imitation game was short on details. How long should the test last? What types of questions are allowed? What qualifications do humans need to act as the judge or the foil? Turing didn’t specify such fine points. He did make one specific prediction: “I believe that in about 50 years’ time it will be possible to programme computers...to make them play the imitation game so well that an average interrogator will not have more than 70 per cent chance of making the right identification after five minutes of questioning.” In short, in a five-minute conversation, the average judge will be fooled 30% of the time. -->

このさりげない予測を、チューリングテスト合格の「公式」基準と捉える人もいる。2014 年、ロンドン王立協会は、5 つのコンピュータプログラム、30 人の人間、そして 30 人の審査員によるチューリングテスト大会を開催した。参加者は、老若男女、英語ネイティブと非ネイティブ、コンピュータの専門家とそうでない人など、多様な人々で構成されていた。各審査員は、2 人 1 組の参加者（人間と機械）と並行して 5 分間の会話を複数ラウンド行い、会話終了後、どちらが人間かを推測する必要があった。ウクライナの 10 代の若者を装った「ユージン・グーストマン」という名のチャットボットが、審査員の 10 人（33.3%）を騙して優勝した。主催者は、「5 分後には 30％ の確率で騙される」という基準を採用し、「65 年前に登場した象徴的なチューリングテストが、コンピュータプログラム、ユージン・グーストマンによって初めて合格した。この画期的な出来事は歴史に残るだろう。」と宣言した。
<!--Some have taken this casual prediction as the “official” criterion for passing the Turing Test. In 2014, the Royal Society in London hosted a Turing Test competition with five computer programs, 30 human foils, and 30 judges. The human participants were a diverse group of young and old, native and non-native English speakers, and computer experts and non-experts. Each judge conducted several rounds of five-minute conversations in parallel with a pair of contestants—one human and one machine—after which the judge had to guess which was human. A chatbot named “Eugene Goostman,” which purported to be a Ukrainian teenager, won the competition by fooling 10 (33.3%) of the judges. Adopting the “30% chance of fooling after five minutes” criterion, the organizers proclaimed, “[t]he 65-year-old iconic Turing Test was passed for the very first time by computer programme Eugene Goostman....This milestone will go down in history...” -->

AI 専門家たちは、ユージン・グーストマンの会話記録を読み、この単純で人間離れしたチャットボットがチューリングが想定していたようなテストに合格したという主張を嘲笑した。限られた会話時間と審査員の専門知識の不均一さから、このテストは機械の知能というよりも、人間の騙されやすさを測るものだった。結果は「Eliza 効果」の顕著な例だった。Eliza 効果とは、1960 年代に登場したチャットボット Eliza にちなんで名付けられた。Eliza は、極めて単純な動作にもかかわらず、多くの人を理解力があり共感力のある心理療法士だと錯覚させ、会話が可能な相手には知性があると見なす人間の性向を巧みに利用した。
<!-- AI experts, reading the transcript of Eugene Goostman’s conversations, scoffed at the claim that this unsophisticated and unhumanlike chatbot had passed the kind of test Turing had in mind. The limited conversation time and uneven expertise of judges made the test one of human gullibility rather than machine intelligence. The results were a stark example of the “ELIZA effect,” named for the 1960s chatbot ELIZA that, in spite of its utter simplicity, managed to fool many people into thinking it was an understanding, sympathetic psychotherapist, playing on our human tendency to ascribe intelligence to any entity that seems able to converse with us.-->

別のチューリングテストコンテストであるローブナー賞では、会話時間がより長く、より多くの専門家審査員が参加し、参加者は審査員の少なくとも半数を騙すことが求められた。30 年近くにわたり毎年開催されてきたコンテストで、このバージョンのテストに合格した機械は一つもなかった。
<!-- Another Turing Test competition, the Loebner Prize, allowed more conversation time, included more expert judges, and required a contestant to fool at least half of them. In nearly 30 years of annual competitions, no machine passed this version of the test. -->

チューリングの原論文にはテストの実施方法に関する具体的な記述はなかったものの、模倣ゲームには 3 人の参加者、すなわちコンピュータ、人間の引き立て役、そして人間の審査員が必要であることは明らかであった。しかしながら、「チューリングテスト」という言葉は、長年にわたる公共の場での議論の中で、より弱い意味を持つものへと変化してきた。それは、コンピュータが十分に人間らしく見える、人間とコンピュータ間のあらゆるやり取りを指すようになった。
<!-- Although Turing’s original paper lacked specifics on how a test should be carried out, it was clear that the imitation game required three participants: a computer, a human foil, and a human judge. However, the meaning of the term “Turing Test” in public discourse has evolved over the years into something considerably weaker: any interaction between a human and a computer in which the computer seems sufficiently humanlike. -->

例えば、ワシントン・ポスト紙が 2022 年に「Google の AI が有名なテストに合格し、そのテストの破綻を証明した」と報じた際、彼らが言及していたのは模倣ゲームではなく、Google のエンジニアであるブレイク・ルモワン が Google の LaMDA チャットボットに「知覚力」があるという印象を抱いたことだったのです。スタンフォード大学は 2024 年にプレスリリースを発表し、スタンフォード大学の研究チームが行った研究は「人工知能が厳格なチューリングテストに合格した初の事例の一つとなる」と宣言しました。しかし、ここでのいわゆるチューリングテストは、GPT-4 の心理調査やインタラクティブゲームにおける行動を人間のそれと比較するという統計的な比較から構成されていた。スタンフォード大学のチームの定式化は、チューリングには理解できなかったかもしれない。「AI の応答が、ランダムに選択された人間の応答と統計的に区別できない場合、AI はチューリングテストに合格したと判断する。」
<!--For example, when the Washington Post reported in 2022 that “Google’s AI passed a famous test—and showed how the test is broken,” they were referring not to an imitation game, but rather to Google engineer Blake Lemoine’s impression that Google’s LaMDA chatbot was “sentient.” A 2024 press release from Stanford University proclaimed that a Stanford team’s research “marks one of the first times an artificial intelligence source has passed a rigorous Turing test.” But here, the so-called Turing Test consisted of comparing statistics of how GPT-4’s behavior on psychological surveys and interactive games compared with those of humans. The Stanford team’s formulation might not be recognizable to Turing: “We say an AI passes the Turing test if its responses cannot be statistically distinguished from randomly selected human responses.” -->


チャットボットがチューリングテストに合格したという最近の主張は、テストの「2 人用形式」を使用した 2024 年の研究に関連している。チューリングの「3 人用」模倣ゲームとは異なり、審査員がコンピュータと人間の対戦相手の両方に質問するのとは異なり、この研究では各審査員はコンピュータまたは人間とのみ対話した。研究者らは 500 人の人間の参加者を募集し、それぞれが審査員または人間の対戦相手に割り当てられた。各審査員は、対戦相手、GPT-4（審査員をだます方法について人間が書いた提案が提示されていた）、または ELIZA チャットボットのバージョンのいずれかを使用して、5 分間のゲームを 1 ラウンドプレイした。ウェブインターフェースで 5 分間会話した後、審査員は会話相手が人間か機械かを推測した。人間の対戦相手はラウンドの 67％ で人間と判断され、GPT-4 はラウンドの 54％ で人間と判断され、ELIZA はラウンドの 22％ で人間と判断された。著者らは「合格」を、審査員を 50% 以上の確率で騙すこと、つまりランダムな推測で達成できる以上の確率で騙すことと定義した。この定義によれば、人間の対戦相手の方が高いスコアを持っていたにもかかわらず、GPT-4 は合格した。
<!-- The most recent claims of a chatbot passing the Turing Test involved a 2024 study that used a “two-player formulation” of the test: Unlike Turing’s “three-player” imitation game, in which a judge questions both a computer and a human foil, here each judge interacted only with a computer or with a human. The researchers recruited 500 human participants, each of whom was assigned to be either a judge or a human foil. Each judge played a single five-minute round of the game with either a foil, GPT-4 (which had been prompted with human-written suggestions on how to fool a judge), or a version of the ELIZA chatbot. After conversing over a web interface for five minutes, the judge guessed whether their conversation partner was human or machine. Human foils were judged to be human on 67% of their rounds; GPT-4 was judged human on 54% of its rounds, and ELIZA was judged human on 22% of its rounds. The authors defined “passing” as fooling judges more than 50% of the time—that is, more than would be achieved by random guessing. By this definition, GPT-4 passed, even though the human foils had a higher score.-->

5 分間の会話で人間の審査員の大多数がGPT-4に騙されたことは確かに懸念すべきことである。生成 AI システムを用いて人間になりすまし、偽情報を拡散したり詐欺を実行したりすることは、社会が真剣に取り組まなければならない真の危険です。しかし、今日のチャットボットはチューリングテストに合格したというのは本当だろうか？
<!-- It’s certainly concerning that a majority of the human judges were fooled by GPT-4 after a five-minute conversation. The use of generative AI systems to impersonate humans to propagate disinformation or carry out scams is a genuine danger that society must grapple with. But is it true that today’s chatbots have passed the Turing Test? -->

もちろん、どのバージョンのテストについて話しているのかによって異なる。専門家の審査員とより長い会話時間を伴う 3 人対戦の模倣ゲームは、未だにどの機械も合格していない（ただし、2029 年には超厳密なバージョンを実施する計画がある）。
<!--The answer is, of course, it depends on which version of the test you’re talking about. A three-player imitation game with expert judges and longer conversation time has still not been passed by any machine (though there are plans to hold an ultra-strict version of it in 2029). -->

チューリングテストは、知能をより直接的にテストするのではなく、人間を騙すことに重点を置いているため、多くの AI 研究者は長らく、このテストを「AI が合格するためのテストではなく、人間が不合格になるためのテスト」であり、邪魔なものだとして軽視してきた。しかし、このテストは大衆文化において依然として重要な位置を占めている。会話をすることは、私たち一人ひとりが他の人間を評価する上で大きな役割を果たす。そのため、流暢に会話できるエージェントは、人間のような知能に加え、信念、欲求、自己意識といった精神的特性を備えているに違いないと考えるのは当然である。
<!-- Because its focus is on fooling humans rather than on more directly testing intelligence, many AI researchers have long dismissed the Turing Test as a distraction, a test “not for AI to pass, but for humans to fail.” But the test’s prominence in popular culture persists. Holding a conversation is a big part of how each of us assesses other humans, so it’s natural to assume that an agent that can converse fluently must possess humanlike intelligence and other mental characteristics such as beliefs, desires, and a sense of self.-->

しかし、AI の歴史が私たちに教えてくれたことがあるとすれば、それは、そのような仮定に関する私たちの直感はしばしば間違っているということである。数十年前、多くの著名な AI 専門家は、チェスで人間に勝てる機械を作るには、完全な人間の知能と同等のものが必要だと考えていた。「もし優れたチェスマシンを開発できれば、人間の知的営みの核心に迫ったと言えるだろう」と、AI の先駆者であるアレン・ニューウェルとハーバート・サイモンは 1958 年に記し、認知科学者のダグラス・ホフスタッターは 1979 年に「将来、チェスで誰にも勝てるプログラムが存在するかもしれないが、それは汎用知能のプログラムとなるだろう」と予測した。もちろん、その後 20 年以内に IBM の DeepBlue は、我々が「汎用知能」と呼ぶものからは程遠い、力ずくのアプローチを用いて、チェスの世界チャンピオン、ガルリ・カスパロフを破った。同様に、AI の進歩は、かつて汎用知能が必要と考えられていた課題、つまり音声認識、自然言語翻訳、さらには運転さえも、人間の理解力に全く欠ける機械によって実行可能であることを示している。
<!-- However, if the history of AI has taught us anything, it’s that our intuitions are often wrong about such assumptions. Decades ago, many prominent AI experts believed that creating a machine that could beat humans at chess would require something equivalent to full human intelligence. “If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor,” wrote AI pioneers Allen Newell and Herbert Simon in 1958, and cognitive scientist Douglas Hofstadter predicted in 1979 that in the future, “there may be programs which can beat anyone at chess, but...they will be programs of general intelligence.” Of course within the next two decades IBM’s DeepBlue beat world chess champion Garry Kasparov using a brute-force approach that is far from anything we would call “general intelligence.” Similarly, progress in AI has shown that tasks that were once thought to require general intelligence—speech recognition, natural language translation, and even driving—can be carried out by machines that lack anything like human understanding. -->

チューリングテストは、我々の知能の概念が変化していく中で、またしても犠牲になる可能性が高い。1950 年、チューリングは人間のような会話能力は「思考」とそれに付随するすべてのものの確固たる証拠となるはずだと直感した。この直感は今日でも揺るぎないものである。しかし、ELIZA とユージン・グーストマンから私たちが学んだこと、そして ChatGPT やその類似の技術から今後も学ぶであろうことは、チェスをするように自然言語で流暢に話す能力は、汎用知能の決定的な証拠ではないということである。
<!--It’s likely that the Turing Test will become yet another casualty of our shifting conceptions of intelligence. In 1950, Turing intuited that the ability for humanlike conversation should be firm evidence of “thinking,” and all that goes with it. That intuition is still strong today. But perhaps what we have learned from ELIZA and Eugene Goostman, and what we may still learn from ChatGPT and its ilk, is that the ability to sound fluent in natural language, like playing chess, is not conclusive proof of general intelligence. -->

実際、神経科学の研究では、言語の流暢さが認知の他の側面と驚くほど切り離されているという証拠が出始めている。MIT の神経科学者、エヴ・フェドレンコとその共同研究者たちは、一連の慎重かつ説得力のある実験を通して、彼らが「形式言語能力」と呼ぶもの、つまり言語産出に関連する能力の基盤となる脳ネットワークは、常識、推論、そして我々が「思考」と呼ぶ他の側面の基盤となるネットワークとは大きく切り離されていることを示した。流暢な言語が汎用知能の十分条件であるという私たちの直感的な仮定は、「誤り」だと、これらの研究者たちは主張している。
<!-- Indeed, there is emerging evidence from neuroscience that language fluency is surprisingly disassociated from other aspects of cognition. MIT neuroscientist Ev Fedorenko and her collaborators have shown in a series of careful and compelling experiments that the brain networks underlying what they call “formal linguistic competence”—the abilities related to language production—are largely separate from the networks underlying common sense, reasoning, and other aspects of what we might call “thinking.” Our intuitive assumption that fluent language is a sufficient condition for general intelligence is, these researchers claim, a “fallacy.”-->

チューリングは 1950 年の論文で、「今世紀末には言葉の使い方と一般的な教養ある意見が大きく変化し、機械が思考していると語っても反論されることを恐れる必要がなくなるだろうと私は信じている」と記している。しかし、我々はまだその段階には至っていない。チューリングの予測が数十年ほど外れているだけなのか、それとも「思考」とは何かという概念が本当に変わるのか、そして知性というものはチューリングや我々が考えていたよりももっと複雑で微妙なものだという認識が変わるのかはまだ分からない。
<!--In his 1950 paper, Turing wrote, “I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.” We’re not there yet. It remains to be seen if Turing’s prediction is merely off by a few decades, or if the real alteration will be in our conceptions of what “thinking” is—and our realization that intelligence is more complex and subtle than Turing, and the rest of us, had appreciated. -->


## [Retrieval-augmented generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)

検索拡張生成（RAG）は、大規模言語モデル（LLM）が新しい情報を検索して組み込むことを可能にする技術である[1]。RAG では、LLM は指定された文書セットを参照するまで、ユーザーのクエリに応答しない。これらの文書は、LLM の既存の訓練データからの情報を補完する[2]。これにより、LLM は訓練データでは利用できないドメイン固有の情報や更新された情報を使用することができる[2]。例えば、これは LLM ベースのチャットボットが社内データにアクセスしたり、信頼できる情報源に基づいて応答を生成したりするのに役立つ。
<!-- Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information.[1] With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data.[2] This allows LLMs to use domain-specific and/or updated information that is not available in the training data.[2] For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources. -->

RAG は、応答を生成する前に情報検索を組み込むことで、大規模言語モデル（LLM）を改善する[3]。静的な訓練データに依存する LLM とは異なり、RAG はデータベース、アップロードされたドキュメント、または Web ソースから関連するテキストを取得する[1]。Ars Technica によると、「RAG は、本質的に LLM プロセスを Web 検索またはその他のドキュメント検索プロセスと融合させることで LLM の性能を向上させる方法であり、LLM が事実に忠実になるようにする。」この方法は、AI の幻覚を軽減するのに役立つ[3]。チャットボットが存在しないポリシーを説明したり、議論を裏付ける引用を探している弁護士に存在しない訴訟事例を推奨したりする原因となった[4]。
<!-- RAG improves large language models (LLMs) by incorporating information retrieval before generating responses.[3] Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources.[1] According to Ars Technica, "RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts." This method helps reduce AI hallucinations,[3] which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.[4] -->

RAG は、LLM を新しいデータで再学習させる必要性を軽減し、計算コストと費用を節約する[1]。効率性の向上に加え、RAG は LLM が回答に出典を含めることを可能にするため、ユーザーは引用元を検証できる。これにより、ユーザーは取得したコンテンツを相互に確認し、正確性と関連性を確認できるため、透明性が向上する。
<!-- RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs.[1] Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance. -->

RAG という用語は、2020 年の研究論文で初めて導入された[3]。
<!-- The term RAG was first introduced in a 2020 research paper.[3] -->

### RAG と LLM の限界<!-- ### RAG and LLM Limitations-->

<div class="figcenter">
<img src="/2025assets/RAG_diagram.svg" width="44%;">
</div><div class="figcaption">

Overview of RAG process, combining external documents and user input into an LLM prompt to get tailored output
</div>

LLM は誤った情報を提供することがある。例えば Google が LLM ツール Google Bard を初公開した際、LLM はジェームズ・ウェッブ宇宙望遠鏡について誤った情報を提供した。この誤りは同社の株価を 1000 億ドル下落させる一因となった[4]。RAG はこうした誤りを防ぐために用いられるが、全ての問題を解決するわけではない。例えば、事実上正しい情報源から引用しても、文脈を誤って解釈すれば誤った情報を生成する可能性がある。[MIT テクノロジーレビュー](https://en.wikipedia.org/wiki/MIT_Technology_Review)は、AI が生成した「米国にはバラク・フセイン・オバマというムスリム大統領が 1 人いた」という回答を例に挙げている。このモデルは学術書『バラク・フセイン・オバマ：アメリカ初のムスリム大統領か？』から引用した。LLM はタイトルの文脈を「知らず」「理解せず」、虚偽の主張を生成した[2]。
<!-- LLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool "Google Bard", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company’s stock value[4]. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, "The United States has had one Muslim president, Barack Hussein Obama." The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America’s First Muslim President? The LLM did not "know" or "understand" the context of the title, generating a false statement[2]. -->

RAG を備えた LLM は、新規情報を優先するようプログラムされている。この手法は プロンプトスタッフィング prompt stuffing と呼ばれる。プロンプトスタッフィングなしでは、LLM の入力はユーザーによって生成される。プロンプトスタッフィングでは、この入力に追加の関連文脈が加えられ、モデルの応答を誘導する。このアプローチにより、LLM はプロンプトの早い段階で重要な情報を得られ、既存の訓練知識よりも提供されたデータを優先するよう促される[5]。
<!--LLMs with RAG are programmed to prioritize new information. This technique has been called "prompt stuffing." Without prompt stuffing, the LLM's input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide the model’s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge[5]. -->

### プロセス<!-- ### Process-->

検索拡張生成（RAG）は、情報検索メカニズムを組み込むことで大規模言語モデル（LLM）を強化する。これによりモデルは元の訓練データセットを超えた追加データにアクセスし活用できる。Ars Technica は「新たな情報が利用可能になった場合、モデルを再訓練する必要はなく、更新された情報でモデルの外部知識ベースを拡張するだけでよい」と指摘している（拡張）[4]。IBM は「生成フェーズにおいて、LLM は拡張されたプロンプトと、その訓練データの内部表現から引き出し、その瞬間のユーザーに合わせた魅力的な回答を合成する」と述べている。[1]
<!--Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that "when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external knowledge base with the updated information" ("augmentation").[4] IBM states that "in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant".[1] -->


### RAG の主要な段階<!-- ### RAG key stages-->

通常、参照対象となるデータは、大規模なベクトル空間の形式で数値表現された LLM 埋め込みに変換されます。RAG は、非構造化データ（通常はテキスト）、半構造化データ、または構造化データ（例えばナレッジグラフ）に使用できる。これらの埋め込みは、ドキュメント検索を可能にするためにベクトルデータベースに保存される。
<!-- Typically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval. -->

ユーザークエリが与えられると、まずドキュメントリト検索器が呼び出され、クエリを拡張するために使用する最も関連性の高いドキュメントが選択される[2][3]。この比較は、使用されるインデックスの種類に応じてさまざまな方法で行うことができる[1]。
<!-- Given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query[2][3]. This comparison can be done using a variety of methods, which depend in part on the type of indexing used[1]. -->

モデルは、ユーザーの元のクエリを迅速にエンジニアリングすることで、この関連性の高い検索情報を LLM に入力する。新しい実装（2023 年現在）では、クエリを複数のドメインに拡張したり、メモリと自己改善を利用して以前の検索から学習したりする機能を備えた特定の拡張モジュールを組み込むこともできる。
<!-- The model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query. Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals. -->

最後に、LLM はクエリと取得した文書の両方に基づいて出力を生成することができる[2][6]。一部のモデルでは、取得した情報の再ランク付け、コンテキスト選択、微調整など、出力を改善するための追加ステップが組み込まれている。
<!--Finally, the LLM can generate output based on both the query and the retrieved documents[2][6]. Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning. -->

### 改善<!--Improvements-->

上述の基本過程の改善には RAG フローの種々の段階で適用されている。
<!-- Improvements to the basic process above can be applied at different stages in the RAG flow. -->

#### 符号化器<!--Encoder-->

これらの手法は、テキストを密ベクトルまたは疎ベクトルとして符号化することに重点を置いている。単語の同一性を符号化する疎ベクトルは、通常辞書長で、ほとんどがゼロで構成される。意味を符号化する密ベクトルはよりコンパクトで、ゼロの数が少なくなる。様々な機能強化により、ベクトルストア（データベース）における類似度の計算方法を改善できる[7]。
<!-- These methods focus on the encoding of text as either dense or sparse vectors. Sparse vectors, which encode the identity of a word, are typically dictionary-length and contain mostly zeros. Dense vectors, which encode meaning, are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).[7] -->

* ベクトル類似度の計算方法を最適化することで、性能が向上する。ドット積は類似度得点を強化し、近似最近傍法（ANN）検索は K 近傍法（KNN）検索よりも検索効率を向上させる[8]。
* 後インタラクション（Late Interactions）によって精度が向上する可能性がある。後インタラクションは、検索後に系がより正確に単語を比較することを可能にする。これは、文書のランキングを精緻化し、検索の関連性を向上させるのに役立つ[9]。
* ハイブリッドベクトルアプローチは、密ベクトル表現とスパースなワンホットベクトルを組み合わせるために使用でき、スパースドット積の計算効率を密ベクトル演算よりも活用できる[7]。
* 他の検索手法は、文書の選択方法を改善することで精度を向上させることに重点を置いている。SPLADE などの一部の検索手法では、スパース表現とクエリ拡張戦略を組み合わせることで、検索精度と再現率を向上させる[10]。

<!-- * Performance improves by optimizing how vector similarities are calculated. Dot products enhance similarity scoring, while approximate nearest neighbor (ANN) searches improve retrieval efficiency over K-nearest neighbors (KNN) searches.[8]
* Accuracy may be improved with Late Interactions, which allow the system to compare words more precisely after retrieval. This helps refine document ranking and improve search relevance.[9]
* Hybrid vector approaches may be used to combine dense vector representations with sparse one-hot vectors, taking advantage of the computational efficiency of sparse dot products over dense vector operations.[7]
* Other retrieval techniques focus on improving accuracy by refining how documents are selected. Some retrieval methods combine sparse representations, such as SPLADE, with query expansion strategies to improve search accuracy and recall.[10] -->

#### 検索器中心の手法<!-- #### Retriever-centric methods-->

これらの手法は、ベクトルデータベースにおける文書検索の品質向上を目的としている。
<!-- These methods aim to enhance the quality of document retrieval in vector databases: -->

* 逆クローズタスク（ICT: Inverse Cloze Task）を用いて検索器を事前学習する。これは、文書内のマスクされたテキストを予測することでモデルが検索パターンを学習するのを支援する手法である[11]。
* 教師あり検索器最適化は、検索確率を生成モデルの尤度分布に一致させる。これは、与えられたプロンプトに対して上位 k 個のベクトルを取得し、生成された応答のパープレキシティをスコアリングし、検索器の選択とモデルの尤度との間の KL ダイバージェンスを最小化することで、検索を改良する[12]。
* 再ランキング手法は、学習中に最も関連性の高い検索文書を優先することで、検索器の性能を向上させることができる[13]。

<!--* Pre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text within documents[11].
* Supervised retriever optimization aligns retrieval probabilities with the generator model’s likelihood distribution. This involves retrieving the top-k vectors for a given prompt, scoring the generated response’s perplexity, and minimizing KL divergence between the retriever’s selections and the model’s likelihoods to refine retrieval[12].
* Reranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training[13]. -->

### 言語モデル<!-- ### Language model-->

<img src="/2025assets/Language_model_in_Deepmind&apos;s_2021_Retro_for_RAG.svg.png" width="22%;">
<div class="figcaption">

RAG 用 Retro 言語モデル。各 Retro ブロックは、注意層、チャンク化交差注意層、フィードフォワード層で構成される。黒文字のボックスは変更されるデータを示し、青文字は変更を実行するアルゴリズムを示す。
<!-- Retro language model for RAG. Each Retro block consists of Attention, Chunked Cross Attention, and Feed Forward layers. Black-lettered boxes show data being changed, and blue lettering shows the algorithm performing the changes. -->
</div>

検索器を考慮して言語モデルを再設計することで、25 分の 1 の小さなネットワークで、はるかに大きなネットワークと同等のパープレキシティを実現できる[14]。この手法（Retro）はゼロから学習するため、従来の RAG 方式では回避できた高い学習コストが発生する。Retro は学習中にドメイン知識を与えることで、ドメインへの集中度が低くなり、より小さな重みリソースを言語意味論にのみ割り当てることができるという仮説が立てられている。再設計された言語モデルを以下に示す。
<!-- By redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts[14]. Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here. -->

Retro は再現性がないことが報告されているため、再現性を高めるための修正が行われた。より再現性の高いバージョンは Retro++ と呼ばれ、コンテキスト内 RAG[15] を備えている。
<!--It has been reported that Retro is not reproducible, so modifications were made to make it so. The more reproducible version is called Retro++ and includes in-context RAG[15]. -->

### チャンク化<!-- ### Chunking-->

チャンキングとは、データをベクトルに分割するための様々な手法を指す。これにより検索システムが詳細情報を見つけられるようになる。
<!-- Chunking involves various strategies for breaking up the data into vectors so the retriever can find details in it. -->

<img src="/2025assets/Rag-doc-styles.png">
<div class="figcaption">

異なるデータ形式には、適切なチャンキングが活用できるパターンが存在する
<!-- Different data styles have patterns that correct chunking can take advantage of. -->
</div>

チャンク化戦略には 3 つの種類がある[要出典]。
<!-- Three types of chunking strategies are:[citation needed] -->

* 固定長でオーバーラップさせる。これは高速かつ簡単である。連続するチャンクをオーバーラップさせることで、チャンク間の意味的コンテキストを維持するのに役立つ。
* 構文ベースのチャンクは、ドキュメントを文に分割できる。spaCy や NLTK などのライブラリも役立つ。
* ファイル形式ベースのチャンク化。特定のファイルタイプには自然なチャンクが組み込まれているため、それを尊重するのが最善である。例えば、コードファイルは関数またはクラス全体としてチャンク化し、ベクトル化するのが最適である。HTML ファイルでは、`<table>` 要素または base64 エンコードされた `<img>` 要素をそのまま残す必要がある。PDF ファイルについても同様の考慮が必要である。Unstructured や Langchain などのライブラリは、この方法に役立つ。

<!--* Fixed length with overlap. This is fast and easy. Overlapping consecutive chunks helps to maintain semantic context across chunks.
* Syntax-based chunks can break the document up into sentences. Libraries such as spaCy or NLTK can also help.
* File format-based chunking. Certain file types have natural chunks built in, and it's best to respect them. For example, code files are best chunked and vectorized as whole functions or classes. HTML files should leave <table> or base64 encoded <img> elements intact. Similar considerations should be taken for pdf files. Libraries such as Unstructured or Langchain can assist with this method. -->

### ハイブリッド検索<!-- ### Hybrid search-->

ベクトルデータベース検索では、ユーザーの質問に答えるために必要な重要な事実が見逃されることがある。これを軽減する方法の一つは、従来のテキスト検索を行い、その結果をベクトル検索で取得したベクトルにリンクされたテキストチャンクに追加し、そのハイブリッドテキストを言語モデルに入力して生成することである[要出典]。
<!-- Sometimes vector database searches can miss key facts needed to answer a user's question. One way to mitigate this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the combined hybrid text into the language model for generation.[citation needed] -->

### 評価とベンチマーク<!-- ### Evaluation and benchmarks -->

RAG システムは、検索可能性、検索精度、生成品質をテストするために設計されたベンチマークを用いて評価されるのが一般的である。よく使用されるデータセットには、多様な分野にわたる情報検索課題群である BEIR や、オープンドメイン QA 用の Natural Questions や Google QA などがある[要出典]。
<!--RAG systems are commonly evaluated using benchmarks designed to test retrievability, retrieval accuracy and generative quality. Popular datasets include BEIR, a suite of information retrieval tasks across diverse domains, and Natural Questions or Google QA for open-domain QA.[citation needed] -->

### 課題<!-- ### Challenges-->

RAG は LLM における幻覚を防ぐものではない。Ars Technica によると、「LLM は依然として応答においてソース資料に関する幻覚を生じる可能性があるため、直接的な解決策ではあない[4]」。
<!-- RAG does not prevent hallucinations in LLMs. According to Ars Technica, "It is not a direct solution because the LLM can still hallucinate around the source material in its response."[4] -->

RAG は大規模言語モデル（LLM）の精度を向上させるが、すべての課題を解消するわけではない。RAG の限界の一つは、頻繁なモデルの再学習の必要性は軽減するものの、完全に排除できるわけではないことである。さらに、LLM は信頼できる応答を提供するのに十分な情報が不足している場合、それを認識するのが困難になる場合がある。特別な学習を行わないと、モデルは不確実性を示すべき場合でも回答を生成する可能性がある。IBM によると、この問題は、モデルが自身の知識の限界を評価できない場合に発生する可能性がある[1]。
<!-- While RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to IBM, this issue can arise when the model lacks the ability to assess its own knowledge limitations.[1] -->

RAG システムは、事実上は正しいものの誤解を招くような情報源を取得し、解釈の誤りにつながる可能性がある。場合によっては、LLM は文脈を考慮せずに情報源から文を抽出し、誤った結論に至ることがある。さらに、矛盾する情報に直面した場合、RAG モデルはどちらの情報源が正確であるかを判断するのに苦労する可能性がある。この制限による最悪の結果は、モデルが複数の情報源からの詳細情報を統合し、古い情報と最新の情報を混在させた誤解を招くような回答を生成する可能性があることである。MIT Technology Review によると、これらの問題は RAG システムが取得したデータを誤って解釈する可能性があるために発生する[2]。
<!--RAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information RAG models may struggle to determine which source is accurate. The worst case outcome of this limitation is that the model may combine details from multiple sources producing responses that merge outdated and updated information in a misleading manner. According to the MIT Technology Review, these issues occur because RAG systems may misinterpret the data they retrieve.[2] -->

## Reference

1. "[What is retrieval-augmented generation?](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)". IBM. 22 August 2023. Retrieved 7 March 2025.
2. "[Why Google's AI Overviews gets things wrong](https://www.technologyreview.com/2024/05/31/1093019/why-are-googles-ai-overviews-results-so-bad/)". MIT Technology Review. 31 May 2024. Retrieved 7 March 2025.
3. Kiela Douwe, Lewis Patrick, Perez Ethan, Piktus Aleksandra, Petroni Fabio, Karpukhin Vladimir, Goyal Naman, Küttler Heinrich, Lewis Mike, Yih Wen-Tau, Rocktäschel Tim, Riedel Sebastian (2020). [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://dl.acm.org/doi/abs/10.5555/3495724.3496517). pp. 9459–9474. arXiv:2005.11401. ISBN 978-1-7138-2954-6.
4. "[Can a technology called RAG keep AI models from making stuff up?](https://arstechnica.com/ai/2024/06/can-a-technology-called-rag-keep-ai-models-from-making-stuff-up/)". Ars Technica. 6 June 2024. Retrieved 7 March 2025.
5. "[Mitigating LLM hallucinations in text summarisation](https://www.bbc.co.uk/rd/articles/2024-06-mitigating-llm-hallucinations-in-text-summarisation)". BBC. 20 June 2024. Retrieved 7 March 2025.
6. Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, 
 Douwe (2020). "[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)". Advances in Neural Information Processing Systems. 33. Curran Associates, Inc.: 9459–9474. arXiv:2005.11401.
7. Luan, Yi; Eisenstein, Jacob; Toutanova, Kristina; Collins, Michael (26 April 2021). "[Sparse, Dense, and Attentional Representations for Text Retrieval](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00369/100684/Sparse-Dense-and-Attentional-Representations-for)". Transactions of the Association for Computational Linguistics. 9: 329–345. arXiv:2005.00181. doi:10.1162/tacl_a_00369. Retrieved 15 March 2025.
8. "[Information retrieval](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-information-retrieval)". Microsoft. 10 January 2025. Retrieved 15 March 2025.
9. Khattab, Omar; Zaharia, Matei (2020). "[ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://dl.acm.org/doi/10.1145/3397271.3401075)". Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 39–48. doi:10.1145/3397271.3401075. ISBN 978-1-4503-8016-4.
10.  Wang, Yup; Conroy, John M.; Molino, Neil; Yang, Julia; Green, Mike (2024). "[Laboratory for Analytic Sciences in TREC 2024 Retrieval Augmented Generation Track](https://trec.nist.gov/pubs/trec33/index.html)". NIST TREC 2024. Retrieved 15 March 2025.
11. Lee, Kenton; Chang, Ming-Wei; Toutanova, Kristina (2019). "[Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://aclanthology.org/P19-1612.pdf)" (PDF).
12. Shi, Weijia; Min, Sewon; Yasunaga, Michihiro; Seo, Minjoon; James, Rich; Lewis, Mike; Zettlemoyer, Luke; Yih, Wen-tau (June 2024). "[REPLUG: Retrieval-Augmented Black-Box Language Models](https://aclanthology.org/2024.naacl-long.463/)". Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 8371–8384. arXiv:2301.12652. doi:10.18653/v1/2024.naacl-long.463. Retrieved 16 March 2025.
13. Ram, Ori; Levine, Yoav; Dalmedigos, Itay; Muhlgay, Dor; Shashua, Amnon; Leyton-Brown, Kevin; Shoham, Yoav (2023). "[In-Context Retrieval-Augmented Language Models](https://aclanthology.org/2023.tacl-1.75/)". Transactions of the Association for Computational Linguistics. 11: 1316–1331. arXiv:2302.00083. doi:10.1162/tacl_a_00605. Retrieved 16 March 2025.
14. Borgeaud, Sebastian; Mensch, Arthur (2021). "[Improving language models by retrieving from trillions of tokens](https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf)" (PDF).
15. Wang, Boxin; Ping, Wei; Xu, Peng; McAfee, Lawrence; Liu, Zihan; Shoeybi, Mohammad; Dong, Yi; Kuchaiev, Oleksii; Li, Bo; Xiao, Chaowei; Anandkumar, Anima; Catanzaro, Bryan (2023). "[Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study](https://aclanthology.org/2023.emnlp-main.482/)". Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 7763–7786. doi:[10.18653/v1/2023.emnlp-main.482](https://doi.org/10.18653%2Fv1%2F2023.emnlp-main.482).
