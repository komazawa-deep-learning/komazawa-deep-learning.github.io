---
title: "第22回 2025年度開講 駒澤大学 人工知能"
author: "浅川 伸一"
layout: home
codemirror_mode: python
codemirror_mime_type: text/x-cython
---

<div align="right">
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 14/Nov/2025<br/>
Appache 2.0 license<br/>
</div>

<link href="/css/asamarkdown.css" rel="stylesheet">

* [課題提出用フォルダ](https://drive.google.com/drive/u/6/folders/1paq7yQ2abtWxxpZZJXgVhm9Ji53_aKqC){:target="_blank"}

#### キーワード

チューリングテスト，RAG (Retrieval Augmented Generation, 検索拡張生成)，幻覚 hallicination

## 実習

- [sentenceBERT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602minimam_sentenceBERT.ipynb){:target="_blank"}
* [BERT の微調整 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0623BERT_SNOW_training.ipynb){:target="_blank"}
* [BERT のマルチヘッド注意の視覚化 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1007BERT_head_view.ipynb){:target="_blank"}
* [日本語 BERT 2 つの文の距離を求めるデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0624BERTja_test.ipynb){:target="_blank"}
- [chatGPT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0608rinna_chatGPT_demo.ipynb){:target="_blank"}
- [ゼロからの Transformer <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602Transformer_from_scratch.ipynb){:target="_blank"}
* [Annotated Transformers <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1007Annotated_Transformer.ipynb){:target="_blank"}

* [加算型注意 (Bahdanu) と 内積型注意 (Loung) の実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb){:target="_blank"}

* Huggingface RTL
  * [TRL - Transformer Reinforcement Learning](https://github.com/huggingface/trl/tree/main){:target="_blank"}


# Transformer

Transformer は人工知能へのアプローチを根本的に変えたゲームチェンジャー。Transformer は [Attention is All You Need](https://dl.acm.org/doi/10.5555/3295222.3295349) で導入され、深層学習モデルの定番アーキテクチャとなっている。OpenAI の **GPT**、Meta の **Llama**、Google の **Gemini** といったテキスト生成モデルで採択されている。Transformer はテキスト以外にも [音声合成](https://huggingface.co/learn/audio-course/en/chapter3/introduction "Hugging Face")，[画像認識](https://huggingface.co/learn/computer-vision-course/unit3/vision-transformers/vision-transformers-for-image-classification "Hugging Face")、[タンパク質構造予測](https://elifesciences.org/articles/82819 "eLife")、さらには[ゲームプレイ](https://www.deeplearning.ai/the-batch/reinforcement-learning-plus-transformers-equals-efficiency/ "Deep Learning AI")まで、様々な分野でその汎用性を示している。

テキスト生成型 Transformer モデルは**次単語予測**原理で動作する。ユーザからのテキストプロンプトが与えられた時、この入力に続く **最も確率の高い次単語** は何かを予測する。
Transformer の中核的な革新性と強みは、自己注意機構の活用にある。これにより、従来のアーキテクチャよりも効果的に系列全体を処理し、長距離依存関係を捕捉できる。

GPT-2 モデルは、テキスト生成型 Transformer の代表例である。Transformer Explainer は、1 億 2400 万のパラメータを持つ [GPT-2](https://huggingface.co/openai-community/gpt2)(small) モデルを基盤としている。最新かつ最強の Transformer モデルではないが、現在の最先端モデルと多くの共通したアーキテクチャ構成要素や原理を備えているため、基礎を理解する理想的な出発点となる。

* Transformer: マルチヘッド注意機構，位置符号化器，埋め込み表現，ソフトマックス関数
* 事前訓練とファインチューニング: マスク言語モデル，次文予測  <!--Masked Language model, next sentence prediction--->
* GTP-4: 符号化器・復号化器モデルを用いた画像と言語との融合
* プロンプトエンジニアリングと RLHF (人間のフィードバックによる強化学習): 報酬モデルと代理方針最適化 (Proximal Policy Optimization)

<div class="figcenter">
<img src="/assets/2017Vaswani_Fig2_1.svg" width="09%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/2017Vaswani_Fig2_2.svg" width="19%">&nbsp;&nbsp;&nbsp;
<img src="/assets/2017Vaswani_Fig1.svg" width="29%">
<div class="figcaption">

Transformer ([2017Vaswani++](https://arxiv.org/abs/1706.03762)) Fig.2 を改変
</div></div>



上図で，`matmul` は行列の積，`scale` は，平均 0 分散 1 への標準化，`mask` は 0 と 1 とで，データを制限すること，`softmax` はソフトマックス関数である。

トランスフォーマーの注意とは，このソフトマックス関数である。

<!-- <div class="figure figcenter">
<img src="figures/2017Vaswani_Fig2_1.svg" width="19%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="figures/2017Vaswani_Fig2_2.svg" width="29%">&nbsp;&nbsp;&nbsp;
<img src="figures/2017Vaswani_Fig1.svg" width="39%"> -->

<!-- # Transformer(2): Attention is all you need -->


$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\text{head}_1,\ldots,\text{head}_{h}\right)W^O
$$
where, $\text{head}_i =\text{Attention}\left(QW_i^Q,KW_i^K,VW_i^V\right)$

The projections are parameter matrices $W_i^Q\in\mathbb{R}^{d_{\text{model}}\times d_k}$, $W_i^K \in\mathbb{R}^{d_{\text{model}}\times d_k}$,
$W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$, and $W^O\in\mathbb{R} ^{hd_v\times d_{\mathop{model}}}$.
$h=8$, $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$
\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2
$$

$$
\text{PE}_{(\text{pos},2i)} = \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

$$
\mathop{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

### [GLUE: General Language Understanding Evaluation](https://gluebenchmark.com/leaderboard){:target="_blank"}

<div class="figure figcenter">
<img src="/2023assets/2019-08-21GLUE_leaderboard.png" width="77%">
<div class="figcaption">

[GLUE: General Language Understanding Evaluation](https://gluebenchmark.com/leaderboard){:target="_blank"}
</div></div>

### LLM(大規模言語モデル) の性能向上

<div class="figcenter">
<img src="/2024assets/2021Brown_GPT3_fig3_13.jpg" width="77%">
<div class="figcaption" style="width:98%;">

ニュース記事がモデルによって生成されたものであるかどうかを識別する人間の能力 (正しい割り当てと中立でない割り当ての比率で測定) は，モデルサイズが大きくなるほど低下する。
意図的に悪い対照モデル (出力のランダム性が高い無条件 GPT-3 小型モデル) の出力に対する精度を上部の破線で示し，ランダムな確率 (50 %) を下部の破線で示す。ベストフィットの線は 95 %信頼区間を持つべき乗則である。
<!-- #### Figure 3.13: People’s ability to identify whether news articles are model-generated (measured by the
 ratio of correct assignments to non-neutral assignments) decreases as model size increases.
Accuracy on the outputs on the deliberately bad control model (an unconditioned GPT-3 Small model with higher
output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with
 the dashed line at the bottom. Line of best fit is a power law with 95% confidence intervals. -->
ニュース記事がモデルによって生成されたものであるかどうかを識別する人間の能力 (正しい割り当てと中立でない割り当ての比率で測定) は，モデルサイズが大きくなるほど低下する。意図的に悪い対照モデル (出力のランダム性が高い無条件 GPT-3 小型モデル) の出力に対する精度を上部の破線で示し，ランダムな確率 (50 %) を下部の破線で示す。ベストフィットの線は 95 %信頼区間を持つべき乗則である。
[Brown+2021, arXiv:2005.14165](https://arxiv.com/abs/2005.14165/) Fig. 3
<!-- People’s ability to identify whether news articles are model-generated (measured by the ratio of correct
assignments to non-neutral assignments) decreases as model size increases.
Accuracy on the outputs on the deliberately bad control model (an unconditioned GPT-3 Small model with higher
output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with
 the dashed line at the bottom.
Line of best fit is a power law with 95% confidence intervals. -->
<!-- #### Figure 3.13: -->
</div></div>



#### GLUE 下位課題

* CoLA: 入力文が英語として正しいか否かを判定
* SST-2: スタンフォード大による映画レビューの極性判断
* MRPC: マイクロソフトの言い換えコーパス。2 文 が等しいか否かを判定
* STS-B: ニュースの見出し文の類似度を 5 段階で評定
* QQP: 2 つの質問文の意味が等価かを判定
* MNLI: 2 入力文が意味的に含意，矛盾，中立を判定
* QNLI: Q and A
* RTE: MNLI に似た 2 つの入力文の含意を判定
* WNI: ウィノグラッド会話チャレンジ

<!-- ### SOTA モデルの特徴

* RoBERTa: BERT の訓練コーパスを巨大 (173GB) にし，ミニバッチサイズを大きした
* XLNet: 順列言語モデル。2 ストリーム注意
* MT-DNN: BERT ベース の転移学習に重きをおいたモデル
* GPT-X: BERT に基づく。人間超えして 2019 年 2 月時点で炎上騒ぎ
* BERT: Transformer に基づく言語モデル。**マスク化言語モデル**  と **次文予測** に基づく **事前訓練**，各下流課題を **ファインチューニング**。
事前訓練されたモデルは一般公開済。
* ELMo: 双方向 RNN による文埋め込み表現-->

* Transformer: 自己注意に基づく言語モデル。多頭注意，位置符号器.


#### 事前訓練とマルチ課題学習

<div class="figcenter">
<img src="/assets/mt-dnn.png" width="66%"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</div>

$$
\mathop{attention}\left(Q,K,V\right)=\mathop{dropout}\left(\mathop{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)\right)V
$$

<!-- <div class="figcenter">
<img src="/assets/2017Vaswani_Fig2_1.svg" width="17%">
<img src="/assets/2017Vaswani_Fig2_2.svg" width="23%"><br/>
From [@2017Vaswani_transformer] Fig. 2
</div>

$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\mathop{head}_ {1},\ldots,\mathop{head}_ {h}\right)W^O
$$

where, $\text{head}_{i} = \text{Attention}\left(QW_i^Q,KW_{i}^K,VW_{i}^V\right)$

The projections are parameter matrices

- $W_i^Q\in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^K \in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$,
- $W^O\in\mathbb{R}^{hd_v\times d_{\mathop{model}}}$. $h=8$
- $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$
\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2
$$

$$
\text{PE}_{(\mathop{pos},2i)} = \sin\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

$$
\text{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$ -->

#### バイト対符号化 Byte pair encoding<!-- ## 5.2 Byte Pair Encoding-->

入力配列を符号化するために，**Byte Pair Encoding** ([**BPE**](https://arxiv.org/abs/1508.07909)) が使用される。
BPE はもともと 1990 年代にデータ圧縮アルゴリズムとして提案され，その後，機械翻訳におけるオープンボキャブラリーの問題を解決するために採用されたもので，新しい言語に翻訳する際に稀な単語や未知の単語に遭遇しやすくするためである。
BPE は，未知の単語はしばしば複数の部分単語に分解できるという直観に基づき，頻出する文字対を反復的かつ貪欲に結合することで最適な単語分割を求める。
<!-- **Byte Pair Encoding** ([**BPE**](https://arxiv.org/abs/1508.07909)) is used to encode the input sequences. BPE was originally proposed as a data compression algorithm in 1990s and then was adopted to solve the open-vocabulary issue in machine translation, as we can easily run into rare and unknown words when translating into a new language.
Motivated by the intuition that rare and unknown words can often be decomposed into multiple subwords, BPE finds the best word segmentation by iteratively and greedily merging frequent pairs of characters. -->

#### 教師付き微調整<!-- ## 5.3 Supervised Fine-Tuning-->

OpenAI GPT が提案した最も実質的なアップグレードは，課題固有のモデルを取り除き，事前に学習された言語モデルを直接使用することである。この方法は，課題特化モデルから，課題特化モデルへの移行を可能にする。
<!-- The most substantial upgrade that OpenAI GPT proposed is to get rid of the task-specific model and use the pre-trained language model directly! -->

分類を例にとって考えてみる。ラベル付きデータセットにおいて，各入力は $n$ 個のトークン，$\mathbf{x}=(x_1, \dots, x_n)$，1 つのラベル $y$ を持っているとする。GPT はまず，入力系列 $\mathbf{x}$ を事前に学習した変換復号化器で処理し，最後のトークン$x_n$ に対する最終層出力は $\mathbf{h}_L^{(n)}$ となる。そして，1 つの新しい学習可能な重み行列 $\mathbf{W}_y$ のみで，クラスラベル上の分布を予測することができる。
<!-- Let's take classification as an example. Say, in the labeled dataset, each input has $n$ tokens, $\mathbf{x} = (x_1, \dots, x_n)$, and one label $y$. GPT first processes the input sequence $\mathbf{x}$ through the pre-trained transformer decoder and the last layer output for the last token $x_n$ is $\mathbf{h}_L^{(n)}$. Then with only one new trainable weight matrix $\mathbf{W}_y$, it can predict a distribution over class labels. -->

<div class="figure figcenter">
<img src="/2024assets/GPT-classification.png" width="49%">
</div>

$$ P(y|x_1, \dots, x_n) = \text{softmax}(\mathbf{h}_L^{(n)}\mathbf{W}_y) $$

この損失は，真のラベルに対する負の対数尤度を最小化するものである。
また，LM 損失を補助的な損失として追加することは，以下の理由で有益であることがわかった。
<!-- The loss is to minimize the negative log-likelihood for true labels.
In addition, adding the LM loss as an auxiliary loss is found to be beneficial, because:-->

1. 学習時の収束を早めることができる。
2. 教師付きモデルの汎化性を向上させることが期待される。

<!-- 1. it helps accelerate convergence during training and
2. it is expected to improve the generalization of the supervised  model. -->

$$ \begin{aligned}
{L}_\text{cls} &= \sum_{(\mathbf{x}, y) \in {D}} \log P(y\mid x_1, \dots, x_n) = \sum_{(\mathbf{x}, y) \in {D}} \log \text{softmax}(\mathbf{h}_L^{(n)}(\mathbf{x})\mathbf{W}_y) \\
{L}_\text{LM}  &= -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1}) \\
{L}            &= {L}_\text{cls} + \lambda {L}_\text{LM}
\end{aligned}$$

同様の設計で，他の終端課題のためにカスタマイズされたモデル構造は必要ない (図 7 参照)。
課題の入力が複数の文を含む場合，特別なデリミタトークン (`$`) が文の各対の間に追加される。
このデリミタトークンの埋め込みは新たに学習する必要のあるパラメータであるが，かなり最小限のものであるはずである。
<!-- With similar designs, no customized model structure is needed for other end tasks (see Fig. 7).
If the task input contains multiple sentences, a special delimiter token (`$`) is added between each pair of sentences.
The embedding for this delimiter token is a new parameter we need to learn, but it should be pretty minimal.-->

文の類似性課題では，順序は重要ではないので，両方の順序が含まれる。多肢選択課題では，文脈は全ての回答候補と対になる。
<!-- For the sentence similarity task, because the ordering does not matter, both orderings are included.
For the multiple choice task, the context is paired with every answer candidate. -->

<div class="figure figcenter">
<img src="/2024assets/GPT-downstream-tasks.png" width="66%">
<div class="figcaption">

##### 下流課題のために変更した GPT Transformer モデルで訓練対象 (画像出典: [原著論文](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))
<!-- ##### Fig. 8. Training objects in slightly modified GPT transformer models for downstream tasks. -->
<!-- (Image source: [original paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)) -->
</div></div>

**まとめ**: このような一般的な枠組みが，当時 (2018 年 6 月) のほとんどの言語課題で SOTA に勝るというのは，超すてきで心強いことである。
第一段階では，言語モデルの生成的な事前学習は，可能な限り多くのフリーテキストを吸収することができる。
その後，第 2 段階では，少ないラベル付きデータセットと学習する新しいパラメータの最小セットで，特定の課題でモデルの微調整を行う。
<!-- **Summary**: It is super neat and encouraging to see that such a general framework is capable to beat SOTA on most language tasks at that time (June 2018).
At the first stage, generative pre-training of a language model can absorb as much free text as possible.
Then at the second stage, the model is fine-tuned on specific tasks with a small labeled dataset and a minimal set of new parameters to learn.-->

GPT の限界の一つは，その一方向的な性質であり，モデルは将来の左から右への文脈を予測するためにのみ学習される。
<!-- One limitation of GPT is its uni-directional nature --- the model is only trained to predict the future left-to-right context. -->



<!-- #### 微調整 (fine-tunig) 以外の課題調整法

<div class="figure figcenter">
<img src="/2023assets/2022Quyang_instructGPT_fig2ja.svg" width="99%">
<div class="figcaption">

instructGPT の概要 [2022Quyang+](https://arxiv.org/abs/2203.02155) Fig.2 を改変

</div></div> -->

#### GPT

OpenAI は **GPT**（**Generative Pre-training Transformer**の略）([Radford+2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)) として，教師なし言語モデルを膨大なフリーテキストコーパスで学習することでより大きな規模に拡張している。

##### 言語モデルとしての Transformer 復号化器<!-- ## 5.1 Transformer Decoder as Language Model-->

[transformer 復号化器](https://arxiv.org/abs/1801.10198) は，[オリジナル Transformer](https://arxiv.org/abs/1706.03762) と比較して，符号化器部分を削除したモデルであり，入力文はソースとターゲットの 2 つではなく，1 つのみである。
<!-- Compared to the [original transformer](https://arxiv.org/abs/1706.03762) architecture, the [transformer decoder](https://arxiv.org/abs/1801.10198) model discards the encoder part, so there is only one single input sentence rather than two separate source and target sequences. -->

このモデルは入力配列の埋め込みに対して複数の Transformer ブロックを適用する。各ブロックはマスク化 **多頭自己注意** 層と **点毎フィードフォワード** 層を含む。最終出力は，ソフトマックス正規化後の目標トークンに対する分布を生成する。
<!-- This model applies multiple transformer blocks over the embeddings of input sequences. Each block contains a masked *multi-headed self-attention* layer and a *pointwise feed-forward* layer. The final output produces a distribution over target tokens after softmax normalization. -->

<div class="figure figcenter">
<img src="/2024assets/OpenAI-GPT-transformer-decoder.png" width="66%">
<div class="figcaption">

OpenAI の GPT における transformer 復号化器モデルアーキテクチャ<!-- Fig. 7. The transformer decoder model architecture in OpenAI GPT. -->
</div></div>

損失は負の対数尤度であり，[ELMo](#elmo) と同じであるが，後方計算は行わない。例えば，サイズ $k$ の文脈窓が対象単語の前にあるとすると，損失は次のようになる:
<!-- The loss is the negative log-likelihood, same as [ELMo](#elmo), but without backward computation. Let's say, the context window of the size $k$ is located before the target word and the loss would look like: -->

$$ {L}_\text{LM} = -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1}) $$


##### GPT-4
chatGPT の後続モデルである GPT-4 では，マルチモーダル，すなわち，視覚と言語の統合が進んだ。

<div class="figcenter">
<img src="/2024assets/2023kosmos_coverpage.png" width="55%">
<div class="figcaption">

[Kosmos-1 の概念図](https://arXiv.org/abs/2302.14045)
</div></div>

### BERT

**BERT** は，**Bidirectional Encoder Representations from Transformers** ([Devlin+2019](https://arxiv.org/abs/1810.04805)) の略で，自由テキストで大規模言語モデルを学習し，ネットワークアーキテクチャをカスタマイズせずに特定の課題で微調整するという，[GPT](#gpt) の直系に当たるものである。
<!-- **BERT**, short for **Bidirectional Encoder Representations from Transformers** ([Devlin+2019](https://arxiv.org/abs/1810.04805)) is a direct descendant to [GPT](#gpt): train a large language model on free text and then fine-tune on specific tasks without customized network architectures.-->

GPT と比較して，BERT の最大の違いと改良点は，学習を **双方向** にすることである。
このモデルは，左右両方の文脈を予測するように学習する。
アブレーション研究による論文では，次のように主張されている。
<!-- Compared to GPT, the largest difference and improvement of BERT is to make training **bi-directional**.
The model learns to predict both context on the left and right.
The paper according to the ablation study claimed that: -->

> "bidirectional nature of our model is the single most important new contribution"

## 事前学習課題
<!-- ## 6.1 Pre-training Tasks-->

BERT のモデル・アーキテクチャは，多層双方向トランスフォーマー符号化器である。
<!-- The model architecture of BERT is a multi-layer bidirectional Transformer encoder. -->

<div class="figure figcenter">
<img src="/2024assets/transformer-encoder-2.png" width="19%">
<div class="figcaption">

図 トランスフォーマー符号化器のモデル・アーキテクチャ
<!-- Fig. 9. Recap of Transformer Encoder model architecture. (Image source: [Transformer paper](https://arxiv.org/abs/1706.03762)) -->
</div></div>

双方向の予測と文レベルの理解を促すために，BERT は基本的な言語課題 (文脈が与えられた次トークンを予測する) の代わりに，2 つの課題で訓練される。
<!-- To encourage the bi-directional prediction and sentence-level understanding, BERT is trained with two tasks instead of the basic language task (that is, to predict the next token given context). -->

**課題 1: マスク化言語モデル (MLM)**<!-- **Task 1: Mask language model (MLM)** -->

> [Wikipedia](https://en.wikipedia.org/wiki/Cloze_test)より。`クローズテスト (クローズ削除テストとも) とは，特定の項目，単語，記号が削除された言語の一部 (クローズテキスト) からなる訓練，検査，評価であり，参加者は失われた言語項目を置き換えるよう求められるものである。...
この訓練は，1953 年に W.L.テイラー によって初めて説明された。`
<!-- > From [Wikipedia](https://en.wikipedia.org/wiki/Cloze_test): "A cloze test (also cloze deletion test) is an exercise, test, or assessment consisting of a portion of language with certain items, words, or signs removed (cloze text), where the participant is asked to replace the missing language item. ...
The exercise was first described by W.L. Taylor in 1953." -->

単語の直後ではなく，単語の周りの文脈を学習する表現が，構文的にも意味的にも，その意味をよりよく捉えることができると考えるのは当然である。
BERT は **言語モデルをマスク化課題** で訓練することにより，モデルがそうなるように促している。
<!-- It is unsurprising to believe that a representation that learns the context around a word rather than just after the word is able to better capture its meaning, both syntactically and semantically.
BERT encourages the model to do so by training on the *"mask language model" task*: -->

1. 各系列に含まれるトークンの 15％ をランダムにマスクする。
なぜなら，マスクされたトークンを特別なプレースホルダ `[MASK]` に置き換えるだけでは，微調整中に特別なトークンに遭遇することはないからである。
そこで，BERT はいくつかの発見的なトリックを採用した。
    - (a) 80 % の確率で，選択された単語を `[MASK]` に置き換える。
    - (b) 10 % の確率で，ランダムな単語で置き換える。
    - (c) 10 % の確率で，そのままの状態を維持する。
2. このモデルは欠落した単語を予測するだけで，どの単語が置換されたのか，どの単語を予測すべきなのかの情報は持っていない。
出力サイズは入力サイズの 15% に過ぎない。

<!-- 1. Randomly mask 15% of tokens in each sequence.
Because if we only replace masked tokens with a special placeholder `[MASK]`, the special token would never be encountered during fine-tuning.
Hence, BERT employed several heuristic tricks:
    - (a) with 80% probability, replace the chosen words with `[MASK]`;
    - (b) with 10% probability, replace with a random word;
    - (c) with 10% probability, keep it the same.
2. The model only predicts the missing words, but it has no information on which words have been replaced or which words should be predicted.
The output size is only 15% of the input size. -->

**課題 2: 次文予測**<!-- **Task 2: Next sentence prediction** -->

下流課題の多くが文の関係を理解すること (例 [QA](#qa), [NLI](#nli)) に動機づけられ，BERT は，ある文が他の文の次の文かどうかを判別する **2値分類器** を学習する補助課題を追加した。
<!-- Motivated by the fact that many downstream tasks involve the understanding of relationships between sentences (i.e., [QA](#qa), [NLI](#nli)), BERT added another auxiliary task on training a *binary classifier* for telling whether one sentence is the next sentence of the other: -->

1. 文の組 (A, B) を以下のようにサンプルする。
    - (a) 50％の確率で，B は A の次の文
    - (b) 50％の確率で，B は A の次の文ではない
2. モデルは両方の文を処理し，B が A の次の文であるかどうかを示す 2 値ラベルを出力する。

<!-- 1. Sample sentence pairs (A, B) so that:
    - (a) 50% of the time, B follows A;
    - (b) 50% of the time, B does not follow A.
2. The model processes both sentences and output a binary label indicating whether B is the next sentence of A.-->

上記 2 つの補助課題の学習データは，任意の単一言語コーパスから些細に生成することができる。
したがって，訓練の規模は無限大である。
訓練損失は，平均マスク化 LM 尤度と平均次文予測尤度の和である。
<!-- The training data for both auxiliary tasks above can be trivially generated from any monolingual corpus.
Hence the scale of training is unbounded.
The training loss is the sum of the mean masked LM likelihood and mean next sentence prediction likelihood. -->

<div class="figure figcenter">
<img src="/2024assets/language-model-comparison.png" width="66%">
<div class="figcaption">

##### BERT, OpenAI GPT, ELMo アーキテクチャの比較 (画像出典: [原著論文](https://arxiv.org/abs/1810.04805))
<!-- Fig. 10. Comparison of BERT, OpenAI GPT and ELMo model architectures.
(Image source: [original paper](https://arxiv.org/abs/1810.04805)) -->
</div></div>

### 入力埋め込み<!-- ## 6.2 Input Embedding-->

入力埋め込みは，3 つの部分の合計: <!-- The input embedding is the sum of three parts: -->

1.  **WordPiece トークン埋込み**:
[WordPiece](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) [model](https://arxiv.org/pdf/1609.08144.pdf) は，もともと日本語や韓国語の文節問題のために提案された。
英単語を自然に分割するのではなく，さらに小さな部分単語単位に分割することで，希少語や未知の単語を扱うのに効果的である。
最適な単語分割方法については [リンク](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) [論文](https://arxiv.org/pdf/1609.08144.pdf) を参照。
2.  **セグメント埋め込み Segment Embeddings**: 入力が 2 つの文を含む場合，それぞれ文 A 埋め込み，文 B 埋め込みとし，特殊文字 `[SEP]` で区切られる。
3.  **位置埋め込み**。位置埋め込みは，ハードコーディングではなく，学習される。

<!-- 1.  *WordPiece tokenization embeddings*:
The [WordPiece](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) [model](https://arxiv.org/pdf/1609.08144.pdf) was originally proposed for Japanese or Korean segmentation problem.
Instead of using naturally split English word, they can be further divided into smaller sub-word units so that it is more effective to handle rare or unknown words.
Please read [linked](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) [papers](https://arxiv.org/pdf/1609.08144.pdf) for the optimal way to split words if interested.
2.  *Segment embeddings*: If the input contains two sentences, they have sentence A embeddings and sentence B embeddings respectively and they are separated by a special character `[SEP]`; Only sentence A embeddings are used if the input only contains one sentence.
3.  *Position embeddings*: Positional embeddings are learned rather than hard-coded. -->

<div class="figure figcenter">
<img src="/2024assets/BERT-input-embedding.png" width="55%">
<div class="figcaption">

**BERT の入力表現** (画像出典: [原著論文](https://arxiv.org/abs/1810.04805))
<!-- Fig. 11. BERT input representation. (Image source: [original paper](https://arxiv.org/abs/1810.04805)) -->
</div></div>

最初のトークンは常に `[CLS]` --- 下流課題で予測するために後で使われるプレースホルダーであることを強制されることに注意
<!-- Note that the first token is always forced to be `[CLS]` --- a placeholder that will be used later for prediction in downstream tasks. -->

#### 下流課題における BERT の使用<!-- ## 6.3 Use BERT in Downstream Tasks-->

BERT の微調整は，OpenAI の GPT と同様，いくつかの新しいパラメータを追加するだけでよい。<!-- BERT fine-tuning requires only a few new parameters added, just like OpenAI GPT. -->

<!-- 分類課題では，特別な最初のトークン `[CLS]` の最終的な隠れ状態 $\mathbf{h}^\text{\[CLS\]}_L$ を取り出し，小さな重み行列 $\text{softmax}(\mathbf{h}^\text{\[CLS\]}_L\mathbf{W}_\text{cls})$ と乗算して予測を得ることができる。 -->
<!-- For classification tasks, we get the prediction by taking the final hidden state of the special first token `[CLS]`, $\mathbf{h}^\text{[CLS]}_L$, and multiplying it with a small weight matrix, $\text{softmax}(\mathbf{h}^\text{[CLS]}_L\mathbf{W}_\text{cls})$. -->

<!-- SQuAD のような [QA](#qa) 課題では，与えられた問題に対して与えられたパラグラフのテキストスパンを予測する必要がある。
BERT は，すべてのトークンについて，テキストスパンの開始と終了という 2 つの確率分布を予測する。
新しい小さな行列は $\mathbf{W}_\text{s}$ と $\mathbf{W}_\text{e}$ の 2 つ，は微調整の際に新たに学習したもので，$\text{softmax}(\mathbf{h}^\text{(i)}_L\mathbf{W}_\text{s})$ と $\text{softmax}(\mathbf{h}^\text{(i)}_L\mathbf{W}_\text{e})$ は二つの確率分布である。 -->
<!-- For [QA](#qa) tasks like SQuAD, we need to predict the text span in the given paragraph for an given question.
BERT predicts two probability distributions of every token, being the start and the end of the text span.
Only two new small matrices, $\mathbf{W}_\text{s}$ and $\mathbf{W}_\text{e}$, are newly learned during fine-tuning and $\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{s})$ and $\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{e})$ define two probability distributions. -->

全体として，エンド課題の微調整のためのアドオン部分は非常に少なく，トランスフォームの隠れ状態を解釈可能な形式に変換するための 1 つか 2 つの重み行列である。
他のケースでの実装の詳細については，論文を参照。
<!-- Overall the add-on part for end task fine-tuning is very minimal --- one or two weight matrices to convert the Transform hidden states to an interpretable format.
Check the paper for implementation details for other cases. -->

<div class="figure figcenter">
<img src="/2024assets/BERT-downstream-tasks.png" width="55%">
<div class="figcaption">

図 下流課題のためにわずかに修正された BERT モデルの訓練目的関数 <!-- ##### Fig. 12. Training objects in slightly modified BERT models for downstream tasks. -->(Image source: [original paper](https://arxiv.org/abs/1810.04805))
</div></div>

以下は [OpenAI GPT](#gpt) と BERT のファインチューニングの違いを比較したまとめ表である。
<!-- A summary table compares differences between fine-tuning of [OpenAI GPT](#gpt) and BERT. -->

|| **OpenAI GPT** | **BERT**|
|:---|:------------|:---------|
|特殊文字| `[SEP]` と `[CLS]` は微調整の段階でのみ導入される。| `[SEP]` と `[CLS]` と 文 A/B の埋め込みは，事前学習の段階で学習する。|
|学習処理| 1M ステップ，バッチサイズ 32 k 語 |  1M ステップ，バッチサイズ 128 k 単語|
|微調整  | すべての微調整課題に対して lr=5e-5 | 微調整は課題固有の lr を使用|


<!-- | | **OpenAI GPT** | **BERT** |
|:---|:------------|:---------|
| Special char| `[SEP]` and `[CLS]` are only introduced at fine-tuning stage.| `[SEP]` and `[CLS]` and sentence A/B embeddings are learned at the pre-training stage. |
|Training process | 1M steps, batch size 32k words. | 1M steps, batch size 128k words.|
| Fine-tuning | lr = 5e-5 for all fine-tuning tasks. | Use task-specific lr for fine-tuning. | -->


#### 位置符号器 Position encoders

 Transformer の入力には，上述の単語表現に加えて，位置符号器からの信号も重ね合わされる。
位置 $i$ の信号は次式で周波数領域へと変換される:

$$
\text{PE}_{(\text{pos},2i)} = \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\\
\text{PE}_{(\text{pos},2i+1)} = \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$

位置符号器による位置表現は，$i$ 番目の位置情報をワンホット表現するのではなく，
周波数領域に変換することで周期情報を表現する試みと見なし得るだろう。

<div class="figcenter">
<img src="/assets/PE_example.svg" width="66%">
<div class="figcaption">
位置符号化に用いられる符号化
</div></div>


<!-- <div class="figure figcenter">
<img src="figures/2018Devlin_BERT_Fig2.svg">
</div> -->

<!-- #### 事前訓練: マスク化言語モデル

全入力系列のうち 15% をランダムに [MASK] トークンで置き換える

* 入力はオリジナル系列を [MASK] トークンで置き換えた系列
* ラベル: オリジナル系列の [MASK] 部分にの正しいラベルを予測

%Rather than always replacing the chosen words with [MASK], the date generator will do the following:

* 80%: オリジナル入力系列を [MASK] で置換
y $\rightarrow$ my dog is  [MASK].
* 10%: [MASK] の位置の単語をランダムな無関連語で置き換える
my dog is hairy $\rightarrow$ my dog is apple
* 10%: オリジナル系列

#### 事前訓練: 次文予測課題

言語モデルの欠点を補完する目的，次の文を予測

[SEP] トークンで区切られた 2 文入力
* 入力: the man went to the store [SEP] he bought a gallon of milk.
* ラベル: IsNext
* 入力: the man went to the store [SEP] penguins are flightless  birds.
* ラベル: NotNext

#### 微調整(ファインチューニング)

<div class="figcenter">
<img src="/2023assets/2019Liu_mt-dnn.png" width="66%">
<div class="figcaption">

[Liu+2019](https://arxiv.org/abs/1901.11504) Fig. 1
</div></div> -->

<!-- <div class="figcenter">
<img src="figures/2017Vaswani_Fig2_1ja.svg" width="22%">
<img src="figures/2017Vaswani_Fig2_2ja.svg" width="22%">
<div class="figcaption">
From Vaswani+2017 transformer Fig. 2
</div></div> -->

<!-- 
<div class="figcenter">
<img src="/assets/2018Devlin_BERT_Fig3.svg">
<div class="figcaption">

(a), (b) は文レベル課題，(c),(d)はトークンレベル課題, E: 入力埋め込み表現,
$T_i$: トークン $i$ の文脈表象。[CLS]: 分類出力記号, [SEP]:文分離記号
</div></div> -->




### StableDiffusion と LangChain

<div class="figure figcenter">
<img src="/2023assets/2022patrickvonplaten_scientific_images_stable_diffusion.png" width="34%">
<img src="/2023assets/2023Polzer_LLM_app_ja_fig1.webp" width="44%">
<div class="figcaption">
左: `https://github.com/patrickvonplaten/scientific_images/tree/master`，
右: [Polzer2023](https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac) より
</div></div>

<!-- * [StableDiffusion <img src="figures/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0714stable_diffusion.ipynb) -->


## [チューリングテスト](https://www.science.org/doi/10.1126/science.adq9356)<!--The Turing Test and our shifting conceptions of intelligence-->

「機械は考えることができるか？」と、アラン・チューリングは 1950 年の論文「計算機械と知能」の中で問いかけた。チューリングはすぐに、思考を定義することの難しさを考えると、この問いは「議論に値しないほど無意味だ」と指摘した。
チューリングは「模倣ゲーム」を構想し，人間の審査員がコンピュータと人間の両方と会話し、それぞれが審査員に自分が人間であることを納得させようと競い合うゲームを考えた。コンピュータ、対話相手、審査員は互いに顔を合わせず、テキストでコミュニケーションをとることである。各候補者と会話した後、審査員は誰が本物の人間かを推測する。チューリングの新たな問いは、「模倣ゲームで優れた成績を収めるデジタルコンピュータは想像できるか？」であった。

コンピュータが（外見やその他の身体的特徴を除けば）人間と区別がつかないのであれば、なぜ思考する存在と見なすべきではないのか、というものであった。なぜ「思考する」というステータスを人間（あるいはより一般的には、生物細胞で構成された存在）だけに限定すべきなのかについては，Chauvinism という言葉がある。Carbon chuvinism あるいは meat chuvinism と言う言い方が用いられるが，要は肉食でなければ，あるいは炭素から構成されている存在でなければ，思考は不可能であるという主義主張である。チューリングテストはこのような chuvinism を回避するためにテキストベースでの対話場面を設定したのであろう。

75 年近くが経ち、AI に関する報道は、OpenAI の ChatGPT や Anthropic の Claude といったチャットボットがついにチューリングテストに合格したという宣言で溢れている。

現代のチャットボットは実際にチューリングテストに合格したのだろうか？もし合格したとしたら、チューリングが提唱したように、思考状態を与えるべきなのだろうか？驚くべきことに、チューリングテストの広範な文化的重要性を考えると、AI コミュニティでは合格基準についてほとんど合意が得られておらず、人間を騙せる会話スキルを持つことが、システムの根底にある知性、つまり「思考状態」について何かを明らかにできるかどうかについても、多くの疑問が持たれている。

AI の歴史が私たちに教えてくれたことがあるとすれば、それは、そのような仮定に関する私たちの直感はしばしば間違っているということである。数十年前、多くの著名な AI 専門家は、チェスで人間に勝てる機械を作るには、完全な人間の知能と同等のものが必要だと考えていた。「もし優れたチェスマシンを開発できれば、人間の知的営みの核心に迫ったと言えるだろう」と、AI の先駆者であるアレン・ニューウェルとハーバート・サイモンは 1958 年に記し、認知科学者のダグラス・ホフスタッターは 1979 年に「将来、チェスで誰にも勝てるプログラムが存在するかもしれないが、それは汎用知能のプログラムとなるだろう」と予測した。もちろん、その後 20 年以内に IBM の DeepBlue は、我々が「汎用知能」と呼ぶものからは程遠い、力ずくのアプローチを用いて、チェスの世界チャンピオン、ガルリ・カスパロフを破った。同様に、AI の進歩は、かつて汎用知能が必要と考えられていた課題、つまり音声認識、自然言語翻訳、さらには運転さえも、人間の理解力に全く欠ける機械によって実行可能であることを示している。
<!-- However, if the history of AI has taught us anything, it’s that our intuitions are often wrong about such assumptions. Decades ago, many prominent AI experts believed that creating a machine that could beat humans at chess would require something equivalent to full human intelligence. “If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor,” wrote AI pioneers Allen Newell and Herbert Simon in 1958, and cognitive scientist Douglas Hofstadter predicted in 1979 that in the future, “there may be programs which can beat anyone at chess, but...they will be programs of general intelligence.” Of course within the next two decades IBM’s DeepBlue beat world chess champion Garry Kasparov using a brute-force approach that is far from anything we would call “general intelligence.” Similarly, progress in AI has shown that tasks that were once thought to require general intelligence—speech recognition, natural language translation, and even driving—can be carried out by machines that lack anything like human understanding. -->

チューリングテストは、我々の知能の概念が変化していく中で、またしても犠牲になる可能性が高い。1950 年、チューリングは人間のような会話能力は「思考」とそれに付随するすべてのものの確固たる証拠となるはずだと直感した。この直感は今日でも揺るぎないものである。しかし、ELIZA とユージン・グーストマンから私たちが学んだこと、そして ChatGPT やその類似の技術から今後も学ぶであろうことは、チェスをするように自然言語で流暢に話す能力は、汎用知能の決定的な証拠ではないということである。
<!--It’s likely that the Turing Test will become yet another casualty of our shifting conceptions of intelligence. In 1950, Turing intuited that the ability for humanlike conversation should be firm evidence of “thinking,” and all that goes with it. That intuition is still strong today. But perhaps what we have learned from ELIZA and Eugene Goostman, and what we may still learn from ChatGPT and its ilk, is that the ability to sound fluent in natural language, like playing chess, is not conclusive proof of general intelligence. -->

チューリングは 1950 年の論文で、「今世紀末には言葉の使い方と一般的な教養ある意見が大きく変化し、機械が思考していると語っても反論されることを恐れる必要がなくなるだろうと私は信じている」と記している。しかし、我々はまだその段階には至っていない。チューリングの予測が数十年ほど外れているだけなのか、それとも「思考」とは何かという概念が本当に変わるのか、そして知性というものはチューリングや我々が考えていたよりももっと複雑で微妙なものだという認識が変わるのかはまだ分からない。


## [Retrieval-augmented generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)

<div class="figcenter">
<img src="/2025assets/2021Lewis_RAG_fig1.svg" width="77%;">
<div class="figcaption">

RAG アプローチの概要<br/>
事前学習済みの検索器（クエリ符号化器 + 文書インデックス）と事前学習済みの seq2seq モデル（生成器）を組み合わせ、エンドツーエンドで微調整を行う。クエリ $x$ に対して、最大内積探索（MIPS）を用いて上位 $K$ 個のドキュメント$z_i$ を検索する。最終的な予測値 $y$ については、$z$ を潜在変数として扱い、異なるドキュメントを与えられた seq2seq 予測値に対して周辺化を行う。
<!-- Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.  -->
</div>

<img src="/2025assets/RAG_diagram.svg" width="55%;">
</div><div class="figcaption">

RAG 概要：外部ドキュメントとユーザー入力を LLM プロンプトに統合し、カスタマイズされた出力を生成する。
<!-- Overview of RAG process, combining external documents and user input into an LLM prompt to get tailored output -->
</div>


検索拡張生成（RAG）は、大規模言語モデル（LLM）が新しい情報を検索して組み込むことを可能にする技術である[1]。
RAG とは LLM プロセスを Web 検索またはその他のドキュメント検索プロセスと融合させることで LLM の性能を向上させる方法である。
RAG によりり LLM が事実に忠実になる。

RAG では、LLM は指定された文書セットを参照するまで、ユーザーのクエリに応答しない。これらの文書は、LLM の既存の訓練データからの情報を補完する。これにより、LLM は訓練データでは利用できないドメイン固有の情報や更新された情報を使用することができる。

静的な訓練データに依存する LLM とは異なり、RAG はデータベース、アップロードされたドキュメント、または Web ソースから関連するテキストを取得する[1]。

この方法は、AI の幻覚を軽減するのに役立つ。

RAG は、LLM を新しいデータで再学習させる必要性を軽減し、計算コストと費用を節約する[1]。効率性の向上に加え、RAG は LLM が回答に出典を含めることを可能にするため、ユーザーは引用元を検証できる。これにより、ユーザーは取得したコンテンツを相互に確認し、正確性と関連性を確認できるため、透明性が向上する。
<!-- RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs.[1] Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance. -->


### RAG と LLM の限界<!-- ### RAG and LLM Limitations-->


LLM は誤った情報を提供することがある。例えば Google が LLM ツール Google Bard を初公開した際、LLM はジェームズ・ウェッブ宇宙望遠鏡について誤った情報を提供した。この誤りは同社の株価を 1000 億ドル下落させる一因となった[4]。RAG はこうした誤りを防ぐために用いられるが、全ての問題を解決するわけではない。例えば、事実上正しい情報源から引用しても、文脈を誤って解釈すれば誤った情報を生成する可能性がある。[MIT テクノロジーレビュー](https://en.wikipedia.org/wiki/MIT_Technology_Review)は、AI が生成した「米国にはバラク・フセイン・オバマというムスリム大統領が 1 人いた」という回答を例に挙げている。このモデルは学術書『バラク・フセイン・オバマ：アメリカ初のムスリム大統領か？』から引用した。LLM はタイトルの文脈を「知らず」「理解せず」、虚偽の主張を生成した[2]。
<!-- LLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool "Google Bard", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company’s stock value[4]. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, "The United States has had one Muslim president, Barack Hussein Obama." The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America’s First Muslim President? The LLM did not "know" or "understand" the context of the title, generating a false statement[2]. -->

RAG を備えた LLM は、新規情報を優先するようプログラムされている。この手法は プロンプトスタッフィング prompt stuffing と呼ばれる。プロンプトスタッフィングなしでは、LLM の入力はユーザーによって生成される。プロンプトスタッフィングでは、この入力に追加の関連文脈が加えられ、モデルの応答を誘導する。このアプローチにより、LLM はプロンプトの早い段階で重要な情報を得られ、既存の訓練知識よりも提供されたデータを優先するよう促される[5]。
<!--LLMs with RAG are programmed to prioritize new information. This technique has been called "prompt stuffing." Without prompt stuffing, the LLM's input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide the model’s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge[5]. -->

### プロセス<!-- ### Process-->

検索拡張生成（RAG）は、情報検索メカニズムを組み込むことで大規模言語モデル（LLM）を強化する。これによりモデルは元の訓練データセットを超えた追加データにアクセスし活用できる。Ars Technica は「新たな情報が利用可能になった場合、モデルを再訓練する必要はなく、更新された情報でモデルの外部知識ベースを拡張するだけでよい」と指摘している（拡張）[4]。IBM は「生成フェーズにおいて、LLM は拡張されたプロンプトと、その訓練データの内部表現から引き出し、その瞬間のユーザーに合わせた魅力的な回答を合成する」と述べている。[1]
<!--Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that "when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external knowledge base with the updated information" ("augmentation").[4] IBM states that "in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant".[1] -->


### RAG の主要な段階<!-- ### RAG key stages-->

通常、参照対象となるデータは、大規模なベクトル空間の形式で数値表現された LLM 埋め込みに変換されます。RAG は、非構造化データ（通常はテキスト）、半構造化データ、または構造化データ（例えばナレッジグラフ）に使用できる。これらの埋め込みは、ドキュメント検索を可能にするためにベクトルデータベースに保存される。
<!-- Typically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval. -->

ユーザークエリが与えられると、まずドキュメントリト検索器が呼び出され、クエリを拡張するために使用する最も関連性の高いドキュメントが選択される[2][3]。この比較は、使用されるインデックスの種類に応じてさまざまな方法で行うことができる[1]。
<!-- Given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query[2][3]. This comparison can be done using a variety of methods, which depend in part on the type of indexing used[1]. -->

モデルは、ユーザーの元のクエリを迅速にエンジニアリングすることで、この関連性の高い検索情報を LLM に入力する。新しい実装（2023 年現在）では、クエリを複数のドメインに拡張したり、メモリと自己改善を利用して以前の検索から学習したりする機能を備えた特定の拡張モジュールを組み込むこともできる。
<!-- The model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query. Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals. -->

最後に、LLM はクエリと取得した文書の両方に基づいて出力を生成することができる[2][6]。一部のモデルでは、取得した情報の再ランク付け、コンテキスト選択、微調整など、出力を改善するための追加ステップが組み込まれている。
<!--Finally, the LLM can generate output based on both the query and the retrieved documents[2][6]. Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning. -->

<!--### 改善 Improvements

上述の基本過程の改善には RAG フローの種々の段階で適用されている。
Improvements to the basic process above can be applied at different stages in the RAG flow.

#### 符号化器Encoder

これらの手法は、テキストを密ベクトルまたは疎ベクトルとして符号化することに重点を置いている。単語の同一性を符号化する疎ベクトルは、通常辞書長で、ほとんどがゼロで構成される。意味を符号化する密ベクトルはよりコンパクトで、ゼロの数が少なくなる。様々な機能強化により、ベクトルストア（データベース）における類似度の計算方法を改善できる[7]。
These methods focus on the encoding of text as either dense or sparse vectors. Sparse vectors, which encode the identity of a word, are typically dictionary-length and contain mostly zeros. Dense vectors, which encode meaning, are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).[7] 

* ベクトル類似度の計算方法を最適化することで、性能が向上する。ドット積は類似度得点を強化し、近似最近傍法（ANN）検索は K 近傍法（KNN）検索よりも検索効率を向上させる[8]。
* 後インタラクション（Late Interactions）によって精度が向上する可能性がある。後インタラクションは、検索後に系がより正確に単語を比較することを可能にする。これは、文書のランキングを精緻化し、検索の関連性を向上させるのに役立つ[9]。
* ハイブリッドベクトルアプローチは、密ベクトル表現とスパースなワンホットベクトルを組み合わせるために使用でき、スパースドット積の計算効率を密ベクトル演算よりも活用できる[7]。
* 他の検索手法は、文書の選択方法を改善することで精度を向上させることに重点を置いている。SPLADE などの一部の検索手法では、スパース表現とクエリ拡張戦略を組み合わせることで、検索精度と再現率を向上させる[10]。

* Performance improves by optimizing how vector similarities are calculated. Dot products enhance similarity scoring, while approximate nearest neighbor (ANN) searches improve retrieval efficiency over K-nearest neighbors (KNN) searches.[8]
* Accuracy may be improved with Late Interactions, which allow the system to compare words more precisely after retrieval. This helps refine document ranking and improve search relevance.[9]
* Hybrid vector approaches may be used to combine dense vector representations with sparse one-hot vectors, taking advantage of the computational efficiency of sparse dot products over dense vector operations.[7]
* Other retrieval techniques focus on improving accuracy by refining how documents are selected. Some retrieval methods combine sparse representations, such as SPLADE, with query expansion strategies to improve search accuracy and recall.[10]

#### 検索器中心の手法Retriever-centric methods

これらの手法は、ベクトルデータベースにおける文書検索の品質向上を目的としている。
These methods aim to enhance the quality of document retrieval in vector databases:

* 逆クローズタスク（ICT: Inverse Cloze Task）を用いて検索器を事前学習する。これは、文書内のマスクされたテキストを予測することでモデルが検索パターンを学習するのを支援する手法である[11]。
* 教師あり検索器最適化は、検索確率を生成モデルの尤度分布に一致させる。これは、与えられたプロンプトに対して上位 k 個のベクトルを取得し、生成された応答のパープレキシティをスコアリングし、検索器の選択とモデルの尤度との間の KL ダイバージェンスを最小化することで、検索を改良する[12]。
* 再ランキング手法は、学習中に最も関連性の高い検索文書を優先することで、検索器の性能を向上させることができる[13]。

* Pre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text within documents[11].
* Supervised retriever optimization aligns retrieval probabilities with the generator model’s likelihood distribution. This involves retrieving the top-k vectors for a given prompt, scoring the generated response’s perplexity, and minimizing KL divergence between the retriever’s selections and the model’s likelihoods to refine retrieval[12].
* Reranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training[13].

### 言語モデルLanguage model

<img src="/2025assets/Language_model_in_Deepmind&apos;s_2021_Retro_for_RAG.svg.png" width="22%;">
<div class="figcaption">

RAG 用 Retro 言語モデル。各 Retro ブロックは、注意層、チャンク化交差注意層、フィードフォワード層で構成される。黒文字のボックスは変更されるデータを示し、青文字は変更を実行するアルゴリズムを示す。
 Retro language model for RAG. Each Retro block consists of Attention, Chunked Cross Attention, and Feed Forward layers. Black-lettered boxes show data being changed, and blue lettering shows the algorithm performing the changes.
</div>

検索器を考慮して言語モデルを再設計することで、25 分の 1 の小さなネットワークで、はるかに大きなネットワークと同等のパープレキシティを実現できる[14]。この手法（Retro）はゼロから学習するため、従来の RAG 方式では回避できた高い学習コストが発生する。Retro は学習中にドメイン知識を与えることで、ドメインへの集中度が低くなり、より小さな重みリソースを言語意味論にのみ割り当てることができるという仮説が立てられている。再設計された言語モデルを以下に示す。
By redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts[14]. Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here.

Retro は再現性がないことが報告されているため、再現性を高めるための修正が行われた。より再現性の高いバージョンは Retro++ と呼ばれ、コンテキスト内 RAG[15] を備えている。
It has been reported that Retro is not reproducible, so modifications were made to make it so. The more reproducible version is called Retro++ and includes in-context RAG[15].

### チャンク化 Chunking

チャンキングとは、データをベクトルに分割するための様々な手法を指す。これにより検索システムが詳細情報を見つけられるようになる。
Chunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.

<img src="/2025assets/Rag-doc-styles.png">
<div class="figcaption">

異なるデータ形式には、適切なチャンキングが活用できるパターンが存在する
Different data styles have patterns that correct chunking can take advantage of.
</div>

チャンク化戦略には 3 つの種類がある[要出典]。Three types of chunking strategies are:[citation needed]

* 固定長でオーバーラップさせる。これは高速かつ簡単である。連続するチャンクをオーバーラップさせることで、チャンク間の意味的コンテキストを維持するのに役立つ。
* 構文ベースのチャンクは、ドキュメントを文に分割できる。spaCy や NLTK などのライブラリも役立つ。
* ファイル形式ベースのチャンク化。特定のファイルタイプには自然なチャンクが組み込まれているため、それを尊重するのが最善である。例えば、コードファイルは関数またはクラス全体としてチャンク化し、ベクトル化するのが最適である。HTML ファイルでは、`<table>` 要素または base64 エンコードされた `<img>` 要素をそのまま残す必要がある。PDF ファイルについても同様の考慮が必要である。Unstructured や Langchain などのライブラリは、この方法に役立つ。

* Fixed length with overlap. This is fast and easy. Overlapping consecutive chunks helps to maintain semantic context across chunks.
* Syntax-based chunks can break the document up into sentences. Libraries such as spaCy or NLTK can also help.
* File format-based chunking. Certain file types have natural chunks built in, and it's best to respect them. For example, code files are best chunked and vectorized as whole functions or classes. HTML files should leave <table> or base64 encoded <img> elements intact. Similar considerations should be taken for pdf files. Libraries such as Unstructured or Langchain can assist with this method.

### ハイブリッド検索 Hybrid search

ベクトルデータベース検索では、ユーザーの質問に答えるために必要な重要な事実が見逃されることがある。これを軽減する方法の一つは、従来のテキスト検索を行い、その結果をベクトル検索で取得したベクトルにリンクされたテキストチャンクに追加し、そのハイブリッドテキストを言語モデルに入力して生成することである[要出典]。
<!-- Sometimes vector database searches can miss key facts needed to answer a user's question. One way to mitigate this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the combined hybrid text into the language model for generation.[citation needed]

### 評価とベンチマークEvaluation and benchmarks

RAG システムは、検索可能性、検索精度、生成品質をテストするために設計されたベンチマークを用いて評価されるのが一般的である。よく使用されるデータセットには、多様な分野にわたる情報検索課題群である BEIR や、オープンドメイン QA 用の Natural Questions や Google QA などがある[要出典]。
RAG systems are commonly evaluated using benchmarks designed to test retrievability, retrieval accuracy and generative quality. Popular datasets include BEIR, a suite of information retrieval tasks across diverse domains, and Natural Questions or Google QA for open-domain QA.[citation needed]

### 課題 Challenges

RAG は LLM における幻覚を防ぐものではない。Ars Technica によると、「LLM は依然として応答においてソース資料に関する幻覚を生じる可能性があるため、直接的な解決策ではあない[4]」。
RAG does not prevent hallucinations in LLMs. According to Ars Technica, "It is not a direct solution because the LLM can still hallucinate around the source material in its response."[4]

RAG は大規模言語モデル（LLM）の精度を向上させるが、すべての課題を解消するわけではない。RAG の限界の一つは、頻繁なモデルの再学習の必要性は軽減するものの、完全に排除できるわけではないことである。さらに、LLM は信頼できる応答を提供するのに十分な情報が不足している場合、それを認識するのが困難になる場合がある。特別な学習を行わないと、モデルは不確実性を示すべき場合でも回答を生成する可能性がある。IBM によると、この問題は、モデルが自身の知識の限界を評価できない場合に発生する可能性がある[1]。
While RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to IBM, this issue can arise when the model lacks the ability to assess its own knowledge limitations.[1]

RAG システムは、事実上は正しいものの誤解を招くような情報源を取得し、解釈の誤りにつながる可能性がある。場合によっては、LLM は文脈を考慮せずに情報源から文を抽出し、誤った結論に至ることがある。さらに、矛盾する情報に直面した場合、RAG モデルはどちらの情報源が正確であるかを判断するのに苦労する可能性がある。この制限による最悪の結果は、モデルが複数の情報源からの詳細情報を統合し、古い情報と最新の情報を混在させた誤解を招くような回答を生成する可能性があることである。MIT Technology Review によると、これらの問題は RAG システムが取得したデータを誤って解釈する可能性があるために発生する[2]。
RAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information RAG models may struggle to determine which source is accurate. The worst case outcome of this limitation is that the model may combine details from multiple sources producing responses that merge outdated and updated information in a misleading manner. According to the MIT Technology Review, these issues occur because RAG systems may misinterpret the data they retrieve.[2]

## Reference-->

1. "[What is retrieval-augmented generation?](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)". IBM. 22 August 2023. Retrieved 7 March 2025.
2. "[Why Google's AI Overviews gets things wrong](https://www.technologyreview.com/2024/05/31/1093019/why-are-googles-ai-overviews-results-so-bad/)". MIT Technology Review. 31 May 2024. Retrieved 7 March 2025.
3. Kiela Douwe, Lewis Patrick, Perez Ethan, Piktus Aleksandra, Petroni Fabio, Karpukhin Vladimir, Goyal Naman, Küttler Heinrich, Lewis Mike, Yih Wen-Tau, Rocktäschel Tim, Riedel Sebastian (2020). [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://dl.acm.org/doi/abs/10.5555/3495724.3496517). pp. 9459–9474. arXiv:2005.11401. ISBN 978-1-7138-2954-6.
4. "[Can a technology called RAG keep AI models from making stuff up?](https://arstechnica.com/ai/2024/06/can-a-technology-called-rag-keep-ai-models-from-making-stuff-up/)". Ars Technica. 6 June 2024. Retrieved 7 March 2025.
5. "[Mitigating LLM hallucinations in text summarisation](https://www.bbc.co.uk/rd/articles/2024-06-mitigating-llm-hallucinations-in-text-summarisation)". BBC. 20 June 2024. Retrieved 7 March 2025.
6. Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, 
 Douwe (2020). "[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)". Advances in Neural Information Processing Systems. 33. Curran Associates, Inc.: 9459–9474. arXiv:2005.11401.

<!-- 7. Luan, Yi; Eisenstein, Jacob; Toutanova, Kristina; Collins, Michael (26 April 2021). "[Sparse, Dense, and Attentional Representations for Text Retrieval](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00369/100684/Sparse-Dense-and-Attentional-Representations-for)". Transactions of the Association for Computational Linguistics. 9: 329–345. arXiv:2005.00181. doi:10.1162/tacl_a_00369. Retrieved 15 March 2025.
8. "[Information retrieval](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-information-retrieval)". Microsoft. 10 January 2025. Retrieved 15 March 2025.
9. Khattab, Omar; Zaharia, Matei (2020). "[ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://dl.acm.org/doi/10.1145/3397271.3401075)". Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 39–48. doi:10.1145/3397271.3401075. ISBN 978-1-4503-8016-4.
10.  Wang, Yup; Conroy, John M.; Molino, Neil; Yang, Julia; Green, Mike (2024). "[Laboratory for Analytic Sciences in TREC 2024 Retrieval Augmented Generation Track](https://trec.nist.gov/pubs/trec33/index.html)". NIST TREC 2024. Retrieved 15 March 2025.
11. Lee, Kenton; Chang, Ming-Wei; Toutanova, Kristina (2019). "[Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://aclanthology.org/P19-1612.pdf)" (PDF).
12. Shi, Weijia; Min, Sewon; Yasunaga, Michihiro; Seo, Minjoon; James, Rich; Lewis, Mike; Zettlemoyer, Luke; Yih, Wen-tau (June 2024). "[REPLUG: Retrieval-Augmented Black-Box Language Models](https://aclanthology.org/2024.naacl-long.463/)". Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 8371–8384. arXiv:2301.12652. doi:10.18653/v1/2024.naacl-long.463. Retrieved 16 March 2025.
13. Ram, Ori; Levine, Yoav; Dalmedigos, Itay; Muhlgay, Dor; Shashua, Amnon; Leyton-Brown, Kevin; Shoham, Yoav (2023). "[In-Context Retrieval-Augmented Language Models](https://aclanthology.org/2023.tacl-1.75/)". Transactions of the Association for Computational Linguistics. 11: 1316–1331. arXiv:2302.00083. doi:10.1162/tacl_a_00605. Retrieved 16 March 2025.
14. Borgeaud, Sebastian; Mensch, Arthur (2021). "[Improving language models by retrieving from trillions of tokens](https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf)" (PDF).
15. Wang, Boxin; Ping, Wei; Xu, Peng; McAfee, Lawrence; Liu, Zihan; Shoeybi, Mohammad; Dong, Yi; Kuchaiev, Oleksii; Li, Bo; Xiao, Chaowei; Anandkumar, Anima; Catanzaro, Bryan (2023). "[Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study](https://aclanthology.org/2023.emnlp-main.482/)". Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 7763–7786. doi:[10.18653/v1/2023.emnlp-main.482](https://doi.org/10.18653%2Fv1%2F2023.emnlp-main.482). -->
