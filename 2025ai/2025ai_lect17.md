---
title: 第17回 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\BRc}[1]{\left[#1\right]}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+2" color="navy"><strong>2025 年度開講 駒澤大学 人工知能 I および II</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 03/Oct./2025<br/>
Appache 2.0 license<br/>
</div>

# 第 17 回 後期第 3 回

* [課題提出用フォルダ](https://drive.google.com/drive/u/5/folders/1_MvPyHi3wWQD-SOpTRd1vXidz121N_9W){:target="_blank"}


### デモ

* [漱石「こころ」冒頭部分を文字ベースリカレントニューラルネットワークで言語モデル javascript 版](/character_demo.html){:target="_blank"}

### 実習ファイル

* [カルマンフィルタ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_1003kalman_filter_olivetti_face.ipynb){:target="_blank"}
* [百人一首の上の句とエンコーダによって符号化し，下の句をデコーダで生成する自作 Transformer モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}

* [三夕の歌 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0925RNN_3twilight_poetries.ipynb){:target="_blank"}

* [日本国憲法第 9 条をリカレントニューラルネットワークで理解する <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/){:target="_blank"}
* [雪女 (小泉八雲 原作，青空文庫より) を用いた RNN <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0928pytorch_charRNN_demo.ipynb){:target="_blank"}

* [足し算モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb){:target="_blank"}
<!-- こちらの足し算モデルは未完成 - [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}-->

- [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb){:target="_blank"}

- [Bahdanau and Loung attentions <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb){:target="_blank"}
* [Attention is all you need <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022The_Annotated_%22Attention_is_All_You_Need%22.ipynb){:target="_blank"}


<!-- - 前回できなかった [GAN のデモ TL-GAN (transparent latent-space GAN)<img src="/assets/kaggle-site-logo.png" style="width:09%">](https://www.kaggle.com/summitkwan/tl-gan-demo){target="_blank"} -->
<!--- <a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_SRN_simulator.ipynb">2019cnps_SRN_simulator<img src="/assets/colab_icon.svg"></a>
master/notebooks/2020_0619SRN_simulator.ipynb){:target="_blank"}
<!-- - [書画のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619sketch_RNN.ipynb){:target="_blank"} -->
<!-- - [word2vecのデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb){:target="_blank"} -->


# キーワード

状態空間モデル，カルマンフィルタ，フィルタリング，予測，平準化，再帰型ニューラルネットワーク

# 系列予測モデルのつづき 状態空間モデル

## カルマンフィルタ Kalman filter

<div class="figcenter">
<img src="/2025assets/residual_chart_with_h_jp.png" width="77%;">
<!-- <img src="/2025assets/04-One-Dimensional-Kalman-Filters_11_jp.png" width="77%;"> -->
</div>

状態方程式: $x_t = G_t x_{t-1} + w_t$,

観測方程式: $y_t = F_t x_{t} + v_t$,

ここで $w$ と $v$ はノイズであり以下の正規分布に従うものとする: $w_t\sim\mathcal{N}(0,W_t), v_t\sim\mathcal{N}(0,V_t)$ 

手順
1. 現在の状態 $x_t$ から，$y_t$ を $F_t x_t + v_t$ を用いて予測する
2. 現在の観測値 $y_t$ を得て，状態 $x_t$ を $x_t=G_t x_{t-1} + w_t$ 用いて更新する

詳しくは [カルマンフィルタ詳説](/2025ai/kalman_filter){:target="_blank"} を参照

# 行列表現

* $\mb{I}$: 単位行列
* $\mb{A}^{\top}$: 行列 $\mb{A}$ の転置 (transposed)
* $\mb{A}^{-1}$: 行列 $\mb{A}$ の逆行列 $\mb{AA}^{-1}=$\mb{I}$
* $\mb{AB}$: 行列の積 ドット積とも呼ばれる。行列 $\mb{A}$ の行数 と列数 を n, m とすると，$\mb{AB}$ が成り立つためには B の列数が m でなければならなない。

* 一次元の行列，列数または行数のいずれかが 1 である行列をベクトルと呼ぶ。
* 回帰とは $\mb{y}=\mb{Xw}$ と表現できる。仮にデータ行列 $\mb{X}$ が各列の平均値を引いてある行列，平均偏差行列である場合には

$$\begin{aligned}
\mb{y} &= \mb{Xw} &\\
\mb{X}^{\top}\mb{y} &= \mb{X}^{\top}\mb{Xw}&\text{: $\mb{X}$ の転置行列を両辺の左から掛ける}\\
(\mb{X}^{\top}\mb{X})^{-1}\mb{X}^{\top}\mb{y} &= (\mb{X}^{\top}\mb{X})^{-1}(\mb{X}^{\top}\mb{X})\mb{w}&\text{: $\mb{X}^{\top}\mb{X}$ の逆行列を両辺の左から掛ける}\\
(\mb{X}^{\top}\mb{X})^{-1}\mb{X}^{\top}\mb{y} &= \mb{I}\mb{w}&\text{: 行列の逆行列の積は単位行列になる}\\
(\mb{X}^{\top}\mb{X})^{-1}\mb{X}^{\top}\mb{y} &= \mb{w} &\text{: 左辺の値が $\mb{w}$ の値，直線の方程式の傾きに相当}
\end{aligned}$$

従って，上式最下行の左辺を最上行の $\mb{w}$ に代入すれば $\mb{y}$ の推定値 $\hat{\mb{y}}=\mb{X}\left(\mb{X}^{\top}\mb{X}\right)^{-1}\mb{X}^{\top}\mb{y}$ を得る。<br/>
$\mb{P}=\mb{X}\left(\mb{X}^{\top}\mb{X}\right)^{-1}\mb{X}^{\top}$ のことを射影行列と呼ぶ。<br/>
射影行列の定義とは
1. 冪等性 (ベキトウセイ) $\mb{P}^{2}=\mb{P}$
2. $\mb{P}^{\top}=\mb{P}$ 
である。

また，$\mb{X}$ の各列の平均を引いた行列 （平均偏差行列）をあらためて $\mb{X}$ とすれば，$\mb{X}^{\top}\mb{X}$ は分散共分散行列である。<br/>
すなわち 各列を変数とみなせば，各変数の相関係数 （正しくは，そのデータ数倍）である。


# 自然言語処理

## 自然言語処理前史

1. 第一次ブーム 1960 年代
極度の楽観論: 辞書を丸写しすれば翻訳は可能だと思っていた，らしい...
2. 第二次ブーム 統計的自然言語処理
* [統計的言語モデル statistical language model](https://en.wikipedia.org/wiki/Language_model){:target="_blank"}
* [Chris Manning (スタンフォード大学)](https://nlp.stanford.edu/manning/){:target="_blank"} and Schutze (1999) 著。
* [教科書 Fundations of Statistical Natural Language Processing](https://nlp.stanford.edu/fsnlp/){:target="_blank"}, あるいは [こちら](https://nlp.stanford.edu/fsnlp/promo/){:target="_blank"}
* [Jurafsky 著 もう一つ定評の教科書](https://web.stanford.edu/~jurafsky/){:target="_blank"}
* [Martin 著 Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) は [改訂版](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf){:target="_blank"} が出版された。
ニューラルネットワークによる言語モデルも掲載されている。

## 用語解説

* [Bow](https://en.wikipedia.org/wiki/Bag-of-words_model){:target="_blank"}: Bag of Words 単語の袋。ある文章を表現する場合に，各単語の表現を集めて袋詰めしたとの意味。
従って語順が考慮されない。
「犬が男を噛んだ」 と「男が犬を噛んだ」では同じ表現となる。
LSA, LDA, fastText なども同じような表現を与える。
* TF-IDF: 単語頻度 (Term Frequency) と 逆(Inverse) 文書頻度 (Document Frequency) で文書のベクトル表現を定義する手法。
何度も出現する単語は重要なので単語頻度が高い文書には意味がある。
一方，全ての文書に出現する単語は重要とは言えないので単語の出現る文書の個数の逆数の対数変換を用いる。
このようにしてできた文章表現を TF-IDF と呼ぶ。
* ワンホット表現:
ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。
一つだけが "熱い" あるいは "辛い" ベクトルという呼び方であるが，以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていた。
最近では，ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派) と呼ばれることが多い。
ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じる。
典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新する。
従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになる。
そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行った。
上の Elman ネットによる文法学習において，ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当する。
単語埋め込み層を用いることで学習効率が改善し，word2vec などの **分散ベクトルモデル** へと発展する。
* 埋め込み表現: すべての要素が実数であるベクトルで表されるニューラルネットワークのある層の状態。


