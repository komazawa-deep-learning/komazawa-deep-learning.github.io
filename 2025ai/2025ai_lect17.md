---
title: 第17回 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\BRc}[1]{\left[#1\right]}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+2" color="navy"><strong>2025 年度開講 駒澤大学 人工知能 I および II</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 03/Oct./2025<br/>
Appache 2.0 license<br/>
</div>

# 第 17 回 後期第 3 回

* [課題提出用フォルダ](https://drive.google.com/drive/u/5/folders/1_MvPyHi3wWQD-SOpTRd1vXidz121N_9W){:target="_blank"}


### デモ

* [漱石「こころ」冒頭部分を文字ベースリカレントニューラルネットワークで言語モデル javascript 版](/character_demo.html){:target="_blank"}

### 実習ファイル

* [カルマンフィルタ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_1003kalman_filter_olivetti_face.ipynb){:target="_blank"}
* [三夕の歌 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0925RNN_3twilight_poetries.ipynb){:target="_blank"}
- [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb){:target="_blank"}
* [雪女 (小泉八雲 原作，青空文庫より) を用いた RNN <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0928pytorch_charRNN_demo.ipynb){:target="_blank"}

<!-- * [足し算モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb){:target="_blank"} -->
<!-- こちらの足し算モデルは未完成 - [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}-->

<!-- * [百人一首の上の句とエンコーダによって符号化し，下の句をデコーダで生成する自作 Transformer モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}

- [Bahdanau and Loung attentions <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb){:target="_blank"}
* [Attention is all you need <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022The_Annotated_%22Attention_is_All_You_Need%22.ipynb){:target="_blank"} -->


<!-- - 前回できなかった [GAN のデモ TL-GAN (transparent latent-space GAN)<img src="/assets/kaggle-site-logo.png" style="width:09%">](https://www.kaggle.com/summitkwan/tl-gan-demo){target="_blank"} -->
<!--- <a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_SRN_simulator.ipynb">2019cnps_SRN_simulator<img src="/assets/colab_icon.svg"></a>
master/notebooks/2020_0619SRN_simulator.ipynb){:target="_blank"}
<!-- - [書画のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619sketch_RNN.ipynb){:target="_blank"} -->
<!-- - [word2vecのデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb){:target="_blank"} -->


# キーワード

状態空間モデル，カルマンフィルタ，フィルタリング，予測，平準化，再帰型ニューラルネットワーク

# 系列予測モデルのつづき 状態空間モデル

## カルマンフィルタ Kalman filter

<div class="figcenter">
<img src="/2025assets/residual_chart_with_h_jp.png" width="77%;">
<!-- <img src="/2025assets/04-One-Dimensional-Kalman-Filters_11_jp.png" width="77%;"> -->
</div>

状態方程式: $x_t = G_t x_{t-1} + w_t$,

観測方程式: $y_t = F_t x_{t} + v_t$,

ここで $w$ と $v$ はノイズであり以下の正規分布に従うものとする: $w_t\sim\mathcal{N}(0,W_t), v_t\sim\mathcal{N}(0,V_t)$ 

手順
1. 現在の状態 $x_t$ から，$y_t$ を $F_t x_t + v_t$ を用いて予測する
2. 現在の観測値 $y_t$ を得て，状態 $x_t$ を $x_t=G_t x_{t-1} + w_t$ 用いて更新する

詳しくは [カルマンフィルタ詳説](/2025ai/kalman_filter){:target="_blank"} を参照

## ベイズ則とパラメータ推定

ベイズ則とは次式をす：<br/>
$$
P(y\vert x)=\frac{P(x\vert y)P(y)}{P(x)}=\frac{P(x\vert y)P(y)}{\sum_{y\in Y}P(x,y)} 
$$

原因と結果が $x$ と $y$ とで与えられている時，因果関係を推論する際に用いられます。

$x$ を $\mathcal{D}$ に置き換えて観測データを，$y$ を $\theta$ に置き換えてモデルの未知パラメータを，全ての項を $m$ (我々が考える確率モデルのクラス) で条件付けることで，確率論を機械学習に適用することができる。
学習の場合，次のようになる。

$$ P(\theta\vert \mathcal{D},m)=\frac{P(\mathcal{D}\vert \theta,m) P(\theta\vert m)}{P(\mathcal{D}\vert m)} $$

ここで、$P(D\vert\theta,m)$ はモデル $m$ におけるパラメータ $\theta$ の尤度，$P( \theta\vert m)$ は$\theta$ の事前確率，$P( \theta\vert \mathcal{D}, m)$ はデータ $\mathcal{D}$ を与えたときの $\theta$ の事後確率である。

例えば，データ $\mathcal{D}$ は自動運転における車載カメラから動画像であり時系列データとみなしうる。モデルは最終目的地まで安全に移動することであるとすると，パラメータ $\theta$ は時間と空間の相関をモデリングする。
学習とは，データ $\mathcal{D}$ を通して，パラメータ $P( \theta\vert m)$ に関する事前知識または仮定を，パラメータに関する事後知識 $P( \theta\vert\mathcal{D},m)$ に変換することである。
この事後知識は，将来のデータに対して使用される事前知識となる。
学習されたモデルは、新しい未知のテストデータ $\mathcal{D}_\text{test}$ に対して，上式を適用して予測を得るだけで，予測や予想に用いることができる。
（ベイズ最適化やデータ同化などの分野と関連）

$$ P(\mathcal{D}_{\text{test}}\vert \mathcal{D},m) = \int P(\mathcal{D}_{\text{test}}\vert \theta, \mathcal{D}, m) P(\theta\vert \mathcal{D},m) d\theta $$

最後に $m$ のレベルでベイズ則を適用することで，異なるモデルを比較することができる。

$$\begin{aligned}
P(m|\mathcal{D}) &= \frac{P(\mathcal{D}|m)P(m)}{P(\mathcal{D})}\\
P(\mathcal{D}|m) &= \int P(\mathcal{D}|\theta,m) P(\theta|m) d\theta
\end{aligned}$$

$P(\mathcal{D}\mid m)$ は周辺尤度またはモデル証拠であり，ベイズのオッカムの剃刀 (Occams' razer) として知られるより単純なモデルの優先順位を実装している。

## エントロピー Entropy

エントロピーとは次式で定義される:<br/>
$$
H = - \sum_i - p_i \log p_i,
$$

ここで，$p_i$ は確率を表す。
換言すれば個々の値を対数変換した値の平均値とみなすことができる。

この量がなぜ情報量と呼ばれるのか，情報科学におけるエントロピーと物理学におけるエントロピーとで同じ言葉が使われる理由についても興味深い逸話が残されている。Neumann と Shannon の会話である。

エントロピーの定義式を確認すれば，同時エントロピー $\displaystyle -\sum p(x,y) \log p(x,y)$ や条件付きエントロピー $\displaystyle -\sum p(x\| y)\log p(x\| y)$ などは自然な拡張であることがわかる。

また次式，相互情報量はピアソンの籍率相関係数と形式的には同じであることが見て取れる。

$$
I(X;Y) = \sum_{i\in I}\sum_{j\in J}p(x_i,y_j)\log\frac{p(x_i,y_i)}{p(x_i) p(y_j)}
$$

一方で KL ダイバージェンスは，次式で定義される。<br/>
$$\begin{aligned}
D_{KL}(P||Q) &= - \sum_i p(x_i)\log\frac{p(x_i)}{q(x_i)} \\
&= -\left(\sum_i p(x_i)\log p(x_i) - \sum_i p(x_i)\log q(x_i)\right)\\
&= H(x) + \sum_i p(x_i)\log q(x_i)
\end{aligned}$$



# 行列表現

* $\mb{I}$: 単位行列
* $\mb{A}^{\top}$: 行列 $\mb{A}$ の転置 (transposed)
* $\mb{A}^{-1}$: 行列 $\mb{A}$ の逆行列 $\mb{AA}^{-1}=$\mb{I}$
* $\mb{AB}$: 行列の積 ドット積とも呼ばれる。行列 $\mb{A}$ の行数 と列数 を n, m とすると，$\mb{AB}$ が成り立つためには B の列数が m でなければならなない。

* 一次元の行列，列数または行数のいずれかが 1 である行列をベクトルと呼ぶ。
* 回帰とは $\mb{y}=\mb{Xw}$ と表現できる。仮にデータ行列 $\mb{X}$ が各列の平均値を引いてある行列，平均偏差行列である場合には

$$\begin{aligned}
\mb{y} &= \mb{Xw} &\\
\mb{X}^{\top}\mb{y} &= \mb{X}^{\top}\mb{Xw}&\text{: $\mb{X}$ の転置行列を両辺の左から掛ける}\\
(\mb{X}^{\top}\mb{X})^{-1}\mb{X}^{\top}\mb{y} &= (\mb{X}^{\top}\mb{X})^{-1}(\mb{X}^{\top}\mb{X})\mb{w}&\text{: $\mb{X}^{\top}\mb{X}$ の逆行列を両辺の左から掛ける}\\
(\mb{X}^{\top}\mb{X})^{-1}\mb{X}^{\top}\mb{y} &= \mb{I}\mb{w}&\text{: 行列の逆行列の積は単位行列になる}\\
(\mb{X}^{\top}\mb{X})^{-1}\mb{X}^{\top}\mb{y} &= \mb{w} &\text{: 左辺の値が $\mb{w}$ の値，直線の方程式の傾きに相当}
\end{aligned}$$

従って，上式最下行の左辺を最上行の $\mb{w}$ に代入すれば $\mb{y}$ の推定値 $\hat{\mb{y}}=\mb{X}\left(\mb{X}^{\top}\mb{X}\right)^{-1}\mb{X}^{\top}\mb{y}$ を得る。<br/>
$\mb{P}=\mb{X}\left(\mb{X}^{\top}\mb{X}\right)^{-1}\mb{X}^{\top}$ のことを射影行列と呼ぶ。<br/>

射影行列の条件とは以下の 2 つを満たす行列のことである:
1. $\mb{P}^{2}=\mb{P}$: 冪等性 (ベキトウセイ) 
2. $\mb{P}^{\top}=\mb{P}$ : 転置しても同じ行列，対称行列


また，$\mb{X}$ の各列の平均を引いた行列 （平均偏差行列）をあらためて $\mb{X}$ とすれば，$\mb{X}^{\top}\mb{X}$ は分散共分散行列である。<br/>
すなわち 各列を変数とみなせば，各変数の相関係数 （正しくは，そのデータ数倍）である。


# 自然言語処理

## 自然言語処理前史

1. 第一次ブーム 1960 年代
極度の楽観論: 辞書を丸写しすれば翻訳は可能だと思っていた，らしい...
2. 第二次ブーム 統計的自然言語処理
* [統計的言語モデル statistical language model](https://en.wikipedia.org/wiki/Language_model){:target="_blank"}
* [Chris Manning (スタンフォード大学)](https://nlp.stanford.edu/manning/){:target="_blank"} and Schutze (1999) 著。
* [教科書 Fundations of Statistical Natural Language Processing](https://nlp.stanford.edu/fsnlp/){:target="_blank"}, あるいは [こちら](https://nlp.stanford.edu/fsnlp/promo/){:target="_blank"}
* [Jurafsky 著 もう一つ定評の教科書](https://web.stanford.edu/~jurafsky/){:target="_blank"}
* [Martin 著 Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) は [改訂版](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf){:target="_blank"} が出版された。
ニューラルネットワークによる言語モデルも掲載されている。

## 用語解説

* [Bow](https://en.wikipedia.org/wiki/Bag-of-words_model){:target="_blank"}: Bag of Words 単語の袋。ある文章を表現する場合に，各単語の表現を集めて袋詰めしたとの意味。
従って語順が考慮されない。
「犬が男を噛んだ」 と「男が犬を噛んだ」では同じ表現となる。
LSA, LDA, fastText なども同じような表現を与える。
* TF-IDF: 単語頻度 (Term Frequency) と 逆(Inverse) 文書頻度 (Document Frequency) で文書のベクトル表現を定義する手法。
何度も出現する単語は重要なので単語頻度が高い文書には意味がある。
一方，全ての文書に出現する単語は重要とは言えないので単語の出現る文書の個数の逆数の対数変換を用いる。
このようにしてできた文章表現を TF-IDF と呼ぶ。
* ワンホット表現:
ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。
一つだけが "熱い" あるいは "辛い" ベクトルという呼び方であるが，以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていた。
最近では，ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派) と呼ばれることが多い。
ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じる。
典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新する。
従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになる。
そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行った。
上の Elman ネットによる文法学習において，ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当する。
単語埋め込み層を用いることで学習効率が改善し，word2vec などの **分散ベクトルモデル** へと発展する。
* 埋め込み表現: すべての要素が実数であるベクトルで表されるニューラルネットワークのある層の状態。

<!-- ## 2.1. NETtalk
系列情報処理を扱った初期のニューラルネットワーク例として NETTalk が挙げられます。
NETTalk[^NETTalk] は文字を音読するネットワークです。下図のような構成になっています。
下図のようにアルファベット 7 文字を入力して，空白はアンダーラインで表現されています，中央の文字の発音を学習する 3 層のニューラルネットワークです。NETTalk は 7 文字幅の窓を移動させながら
逐次中央の文字の発音を学習しました。たとえば /I ate the apple/ という文章では
"the" を "ザ" ではなく "ジ" と発音することになります。

印刷単語の読字過程のニューラルネットワークモデルである SM89[^SM89], PMSP96[^PMSP96] で用いられた発音表現は <a target="_blank" href="https://en.wikipedia.org/wiki/ARPABET">ARPABET</a> の亜種です。Python では `nltk` ライブラリを使うと ARPABET の発音を得ることができます(<a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_arpabet_test.ipynb">ARPABET のデモ<img src="/assets/colab_icon.svg"></a>)。

[^NETTalk]: Sejnowski, T.J. and Rosenberg, C. R. (1987) Parallel Networks that Learn to Pronounce English Text, Complex Systems 1, 145-168.
[^SM89]: Seidenberg, M. S. & McClelland, J. L. (1989). A distributed, developmetal model of word recognition and naming. Psychological Review, 96(4), 523–568.
[^PMSP96]: Plaut, D. C., McClelland, J. L., Seidenberg, M. S. & Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103, 56–115.

<center>
<img src="/assets/1986Sejnowski_NETtalkFig2.svg" style="width:47%"><br/>
Sejnowski (1986) Fig. 2
</center> -->

# 単純再帰型ニューラルネットワーク (SRN: Simple Recurrent Neural Networks)

<!-- NETTalk を先がけとして **単純再帰型ニューラルネットワーク** Simple Recurrent Neural networks (SRN) が提案された。 -->
発案者の名前で **Jordan ネット**，**Elman ネット** と呼ばれる。
<!-- [JordanNet]: Joradn, M.I. (1986) Serial Order: A Parallel Distributed Processing Approach, UCSD tech report.
[ElmanNet]: Elman, J. L. (1990)Finding structure in time, Cognitive Science, 14, 179-211. -->
Jordan ネットも Elman ネットも上位層からの **帰還信号** を持つ。
これを **フィードバック結合** と呼び，位置時刻前の状態が次の時刻に使われる。
Jordan ネットでは一時刻前の出力層の情報が用いられる (下図)。
一方，Elman ネットでは一時刻前の中間層の状態がフィードバック信号として用いられる。

<center>
<img src="/assets/SRN_J.svg" style="width:47%"><br/>
<div style="width:74%" align="center">
図：マイケル・ジョーダン発案ジョーダンネット [@1986Jordan]
</div>
</center>

<!-- - 駄菓子菓子 <a target="_blank" href="/assets/MJ_air.jpg">彼（マイケル・ジェフェリー(エアー)・ジョーダン）</a> ではない :)
- <a target="_blank" href="/assets/c3-s4-jordan.jpg">マイケル・アーウィン・ジョーダン。ミスター機械学習[^jordan_ai_revolution_not_yet]</a> -->
<!-- [^jordan_ai_revolution_not_yet]: 彼は(も？)神様です。多くの機械学習アルゴリズムを提案し続けている影響力のある人です。長らく機械学習の国際雑誌の編集長でした。2018年 <a target="_blank" href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">AI 革命は未だ起こっていない</a> と言い出して議論を呼びました。
-->

<center>
<img src="/assets/SRN_E.svg" style="width:47%"><br/>
<div style="align:center; width:47%">
図：ジェフ・エルマン発案のエルマンネット[@lman1990],[@Elman1993]
</div>
</center>

どちらも一時刻前の状態を短期記憶として保持して利用するのですが，実際の学習では一時刻前の状態をコピーして保存しておくだけで，実際の学習では通常の **誤差逆伝播法** すなわちバックプロパゲーション法が用いられる。
上 2 つの図に示したとおり U と W とは共に中間層への結合係数であり，V は中間層から出力層への結合係数である。
Z=I と書き点線で描かれている矢印はコピーするだけですので学習は起こりません。このように考えれば SRN は 3 層のニューラルネットワークであることが分かる。

SRN はこのような単純な構造にも関わらず **チューリング完全** であろうと言われてきた。
すなわちコンピュータで計算可能な問題はすべて計算できるくらい強力な計算機だという意味である。

- Jordan ネットは出力層の情報を用いるため **運動制御** に
- Elan ネットは内部状態を利用するため **言語処理** に

それぞれ用いられる。
従って **失行** aparxia (no matter what kind of apraxia such as 'ideomotor' or 'conceptual')，**行為障害** のモデルを考える場合 Jordan ネットは考慮すべき選択肢の候補の一つとなるだろう。

## リカレントニューラルネットワークの時間展開

一時刻前の状態を保持して利用する SRN は下図左のように描くことができる。
同時に時間発展を考慮すれば下図右のように描くことも可能である。

<center>
<img src="/assets/RNN_fold.svg" style="width:49%"><br/>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてみよ。
あるいは同義だ上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してほしい。
このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かる。

<!-- ## 2.4. エルマンネットによる言語モデル

下図に <a target="_blank" href="/assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示しました。
図中の数字はニューロンの数を表します。入力層と出力層のニューロン数 26 とは，もちいた語彙数が 26 であったことを表します。

<center>
<img src="/assets/1991Elman_starting_small_Fig1.svg" style="width:47%"><br/>
from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示しました。Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示されました(Elman, 1990, 1991, 1993)。

- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | lives

これらの規則にはさらに 2 つの制約があります。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもとらなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というものです。

下図に訓練後の中間層の状態を主成分分析にかけた結果を示しました。"boy chases boy", "boy sees boy", および "boy walks" という文を逐次入力した場合の遷移を示しています。
同じ文型の文章は同じような状態遷移を辿ることが分かります。 -->

<!-- <center>
<img src="/assets/1991Elman_Fig3.jpg" style="width:49%"><br/>
<p align="left" style="width:47%">
<!-- Trajectories through state space for sentences boy chases boy, boy sees boy, boy walks.
Principal component 1 is plotted along the abscissa; principal component 3 is plotted along the ordinate.
These two PC’s together encode differences in verb-argument expectations.
</p>
</center> -->

<!-- <img src="/assets/1991Elman_Fig4a.jpg" style="width:84%"><br> -->

<!-- 下図は文 "boy chases boy who chases boy" を入力した場合の遷移図です。この文章には単語 "boy" が 3 度出てきます。それぞれが異なるけれど，他の単語とは異なる位置に附置されていることがわかります。
同様に 'chases" が 2 度出てきますが，やはり同じような位置で，かつ，別の単語とは異なる位置に附置されています。<br/>

<center>
<img src="/assets/1991Elman_Fig4b.jpg" style="width:49%"><br/>
</center>

同様にして "boy who chases boy chases boy" (男の子を追いかける男の子が男の子を追いかける) の状態遷移図を下図に示しました。<br/>
<center>

<img src="/assets/1991Elman_Fig4c.jpg" style="width:48%"><br/>
</center>

さらに複雑な文章例 "boy chases boy who chases boy who chases boy" の状態遷移図を下図に島します。</br>
<center>

<img src="/assets/1991Elman_Fig4d.jpg" style="width:48%"><br/>
</center>

Elman ネットが構文，文法処理ができるということは上図のような中間層での状態遷移で同じ単語が異なる文位置で異なる文法的役割を担っている場合に，微妙に異なる表象を，図に即してで言えば， 同じ単語では，同じような場所を占めるが，その文法的役割によって異なる位置を占めることが示唆されます。
このことから中間層の状態は異なる文章の表現を異なる位置として表現していることが考えられ，後述する **単語の意味** や **自動翻訳** などに使われることに繋がります(浅川の主観半分以上) -->

<!--
<p align="left" style="width:74%">
Movement through state space for sentences with relative clauses. Principal component 1 is displayed along the abscissa; principal component 11 is displayed along the ordinate. These two PC’s encode depth of embedding in relative clauses.
</p>
</center>
-->

<!-- ## 2.5. Seq2sep 翻訳モデル

上記の中間層の状態を素直に応用すると **機械翻訳** や **対話** のモデルになります。
下図は初期の翻訳モデルである "seq2seq" の概念図を示しました。
"`<eos>`" は文末 end of sentence を表します。中央の "`<eos>`" の前がソース言語であり，中央の "`<eos>`" の後はターゲット言語の言語モデルである SRN の中間層への入力として用います。

注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がないことです。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことです。
この点を改善することを目指すことが 2014 年以降盛んに行われてきました。
顕著な例が後述する **双方向 RNN**， **LSTM** を採用したり，**注意** 機構を導入することでした。 -->

<!--
<center>
<img src="/assets/RNN_fold.svg" style="width:94%"></br>
Time unfoldings of recurrent neural networks
</center>
-->

<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" style="width:88%"><br/>
From [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$
\mbox{argmax}_{\theta}
\left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)
$$ -->

## 多様な RNN とその万能性
双方向 RNN や LSTM を紹介する前に，カルパシーのブログから下図に引用する。
下の 2 つ図ではピンク色が入力層，緑が中間層，青が出力層を示している。

<!-- [^karpathy]: 去年までスタンフォード大学の大学院生。
現在はステラ自動車，イーロン・マスクが社長，の AI 部長さんです。図は彼のブログから引用です。蛇足ですがブログのタイトルが unreasonable effectiveness of RNN です。
過去の偉大な論文 Wiegner (1960), Hamming (1967), Halevy (2009) からの <del>パクリ</del> **敬意を表したオマージュ**です。
"unreasonable effectiveness of [science|mathematics|data]" $\ldots$ www -->

<center>
<img src="/assets/diags.jpeg" style="width:77%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

- 上図最左は通常の多層ニューラルネットワークで画像認識，分類，識別問題に用いられます。
- 上図左から 2 つ目は，画像からの文章生成
- 上図中央，左から 3 つ目は，極性分析，文章のレビュー，星の数推定
- 上図右から 2 つ目は翻訳や文章生成
- 上図最右はビデオ分析，ビデオ脚注付け

などに用いられます。これまで理解を促進する目的で中間層をただ一層として描いてきた。
だが中間層は多層化されていることの方が多いこと，中間層各層のニューロン数は 1024 程度まで用いられていることには注意。

数は各層のニューロン数が 4 つである場合の数値例を示しています。入力層では **ワンホット** 表現が用いられている。
<!-- [^onehot]: ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。一つだけが "熱い" あるいは "辛い" ベクトルと呼びます。以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていたのですが ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派)と呼ばれることが多いです。ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じます。典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新します。従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになります。そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行いました。上のエルマンネットによる文法学習において,ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当します。単語埋め込み層を用いることで学習効率が改善し，後に示す word2vec などの **分散ベクトルモデル** へと発展します。 -->


<!--
## 実習
<!-- - [実習 画像認識 Keras 版](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kera_CNN_demo_with_wordnet_ja.ipynb) -->
<!-- - [実習 Keras CNN](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb){:target="_blank"} -->
<!-- - [実習 PyTorch CNN](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0416PyTorch_CNN_demo.ipynb){:target="_blank"} -->
<!-- - [実習 Kermack McKendric model](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}-->
<!--- [実習](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb#scrollTo=oD497lby40Fp)-->

