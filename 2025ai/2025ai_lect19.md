---
title: 第19回 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\BRc}[1]{\left[#1\right]}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+2" color="navy"><strong>2025 年度開講 駒澤大学 人工知能 I および II</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 17/Oct./2025<br/>
Appache 2.0 license<br/>
</div>

# 第 19 回 後期第 5 回

* [課題提出用フォルダ](https://drive.google.com/drive/u/6/folders/15wv7al3OK7up_iZt5pXQq_j1Lnt2Y8w9){:target="_blank"}

### キーワード

LSTM, ゲート，注意，エンコーダ・デコーダモデル，

### 実習ファイル

* [オリベッティ顔データを用いた LSTM と SRN 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_1017olivetti_face_LSTM_SRN.ipynb){:target="_blank}
* [足し算モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb){:target="_blank"}
<!-- こちらの足し算モデルは未完成 - [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}-->
* [加算型注意 (Bahdanau) と 内積型注意 (Loung) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_1017Enc_Dec_models_with_attention.ipynb){:target="_blank"}

<!-- * [百人一首の上の句とエンコーダによって符号化し，下の句をデコーダで生成する自作 Transformer モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}-->

<!--* [Attention is all you need <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022The_Annotated_%22Attention_is_All_You_Need%22.ipynb){:target="_blank"} -->

<!-- - 前回できなかった [GAN のデモ TL-GAN (transparent latent-space GAN)<img src="/assets/kaggle-site-logo.png" style="width:09%">](https://www.kaggle.com/summitkwan/tl-gan-demo){target="_blank"} -->
<!--- <a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_SRN_simulator.ipynb">2019cnps_SRN_simulator<img src="/assets/colab_icon.svg"></a>
master/notebooks/2020_0619SRN_simulator.ipynb){:target="_blank"}
<!-- - [書画のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619sketch_RNN.ipynb){:target="_blank"} -->
<!-- - [word2vecのデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb){:target="_blank"} -->


# キーワード

単純再帰型ニューラルネットワーク (SRN), LSTM, ワンホットベクトル，埋め込みベクトル，前期の復習 勾配降下法，ソフトマックス関数，[前期第 9 回資料](/2025ai/2025ai_lect09){:target="_blank"} を参照


### 多様な RNN とその万能性

<!-- 双方向 RNN や LSTM を紹介する前に，カルパシーのブログから下図に引用する。 -->
下の 2 つ図ではピンク色が入力層，緑が中間層，青が出力層を示している。

<!-- [^karpathy]: 去年までスタンフォード大学の大学院生。
現在はステラ自動車，イーロン・マスクが社長，の AI 部長さんです。図は彼のブログから引用です。蛇足ですがブログのタイトルが unreasonable effectiveness of RNN です。
過去の偉大な論文 Wiegner (1960), Hamming (1967), Halevy (2009) からの <del>パクリ</del> **敬意を表したオマージュ**です。
"unreasonable effectiveness of [science|mathematics|data]" $\ldots$ www -->

<center>
<img src="/assets/diags.jpeg" style="width:77%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

- 上図最左は通常の多層ニューラルネットワークで画像認識，分類，識別問題に用いられます。
- 上図左から 2 つ目は，画像からの文章生成
- 上図中央，左から 3 つ目は，極性分析，文章のレビュー，星の数推定
- 上図右から 2 つ目は翻訳や文章生成
- 上図最右はビデオ分析，ビデオ脚注付け

などに用いられます。これまで理解を促進する目的で中間層をただ一層として描いてきた。
だが中間層は多層化されていることの方が多いこと，中間層各層のニューロン数は 1024 程度まで用いられていることには注意。

数は各層のニューロン数が 4 つである場合の数値例を示しています。入力層では **ワンホット** 表現が用いられている。
<!-- [^onehot]: ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。一つだけが "熱い" あるいは "辛い" ベクトルと呼びます。以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていたのですが ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派)と呼ばれることが多いです。ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じます。典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新します。従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになります。そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行いました。上のエルマンネットによる文法学習において,ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当します。単語埋め込み層を用いることで学習効率が改善し，後に示す word2vec などの **分散ベクトルモデル** へと発展します。 -->


<!--
## 実習
<!-- - [実習 画像認識 Keras 版](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kera_CNN_demo_with_wordnet_ja.ipynb) -->
<!-- - [実習 Keras CNN](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb){:target="_blank"} -->
<!-- - [実習 PyTorch CNN](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0416PyTorch_CNN_demo.ipynb){:target="_blank"} -->
<!-- - [実習 Kermack McKendric model](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}-->
<!--- [実習](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb#scrollTo=oD497lby40Fp)-->


### リカレントニューラルネットワークの時間展開

一時刻前の状態を保持して利用する SRN は下図左のように描くことができる。
同時に時間発展を考慮すれば下図右のように描くことも可能である。

<center>
<img src="/assets/RNN_fold.svg" style="width:49%"><br/>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてみよ。
あるいは同義だ上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してほしい。
このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かる。

### 長距離依存

上では RNN は時間方向でのディープラーニング(深層学習)であると説明しました。
ですが過去の情報を用いるために，一時刻前，すなわち直前の情報ではなく過去のある時点での情報を保持しておいて使いたい場合がありまs。英語の関係代名詞節を名詞の修飾に用いるような **中央埋め込み文** では，
主語と動詞との間で時制の一致が必要ですが，主語の後に関係代名詞節が埋め込まれると，主語の時制や数を
覚えておく必要が生じます。

文 "boy that girls chase plays the guitar" では関係代名詞節内の主語 "girls" が複数形です。
この複数形 "girls" に引きづられて動詞 "plays" を "play" としては正しい文法になりません。

このように過去の情報を覚えておく必要があります。これを **長距離依存** long term dependency と言います。SRN は長距離依存解消のために学習時間が長くなるという問題点があります。
これは中間層の内容が時々刻々変化し続けるため，特定の内容を保持することが困難になると考えられます。
この長距離依存解消が難しいという短所は，記憶内容を保持しておく別の場所，短期記憶バッファを用意するなどの解消方法も存在します。一方，短期記憶を保持する機構をリカレントニューラルネットワーク内に組み込むという考え方もあります。後者の考え方を実現する方法として次に紹介する長=短期記憶モデルがあります。

<div class="figcenter">

<img src="/assets/LTD.svg" style="width:33%"><br/>
Schematic description of a long term dependency
</div>


### 長=短期記憶 (LSTM: Long Short-Term Memory)

**長=短期記憶** (Long Short-Term Memory: LSTM, henceforth) はシュミットフーバー (Shumithuber, J.) 一派により提案された長距離依存解消のためのニューラルネットワークモデル。
長距離依存を解消するためには，ある内容を保持し続けて必要に応じてその内容を表出する必要がある。
このことを実現するために，ニューロンへの入力に門 (gate) を置くことが提案された。
下図に長=短期記憶モデルの概念図を示す。

<div class="figcenter">

<img src="/assets/2015Greff_LSTM_ja.svg" style="width:47%"><br>

LSTM from [@2016Asakawa_AIdict]
</div>

上図の LSTM は一つのニューロンに該当します。このニューロンには 3 つのゲート(gate, 門) が付いている。
3 つのゲートは以下の名前で呼ばれる。

1. 入力ゲート input gate
2. 出力ゲート output gate
3. 忘却ゲート forget gate

入力ゲートと出力ゲートが閉じていれば，セルの内容(これまでは中間層の状態と呼んできました)が保持されることになる。
出力ゲートが開いている場合には，セル内容が出力される。一方出力ゲートが閉じていればそのセル内容は出力されない。このように入力ゲートと出力ゲートはセル内容の入出力に関与する。
忘却ゲートはセル内容の保持に関与する。忘却ゲートが開いていれば一時刻前のセル内容が保持されることを意味する。反対に忘却ゲートが閉じていれば一時刻前のセル内容は破棄される。
全セルの忘却ゲートが全閉ならば通常の多層ニューラルネットワークであることと同義である。
すなわち記憶内容を保持しないことを意味する。
SRN でフィードバック信号が存在しない場合に相当する。セルへの入力は，以下のように

1. 下層からの信号，
2. 上層からの信号, すなわち Jordan ネットの帰還信号
3. 自分自身の内容，すなわち Elman ネットの帰還信号

が用いられる。これら入力信号が

1. 入力信号そのもの
2. 入力ゲートの開閉制御用信号
3. 出力ゲートの開閉制御用信号
4. 忘却ゲートの開閉制御用信号

という 4 種類に用いられる。従って LSTM のパラメータ数は SRN に比べて 4 倍となる。

LSTM に限らず一般のニューラルネットワークの出力には非線形関数が用いられる。代表的な非線形出力関数としては，以下のような関数が挙げられる。

1. シグモイド関数[^sigmoid]: $f(x)=\left[1+e^{-x}\right]^{-1}$
2. ハイパーボリックタンジェント関数:  $f(x)=\left(e^{x}-e^{-x}\right)/\left(e^{x}+e^{-x}\right)$
3. 整流線形ユニット関数: $f(x)=\max\left(0,x\right)$

[^sigmoid]: 1980 年代に用いられたシグモイド関数が用いられることはほとんどなくなりました。理由は収束が遅いからです[@1999LeCun]

この中で，セルの出力関数として 2. のハイパーボリックタンジェント関数が，ゲートの出力関数にはシグモイド関数が使われます。その理由はハイパーボリックタンジェント関数の方が収束が早いこと，シグモイド関数は値域が $[0,1]$ であるためゲートの開閉に直接対応しているからです。

- Le Cun, Y. Bottou, L., Orr, G. B, Muller K-R. (1988) Efficient BackProp, in Orr, G. and Muller, K. (Eds.) Neural Networks: tricks and trade, Springer.

<!--
The LSTM (left figure) can be described as the input signals $\mathbf{x}_t$ at
time $t$, the output signals $\mathbf{o}_t$, the forget gate $\mathbf{f}_t$, and
the output signal $\mathbf{y}_t$, the memory cell $\mathbf{c}_t$, then we can get
the following:
$i_{t}=\sigma\left(W_{xi}x_{t}+W_{hi}y_{t-1}+b_{i}\right)$, <br/>
$f_{t}=\sigma\left(W_{xf}x_{t}+W_{hf}y_{t-1}+b_{f}\right)$, <br/>
$o_{t}=\sigma\left(W_{xo}x_{t}+W_{ho}y_{t-1}+b_{o}\right)$, <br/> 
$g_{t}=\phi\left(W_{xc}x_{t}+W_{hc}y_{t-1}+b_{c}\right)$,<br/>
$c_{t}=f_{t}\odot c_{t-1} + i_{t}\odot g_{t}$,<br>
$h_{t}=o_{t}\odot\phi\left(c_{t}\right)$<br/><!--\label{eq:LSTM}
where
$\sigma\left(x\right)=\displaystyle\frac{1}{1+\mbox{exp}\left(-x\right)}$ (logistic function)
%% =1/2\left(\phi\Brc{x}+1\right)$,
$\phi\left(x\right)=\displaystyle\frac{\mbox{exp}\left(x\right)-\mbox{exp}\left(-x\right)}{\mbox{exp}\left(x\right)+\mbox{exp}\left(-x\right)}$ (hyper tangent)
%% $=2\sigma\left(x\right)-1$
and $\odot$ menas Hadamard (element--wise) product.
-->

### LSTM におけるゲートの生理学的対応物 <!--Physiological correlates of gates in LSTM-->

以下の画像は <http://kybele.psych.cornell.edu/~edelman/Psych-2140/week-2-2.html> よりの引用。
ウミウシのエラ引っ込め反応時に，ニューロンへの入力信号ではなく，入力信号を修飾する結合が存在する。下図参照。

<!--<center>
<img src="/assets/2016McComas_presynaptic_inhibition.jpg" style="width:74%"><br/>
</center>-->

<div class="figcenter">

<!-- sea slug, ウミウシ。Mollush 軟体動物，-->
<img src="/assets/C87-fig2_24.jpg" style="width:17%">
<img src="/assets/shunting-inhibition.jpg" style="width:29%"><br/>
<img src="/assets/C87-fig2_25.jpg" style="width:44%"><br/>

アメフラシ (Aplysia) のエラ引っ込め反応(a.k.a. 防御反応)の模式図[^seaslang]
</div>

[^seaslang]: from <http://kybele.psych.cornell.edu/~edelman/Psych-2140/week-2-2.html> の 222ページより<br>
画像はそれぞれ http://kybele.psych.cornell.edu/~edelman/Psych-2140/shunting-inhibition.jpg<br>http://kybele.psych.cornell.edu/~edelman/Psych-2140/C87-fig2.25.jpg<br>http://kybele.psych.cornell.edu/~edelman/Psych-2140/C87-fig2.24.jpg<br>

また古くは PDP のバイブルにもシグマパイユニット ($\sigma\pi$ units) として既述が見られます。各ユニットを掛け算 ($\pi$) してから足し算 ($\sum$) するのでこのように命名されたのでしょう。

<div class="figcenter">

<img src="/assets/sigma-pi.jpg" style="width:33%"><br/>
From [@PDPbook] chaper 7
</div>

### エンコーダ・デコーダモデル 符号化器・復号化器モデル (Encoder-Decoder models, a.k.a. Seq2seq models)

<div class="figure figcenter">
<img src="/assets/2014Sutskever_S22_Fig1.svg" width="77%">
<div class="figcaption">

Sutskever+2014 Fig. 1, 翻訳モデル `seq2seq` の概念図
</div>
</div>

`eos` は文末を表す。
中央の `eos` の前がソース言語であり，中央の `eos` の後はターゲット言語の言語モデルである SRN の中間層への入
力として用いる。

注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，そ
れ以外の時刻ではソース言語とターゲット言語は関係がない。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなしうる。
この点を改善することを目指すことが 2014 年以降盛んに行われてきた。
顕著な例が後述する **双方向 RNN**，**LSTM** 採用したり，**注意** 機構を導入することであった。

<div class="figure figcenter">
<img src="/assets/2014Sutskever_Fig2left.svg" width="44%">
<img src="/assets/2014Sutskever_Fig2right.svg" width="44%">
<div class="figcaption">

From Sutskever+2014, Fig. 2
</div></div>

### 自然言語系の注意

<div class="figure figcenter">
<img src="/assets/2015Bahdanau_attention.jpg" width="30%">
<img src="/assets/2015Luong_Fig2.svg" width="30%">
<img src="/assets/2015Luong_Fig3.svg" width="30%">
<div class="figcaption">

左: Bahdanau+2014,
中: Luong+2015, Fig. 2,
右: Luong+2015, Fig. 3
</div></div>


### 画像と言語との融合へ向けて

2014 年に提案されたニューラル画像脚注付けのモデルを下図に示す。

<!--<center>

<img src="/assets/2014KarpathyImageDescriptionsFig3.svg" style="width:84%"><br>
[@2015Karpathy_FeiFei_caption]
</center>-->

<div class="figcenter">

<img src="/assets/2014Vinyals_Fig1.svg" style="width:48%"><br/>
[@2014Vinyals_Bengio_Show_and_Tell]
</div>

<!--<center>
<img src="/assets/2015Xu_ShowAttendTellFig1.svg" style="width:84%"><br/>
</center>-->

画像に対して注意を付加した脚注付けモデルの出力例を下図に示す。

<!--<center>

<img src="/assets/2015Xu_ShowAttendTellFig2_upper.svg" style="width:84%"><br>
[@2015Xu_Bengio_NIC_attention]
</center>-->
各画像対は右が入力画像であり，左はその入力画像の脚注付けである単語を出力している際にどこに注意しているのかを白色で表している。

<div class="figcenter">

<img src="/assets/2015Xu_ShowAttendTellFig2_lower.svg" style="width:66%"><br/>
[@2015Xu_Bengio_NIC_attention]
</div>


<div class="figcenter">

<img src="/2025assets/2015Xu_Bengio_fig1.svg" style="width:44%"><br/>
<img src="/2025assets/2015Xu_Bengio_fig5.svg" style="width:77%"><br/>
[@2015Xu_Bengio_NIC_attention]
</div>


<div class="figcenter">

<img src="/assets/2014MnihGraves_Fig1a.svg" width="33%;">
<img src="/assets/2014MnihGraves_Fig1b.svg" width="33%;"><br/>
<img src="/assets/2014MnihGraves_Fig1c.svg" width="44%;"><br/>
<!-- <img src="/assets/2014Mnih_attention.svg"><br/> -->
</div>


Glimpse Sensor: Glimpse の座標と入力画像が与えられると Glimpse Sensor は $l_{t-1}$ を中心とし、複数の解像度パッチを含む **網膜様** 表現 $\rho\left(x_t,l_{t-1}\right)$ を抽出する。

* B. **Glimpse Network**: 位置 $\left(l_{t-1}\right)$ と入力画像 $\left(x_t\right)$ が与えられると、Glimpse Sensor を用いて網膜表現 $\rho\left(x_t,l_{t-1}\right)$ を抽出する。網膜表現と Glimpse の位置は、それぞれ $\theta_g^{0}$ と $\theta_g^{1}$ でパラメータ化された独立した線形層を用いて隠れ空間に写像される。線形層は平行化素子を用いており、その後に別の線形層 $\theta_2^{2}$ が続き、両成分からの情報を結合する。 glimpse ネットワーク $f_{g}\left(\dot;\left[\theta_g^0,\theta_g^1,\theta_g^2\right]\right)$ は、glimpse 表現 $g_t$ を生成する注意ネットワークのための、学習可能な帯域幅制限センサを定義する。
* C. **モデルアーキテクチャ**: 全体として、モデルは RNN である。モデルの核心ネットワーク $f_h\left(\cdot;\theta_h\right)$ は、glimpse 表現 $g_t$ を入力として受け取り、前のタイムステップ $h_{t-1}$ における内部表現と組み合わせることで、モデルの新しい内部状態 $h_t$ を生成する。位置ネットワーク$f_l\left(\cdot;\theta_a\right)$ とアクションネットワーク $f_a\left(\cdot;\theta_a\right)$ は、モデルの内部状態 $h_t$ を用いて、それぞれ $l_t$ における次の注目位置と、$l_t$ におけるアクション/分類を生成する。この基本的なRNN反復処理は、可変ステップ数だけ繰り返される。[@2014Mnih_RNN_attention]

<!-- Glimpse Sensor: Given the coordinates of the glimpse and an input image, the sensor extracts a __retina-like__ representation $\rho\left(x_t,l_{t-1}\right)$ centered at $l_{t-1}$ that contains multiple resolution patches. 

* B. **Glimpse Network**: Given the location $\left(l_{t-1}\right)$ and input image $\left(x_t\right)$, uses the glimpse sensor to extract retina representation $\rho\left(x_t,l_{t-1}\right)$.  The retina representation and glimpse location is then mapped into a hidden space using independent linear layers parameterized by $\theta_g^{0}$ and $\theta_g^{1}$ respectively using rectified units followed by another linear layer $\theta_2^{2}$ to combine the information from both components. The glimpse network $f_{g}\left(\dot;\left[\theta_g^0,\theta_g^1,\theta_g^2\right]\right)$ defines a trainable bandwidth limited sensor for the attention network producing the glimpse representation $g_t$. 
* C. **Model Architecture**: Overall, the model is an RNN. The core network of the model $f_h\left(\cdot;\theta_h\right)$ takes the glimpse representation $g_t$ as input and combining with the internal representation at previous time step $h_{t-1}$, produces the new internal state of the model $h_t$. The location network $f_l\left(\cdot;\theta_a\right)$ and the action network $f_a\left(\cdot;\theta_a\right)$ use the internal state $h_t$ of the model to produce the next location to attend to $l_t$ and the action/classification at respectively. This basic RNN iteration is repeated for a variable number of steps.[@2014Mnih_RNN_attention] -->


<!--
#  World Models
<center>

<img src="/assets/2018Ha_WorldModel.svg" style="width:84%"><br/>
[@2018Ha_WorldModels] Fig.1
</center>
<center>

<img src="/assets/2018HaWorldModelsFig1.svg"><br/>
A World Model, from Scott McCloud’s Understanding Comics. (McCloud, 1993; E, 2012)
</center>

Jay Wright Forrester, the father of system dynamics,
described a mental model as:\\
    \begin{quote}
      The image of the world around us, which we carry in our
      head, is just a model. Nobody in his head imagines all
      the world, government or country. He has only selected
      concepts, and relationships between them, and uses those
      to represent the real system. \citep{1971Forrester}
    \end{quote}

<center>
-->

<!--
<img src="/assets/2015Greff_LSTM_ja.svg" style="width:74%"><br>
<p align="left" style="width:49%">
LSTM の概念 (Shumithuber ら 2015)を改変
</p>
</center>
-->


<!--
<center>

<img src="/assets/2010Mikolov_Fig1.svg" style="width:49%"><br/>
\cite{2010Mikolov2010}
</center>

<center>

<img src="/assets/2011Mikolov_Extention_Fig1.jpg" style="width:49%"><br>
Mikolov Extension
</center>

<center>

<img src="/assets/2001Boden_Fig5.jpg" style="width:94%"><br/>
Boden's BPTT
</center>
-->

<!--
- モチベーション
- ニューラルネットワーク言語モデル
- 訓練アルゴリズム
  - リカレントニューラルネットワーク
  - クラス
  - エントロピー最大化言語モデル

### モチベーション

### モチベーション (2) チューリングテスト
- チューリングテストは原理的に言語モデルの問題とみなすことが可能
- 会話の履歴が与えられた時，良い言語モデルは正しい応答に高い確率を与える

- 例:
  - $P\left(\mathbf{ox{月曜日}\vert \mathbf{ox{今日は何曜日ですか？}}} = ?$\\
  - $P\left(\mathbf{ox{赤}\vert \mathbf{ox{バラは何色？}}} = ?$\\

言語モデルの問題と考えれば以下の文のような問題と等価とみなせる:\\
$P\left(\mathbf{ox{赤}\vert {\mathbf{ox{バラの色は}}}=?$

### モチベーション(3) n-グラム言語モデル

- どうすれば「良い言語モデル」を創れるか？
- 伝統的解: n-グラム言語モデル: $P\left{w\vert h}=\displaystyle\frac{C\left{h,w\right}}{C\left(h\right)}$
-->


# 意味論

ここでは意味論の研究史を心理学関連領域に絞ってまとめることを試みます。
<!-- 神経心理学症状との関連については
付録 <a target="_blank" href="https://github.com/ShinAsakawa/wbai_aphasia/blob/master/2019Primer_AphasiaDyslexia.pdf">失語，失読に関する神経心理学モデルの基礎</a> をご覧ください。-->

意味についての言及は言語学者 Firth さらに遡れば Witgenstein まで辿ることが可能です。
ですがここでは直接関連する研究として以下をとりげます

- 第 1 世代 意味微分法 Osgood 
- 第 2 世代 潜在意味解析 Ladauer
- 第 3 世代 潜在ディレクリ配置，トピックモデル
- 第 4 世代 分散埋め込みモデル word2vec とその後継モデル
- 最近の展開


### 1952 年 意味微分法 Semantics Differential (SD)
チャールズ・オズグッドによって提案された意味微分法は，被験者に対象を評価させる際に形容詞対を用います。
形容詞対は 5 件法あるいはその他の変種によって評価されます。
得られた結果を 評価対象 X 形容詞対の行列にします。
すなわち評価対象者の平均を求めて得た行列を **固有値分解**，正確には因子分析 FA を行います。
最大固有値から順に満足の行くまで求めます。
固有値行列への射影行列を因子負荷量と呼びます。得られた結果を下図に示しました。

<center>

<img src="/assets/1957Osgood_Tab1.svg" style="width:84%"><br>
From  Osgood (1952) Tab. 1
</center>
<!-- <a target="_blank" href="figures/1957Osgood_Tab1.svg">Osgood (1952) Tab. 1</a>-->

上図では，50 対の形容詞対によって対象を評価した値が描かれています。

<a target="_blank" href="https://ja.wikipedia.org/wiki/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90">因子分析(FA)</a> 形容詞対による多段階評定

<center>

<img src="/assets/1957Osgood_Fig2.svg" style="width:64%"><br>
From  Osgood (1952) Fig.2
<!--- <a target="_blank" href="figures/1957Osgood_Fig2.svg">Osgood (1952) Fig. 2</a>-->
</center>

意味微分法においては，研究者の用意した形容詞対の関係に依存して対象となる概念やモノ，コトが決まります。
従って研究者の想定していない概念空間については言及できないという点が問題点として指摘できます。

このことは評価対象がよくわかっている問題であれば精度良く測定できるという長所の裏返しです。

一般的な意味，対象者が持っている意味空間全体を考えるためには，50 個の形容詞対では捉えきれないことも意味します。従って以下のような分野に適用する場合には問題が発生する可能性があると言えます

- 神経心理学的な症状である **意味痴呆** semantic dimentia を扱う場合
- 入試問題などの一般知識を評価したい場合
- 一般言語モデルを作成する場合


### 1997 年 潜在意味分析 Latent Semantic Analysis (LSA, LSI)
- **潜在意味分析**: <a target="_blank" href="https://ja.wikipedia.org/wiki/%E7%89%B9%E7%95%B0%E5%80%A4%E5%88%86%E8%A7%A3">特異値分解(SVD)</a> は，当時増大しつつあったコンピュータ計算資源を背景に一般意味論に踏み込む先鞭をつけたと考えることができます。

すなわち先代の意味微分法が持つ問題点である，評価方法が 50 対の形容詞であること，
50 をいくら増やしても，結局は研究者の恣意性が排除できないこと，評価者が人間であるため大量の評価対象を評価させることは，
心理実験参加者の拘束時間を長くするため現実的には不可能であることを解消するために，辞書そのものをコンピュータで解析するという手法を採用しました。

1. 辞書の項目とその項目の記述内容とを考えます
2. 特定の辞書項項目にはどの単語が使われているいるのかという共起行列 内容 $\times$ 単語 を
考え，この行列について **特異値分解** を行います。

Osgood の意味微分法で用いられた行列のサイズと比較すると，単語数が数万，項目数は数万から数十万に増加しています。
数の増加は網羅する範囲の拡大を意味します。
下図は持ちられたデータセット例を示したものです。

<center>

<img src="/assets/1997Landauer_Dumais_FigA2.svg" style="width:84%"><br>
From Landauer and Duman (1997) Fig. A2
</center>

LSA (LSI) の問題点としては以下図を見てください

<center>

<img src="/assets/1997Landauer_Fig3.svg" style="width:64%"><br>
From Landauer and Dumas (1997) Fig.3 
</center>

上図は，得られた結果を元に類義語テストを問いた場合に特異値分解で得られる次元数を横軸に，正解率を縦軸にプロットした図です。
次元を上げると成績の向上が認められます。
ですが，ある程度 300 以上の次元を抽出しても返って成績が低下することが示されています。

次元数を増やすことで本来の類義語検査に必要な知識以外の情報が含まれてしまうため推察されます。


<!--- <a target="_blank" href="figures/1997Landauer_Fig3.svg">Landauer (1997) Fig. 3</a>-->

### 2003 年 潜在ディレクリ配置 Latent Direchlet Allocation (LDA)

潜在ディレクリ配置 Latent Direchlet Allocation: LDA[^LDA] は LSA (LSI) を確率的に拡張したモデルであると考えることができます。すなわち LDA では単語と項目との関係に確率的な生成モデルを仮定します。

[^LDA]: 伝統的な統計学においては Fischer の線形判別分析を LDA と表記します。ですがデータサイエンス，すなわち統計学の一分野では近年の潜在ディレクリ配置の成功により LDA と未定義で表記された場合には潜在ディクレクリ配置を指すことが多くなっています。

その理由としては，対象となる項目，しばしば **トピック** と言い表すと，項目の説明に用いられる単語との間に，決定論的な関係を仮定しないと考えることによります。確率的な関係を仮定することにより柔軟な関係をモデル化が可能であるからです。

例えば，ある概念，話題(トピック) "神経" を説明する場合を考えます。
"神経" を説明するには多様な表現や説明が可能です。
"神経" を説明する文章を数多く集めてると，単語 "脳" は高頻度で出現すると予想できます。
同様にして "細胞" や "脳" も高頻度で観察できるでしょう。ところが単語 "犬" は低頻度でしょう。
単語 "アメフラシ" や "イカ" は場合によりけりでしょう。どちらも神経生理学の発展に貢献した実験動物ですから単語 "アメフラシ" や "イカ" が出現する文章もあれば，単語 "脳梗塞" や単語 "失語" と同時に出現する確率もありえます。
このように考えると確率的に考えた方が良い場合があることが分かります。

#### ディレクリ分布

もう一点，ノンパラメトリックモデルについて説明します。
parametric model はパラーメータを用いたモデルほどの意味です。
心理統計学の古典的な教科書では，ノンパラメトリック検定とは母集団分布のパラメータに依存 **しない** 統計的検定という意味で用いられます。一方 LDA の場合には推定すべき分布のパラメータ(の数)を **事前に定めない** という意味で **ノンパラメトリック** なモデルであると言います。
すなわちある話題(トピック)とそれを説明する単語の出現確率について，取り扱う現象の複雑さに応じてモデルを記述するパラメータ数を適応的に増やして行くことを考えます。

数学的既述は省略しますが，<a target="_blank" href="https://en.wikipedia.org/wiki/Beta_distribution">ベータ分布</a> を用いると区間 $[a,b]$ の間をとる分布でパラメータにより分布が柔軟に記述できます。ベータ分布の多次元拡張を <a target="_blank" href="https://en.wikipedia.org/wiki/Dirichlet_distribution">ディククリ分布</a> と言います。

確率空間に対して一定の成約を付した表現をシンプレックスと言ったりします。<!--例えばコインの裏表は
2 値ですからベータ分布を用いても表す事ができます。-->
たとえばじゃんけんで対戦相手が，グー，チョキ，パー のいずれかを出す確率は，2 つが分かれば 3 つ目の手は自ずと分かってきます。このような関係は 3 つの手の確率分布でディククリ分布として扱うことが可能です。
下図はウィキペディアから持ってきました。この図はそのようなじゃんけんの手の出現確率をディレクリ分布として表現した例だと思ってください。

<!--## ディククリ分布 (多次元ベルヌーイ分布)-->
<!-- 多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定-->

<center>

<img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Dirichlet-3d-panel.png" style="width:64%"><br>
<!--<img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Dirichlet_distributions.png" style="width:49%"><br/>-->
<p align="left" style="width:74%">
多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定<br>
図は <https://en.wikipedia.org/wiki/Dirichlet_distribution> より
</p>
</center>

トピック毎の単語の出現確率も上図と同じ枠組みで記述することが可能です。かつ，上図ではとりうる値が 3 つの場合ですが，話題が複雑になれば適応的に選択肢の数，すなわちディレクリ分布の次元数が増加することになります。

#### プレート表記

あらかじめ定められた数のパラメータを用いて分布を記述するのではなく，
解くべき問題の複雑さに応じて適応的にパラメータ数を定めることに対応して，
LDA あるいはトピックモデルの図示方法として **プレート表記** plate notation があります。
下図にプレート表記の例を示しました。

<center>

<img src="/assets/2009Blei_Topic_Models_02.svg" style="width:74%"><br/>
プレート表記: ノンパラメトリックモデルの表現に用いられる
</center>

- 丸は確率変数 
- 矢印は確率的依存関係を表現 
- 観測変数は影付き(文献によっては二重丸) 
- プレートは繰り返しを表す


Y からパラメータ X が生成される場合，矢印を使ってその依存関係を表現します。ノンパラメトリックモデルの場合，矢印の数を予め定めません。そのため矢印を多数描くのが煩雑なので，一つの箱代用して表現します。
これがプレート表記になります。

観測可能な変数をグレー，または二重丸で表し，観測不能な，類推すべきパラメータを白丸で表記します。
実際には観測不可能な潜在パラメータを観測データから類推することになります。

大まかなルールとして，潜在変数をギリシャアルファベット表記，観測される変数はローマアルファベット表記の場合が多いですが，一般則ですので例外もあります。

下図に潜在ディレクリ配置  LDA のプレート表記を示しました。
<!--### 潜在ディレクリ配置のプレート表記-->

<center>

<img src="/assets/2009Blei_Topic_Models_03.svg" style="width:74%"><br/>
<!--<img src="/assets/2009Blei_Topic_Models_04.svg" style="width:94%"><br/>-->
</center>

#### トピックと単語の関係

トピックモデルの要点をまとめた下図はこれまでの説明をすべて含んでいます。

<center>

<img src="/assets/2009Blei_Topic_Models_01.svg" style="width:74%"><br/>
<p align="left" style="width:74%">
 出典: ブライのスライド(2009)より，文章は話題(トピック)の混合<br/>
 各文章はその話題から文章が生成されたと考える
 </p>
</center>

興味深い応用例として Mochihashi ら(2009) の示した教師なし学習による日本語分かち書き例を示します。
下図は源氏物語をトピックモデルにより分かち書きさせた例です。どこに空白を挿入すると文字間の隣接関係を表現できるかをトピックモデルで解くことを考えた場合，空白の挿入位置が確率的に定まると仮定して居ます。


<center>

<img src="/assets/2009Mochihashi_Fig10.svg" style="width:84%"><br/>
</center>

Mochihashi らは，ルイス・キャロルの小説 "不思議の国のアリス" 原文から空白を取り除き，
文字間の隣接関係から文字の区切り，すなわち空白を推定することを試みました。結果を下図に示しました。

<center>

<img src="/assets/2009Mochihashi_Fig12.svg" style="width:84%"><br/>
</center>

### 原著論文

- <a target="_blank" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Blei, Jordan (2003) Latent Dirichlet Allocation 原著論文</a>
- 発展モデル: <a target="_blank" href="http://arxiv.org/abs/0710.0845" >中華レストラン過程 CRP</a>
- <a target="_blank" href="http://www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf">インド食堂過程 IBP</a> 
<!-- - パチンコ過程 -->