---
title: 第12回 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+2" color="navy"><strong>2025 年度開講 駒澤大学 人工知能 I および II</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 11/Jul./2025<br/>
Appache 2.0 license<br/>
</div>

# 第 12 回

* [課題提出用フォルダ](https://drive.google.com/drive/u/3/folders/1D-hqGgZ2XldKrrMPqhTxEPiiBI3lx5XC){:target="_blank"}


# キーワード

- 敵対的生成ネットワーク (GAN: Generative Adversarial Networks)
- 画風変換 (Style Transfer)
- EM アルゴリズム (Estimation Maximization Algorithm)
- 変分自己符号化器モデル (VAE: Variational Auto-Encoders)
- 変分下限 (ELBO: Evidence Lower BOund) [ビデオ ](https://www.youtube.com/watch?v=jugUBL4rEIM){:target="_blank"}
- [幻視 パレイドリア (松沢病院 西尾先生 の資料より)](https://komazawa-deep-learning.github.io/2021/2018Nishio_LBDtmp.pdf){:target="_blank"}

# 実習教材

* [畳み込みニューラルネットワークの事前訓練済モデルによる中間表現の可視化 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_1024CNN_layer_visualization.ipynb){:target="_blank"}
* [特徴点検出アルゴリズム 画像フィルタ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_visit_feature_extractions_demo.ipynb){:target="_blank"}
* [DOG などのフィルタと Harr 特徴による顔検出 a.k.a ビオラ＝ジョーンズ アルゴリズム <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0528edge_and_face_detection_algorithm_not_cnn.ipynb){:target="_blank"}
- [LeNet PyTorch <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0528LeNet_pytorch.ipynb){:target="_blank"}

- [DeepDream 実習 <img src="/assets/colab_icon.svg"> ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb){:target="_blank"}
- [CartoonGAN 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0628CartoonGAN_demo.ipynb){:target="_blank"}
- [CycleGAN 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0625_CycleGAN_demo.ipynb){:target="_blank"}
- [VAE の実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit_vae_demo.ipynb#scrollTo=GC8fpkk6cFbw){:target="_blank"}

#  ビデオ供覧

- [GAN ビデオ教材](https://drive.google.com/file/d/18uxufT2rf49ItV1PjP2J1-QgmseVoJgT/view?usp=sharing){:target="_blank"}
[資料](https://drive.google.com/file/d/1QE5X5enbEXKzHXj82veUC3dMGO3yrRrN/view?usp=sharing){:target="_blank"}
- [VAE の予備知識ビデオ](https://drive.google.com/file/d/1AKvOa-EeGnpfNznpfBGZ4xqZJR7Z1ro8/view?usp=sharing){:target="_blank"}
[資料](https://drive.google.com/file/d/1hIenuObaY-DX244pjvKh5zjOORlSbeT3/view?usp=sharing){:target="_blank"}
- [VAE ビデオ](https://drive.google.com/file/d/10jxvAeJzfRv2RkJcv-BKX--EerpwMWu1/view?usp=sharing){:target="_blank"}
[資料](https://drive.google.com/file/d/1hIenuObaY-DX244pjvKh5zjOORlSbeT3/view?usp=sharing){:target="_blank"}


# 生成モデル Generative models

<center>
<img src="/assets/Tenn2.jpg" width="48%">
<img src="/assets/Tenn_deepdream.jpg" width="48%"><br/>
天安門前広場の夢 (撮影は自民解放軍の兵士に依頼した)
</center>

畳み込みニューラルネットワークの精度は向上しました。ですが，2014 年まで画像の合成はできませんでした。

<!-- 
Throughout most of this book, we have talked about how to make predictions. 
In some form or another, we used deep neural networks learned mappings from data examples to labels. 
This kind of learning is called discriminative learning, as in, weʼd like to be able to discriminate between photos cats and photos of dogs. 
Classifiers and regressors are both examples of discriminative learning. 
And neural networks trained by backpropagation have upended everything we thought we knew about discriminative learning on large complicated datasets. 
Classification accuracies on high-res images has gone from useless to human-level (with some caveats) in just 5-6 years. 
We will spare you another spiel about all the other discriminative tasks where deep neural networks do astoundingly well.

But there is more to machine learning than just solving discriminative tasks. 
For example, given a large dataset, without any labels, we might want to learn a model that concisely captures the characteristics of this data. 
Given such a model, we could sample synthetic data examples that resemble the distribution of the training data. 
For example, given a large corpus of photographs of faces, we might want to be able to generate a new photorealistic image that looks like it might plausibly have come from the same dataset. This kind of learning is called generative modeling.

Until recently, we had no method that could synthesize novel photorealistic images. 
But the success of deep neural networks for discriminative learning opened up new possibilities. 
One big trend over the last three years has been the application of discriminative deep nets to overcome challenges in problems that we do not generally think of as supervised learning problems. 
The recurrent neural network language models are one example of using a discriminative network (trained to predict the next character) that once trained can act as a generative model.

In 2014, a breakthrough paper introduced Generative adversarial networks (GANs) (Goodfellow et al., 2014), a clever new way to leverage the power of discriminative models to get good generative models. 
At their heart, GANs rely on the idea that a data generator is good if we cannot tell fake data apart from real data. 
In statistics, this is called a two-sample test - a test to answer the question whether datasets $X = \{x_1,\ldots, x_n\}$ and $X' = \{x'_1,\ldots,x'_n\}$ were drawn from the same distribution.
The main difference between most statistics papers and GANs is that the latter use this idea in a constructive way. 
In other words, rather than just training a model to say “hey, these two datasets do not look like they came from the same distribution”, they use the two-sample test to provide training signals to a generative model. 
This allows us to improve the data generator until it generates something that resembles the real data. 
At the very least, it needs to fool the classifier. 
Even if our classifier is a state of the art deep neural network.
-->
<!-- 
2014 年 画期的な論文により Generative adversarial networks (GAN) (Goodfellow et al., 2014) が紹介されました。
これは 識別モデルの力を活用して 優れた生成モデルを得るための巧妙な新しい方法です。
GAN の核心は 「偽のデータと本物のデータを見分けることができなければ データ生成器は優れている」という考え方にあります。
統計学では  「2 標本検定」といい、データセット $X=\left\\{x_1,\ldots, x_n\right\\}$ と $X'=\left\\{x'_1,\ldots,x'_n\right\\}$ が同じ分布から引き出されているかどうかを調べる検定です。
統計学の論文と GAN の大きな違いは GAN がこの考え方を建設的に利用していることです。
つまり 単に「おい、この 2 つのデータセットは同じ分布から来たようには見えないぞ」 とモデルを訓練するのではなく 生成モデルに訓練信号を与えるために 2 標本検定 を使うのです。
これにより 実際のデータに似たものを生成するまで、データ生成器 を改良することができます。
少なくとも、分類器 を騙す必要があります。
たとえ 分類器 が最先端のディープニューラルネットワークであったとしてもです。


本物そっくりのデータを生成できる可能性のあるネットワークが必要です。
画像を扱うのであれば、画像を生成する必要があります。画像を扱う場合は画像を、音声を扱う場合は音声シーケンスを生成する必要があります。
これを「生成器ネットワーク」と呼びます。
2 つ目のネットワークは 識別器ネットワークです。
このネットワークは、偽物と本物のデータを区別しようとします。
この2つのネットワークはお互いに競争しています。
生成器ネットワークは、識別器ネットワークを欺こうとします。
この時、識別器ネットワークは、新しい偽のデータに適応します。
この情報は 今度は生成器ネットワークを改善するために使用され という具合です。
 -->

<!-- The GAN architecture is illustrated in Fig. 17.1.1.  -->

<!-- As you can see, there are two pieces in GAN architecture - first off, we need a device (say, a deep network but it really could be anything, such as a game rendering engine) that might potentially be able to generate data that looks just like the real thing. 
If we are dealing with images, this needs to generate images. If we are dealing with speech, it needs to generate audio sequences, and so on. We call this the generator network. 
The second component is the discriminator network. 
It attempts to distinguish fake and real data from each other. 
Both networks are in competition with each other. The generator network attempts to fool the discriminator network. 
At that point, the discriminator network adapts to the new fake data. 
This information, in turn is used to improve the generator network, and so on.

識別器は 入力 $x$ が（実データからの）本物か（生成器からの）偽物かを区別するための二値分類器です。
一般に識別器は 隠れたサイズが 1 の密な層を使用するなどして、入力 $masbf{x}$ に対してスカラー予測 $o\in\mathcal{R}$ を出力し シグモイド関数を適用して予測確率 $D(x) = 1/(1 + e^o)$ を得ることができます。
真のデータのラベル $y$ は 1 偽のデータは 0 だとします。
交差エントロピー損失を最小化するように識別器を学習:

$$
\min_D\left\{y\log(D(x))-(1-y)\log(1-D(x))\right\},
$$
-->

<!-- The discriminator is a binary classifier to distinguish if the input $x$ is real (from real data) or fake (from the generator). 
Typically, the discriminator outputs a scalar prediction $o\in\mathcal{R}$ for input $\mathbf{x}$, such as using a dense layer with hidden size 1, and then applies sigmoid function to obtain the predicted probability $D(x) = 1/(1 + e^o)$. 
Assume the label $y$ for the true data is 1 and 0 for the fake data. 
We train the discriminator to minimize the cross-entropy loss, i.e.,
-->


<!-- - 生成器は ランダムなソース 正規分布 $z\sim\mathcal{N}(0, 1)$ から パラメータ $z$ をサンプリング
- $z$ を潜在変数として扱う
- 生成器ネットワークを適用して $x'=G(z)$ を生成
-->

<!-- For the generator, it first draws some parameter $z\in\mathbb{R}^d$ from a source of randomness, e.g., a normal distribution $z\sim\mathcal{N}(0, 1)$. 
We often call $z$ as the latent variable. 
It then applies a function to generate $x'= G(z)$. 
The goal of the generator is to fool the discriminator to classify $x'=G(z)$ as true data, i.e., we want $D(G(z))\sim 1$. 
In other words, for a given discriminator $D$, we update the parameters of the generator $G$ to maximize the cross-entropy loss when $y=0$, i.e.,
-->

## 敵対的生成ネットワーク GAN

$$
\min_G\left\{-(1-y)\log(D(G(\mathbf{z})))\right\} = \min_G\left\{-\log(D(G(\mathbf{z})))\right\}
$$

- 認識の反対の操作が 生成器。**生成敵対ネットワーク** Generative Adversarial Networks: GAN 
- GAN では 2 つのニューラルネットワークが用いられ，**識別器** descriminator と **生成器** generator と呼びます(Goodfellow,2014)。
- 識別器も生成器も多層ニューラルネットワーク

- 通常の画像分類課題では，最上位層において推論，すなわち入力画像が何であるかを計算するためにソフトマックス関数などが用いられる。
- これに対して GAN の識別器は，0 か 1 かの出力
- 入力画像が通常の画像であれば 1 を，生成器によって生成 された画像であれあば 0 を出力
- 生成器は，識別器の最終直下層で得られたような画像表現に雑音を加えた値から画像を生成
- 生成器は，識別器が入力データから画像を推論するのと逆方法に推論から画像を生成
- すなわち GAN は入力が実在するか，偽造品，すなわちフェイクかを見破る訓練がなされる
- 生成器の目的は 識別器を騙すこと。 $x'=G(z)$ を真のデータとして分類させること。すなわち $D(G(z))$ を $\sim 1$ に近づけること。
- 識別器 $D$ に対して $y=0$ のときの交差エントロピー損失 が最大になるように 生成器 $G$ のパラメータを更新する
- 生成器 は 識別器 の学習成果であるデータの内部表現を模倣し，生成器を欺こうとする
- 識別器 と 生成器 との間で  **ゲーム理論** でいう **ナッシュ均衡** Nash's equilibrium が成り立つ (Heusel, 2017)。

<!-- GAN の模式的な流れを下図 に示しました。 -->
<center>
<img src="/assets/2016Goodfellow_fig12ja.svg" style="width:49%"><br/>
</center>

## 画像変換

<center>
<img src='/assets/2017Taigman_fig.svg' style='width:49%'><br/>
</center>

<center>
<img src='/assets/2017Reed_tmp_5.svg' style='width:49%'>
</center>

<center>
<img src='/assets/2017Reed_tmp_1.svg' style='width:49%'>
</center>


<center>
<img src='/assets/2017Reed_tmp_6.svg' style='width:49%'>
</center>

## サイクル GAN
<center>
<img src='/assets/2017Reed_tmp_7.svg' style='width:49%'>
</center>

### サイクル GAN による領域変換
<center>
<img src='/assets/2017Reed_tmp8.svg' style='width:49%'>
</center>

<center>
<img src='/assets/2017Zhu_cycleGAN1.svg' style='width:49%'><br/>
<img src='/assets/2017Zhu_cycleGAN2.svg' style='width:49%'><br/>
<img src='/assets/2017Zhu_cycleGAN3.svg' style='width:49%'><br/>
</center>

---

## まんがの画風変換

<center>
<img src="/assets/2018Chen_CartoonGAN.svg" style="width:49%"><br/>
`CartoonGAN: Generative Adversarial Networks for Photo Cartoonization` CVPR 2018 (Conference on Computer Vision and Pattern Recognition)
</center>
<br/><br/><br/>

<center>
<img src="https://cdn-images-1.medium.com/max/1800/1*GS5_bEgpy00cotNRFvAPyA.png" style="width:46%">
<img src="https://d2l930y2yx77uc.cloudfront.net/production/uploads/images/7291694/rectangle_large_type_2_86d2e2a52336624f98ed8aa3163a1865.jpg" style='width:49%'><br/>
左: 君の名は。右: 風の谷のナウシカ，より
</center>
<br/><br/><br/>



## Colab ファイル

* [Stable Diffusion <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2025_0110stable_diffusion.ipynb){:target="_blank"}
* [Diffusion Models from Scratch <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2025_0110_02diffusion_models_from_scratch.ipynb){:target="_blank"}

## [基盤モデル LxM](https://en.wikipedia.org/wiki/Foundation_model)

Large X model (LxM) としても知られる基盤モデルは，幅広いユースケースに適用できるように，膨大なデータセットで学習される機械学習または深層学習モデルである[1]。
大規模言語モデルのような生成 AI アプリケーションは，しばしば基盤モデルの例である[1]。
<!-- A foundation model, also known as large X model (LxM), is a machine learning or deep learning model that is trained on vast datasets so it can be applied across a wide range of use cases.[1]
Generative AI applications like Large Language Models are often examples of foundation models.[1]-->

基盤モデル (LxM) は，深層学習ネットワーク，転移学習，自己教師あり学習のような確立された機械学習技術を使用して構築される。
基盤モデルは，特注の単一課題に特化したモデルではなく，再利用可能なインフラとして機能する汎用モデルである点が，これまでの技術とは異なる。
<!-- Technologically, foundation models are built using established machine learning techniques like deep neural networks, transfer learning, and self-supervised learning. Foundation models differ from previous techniques as they are general purpose models function as a reusable infrastructure, instead of bespoke and one-off task-specific models.-->

コンピュータの並列処理 (CUDA GPU など) の進歩や，ニューラルネットワークアーキテクチャ (Transformers など) の新展開，最小限の監視で学習データを使用するようになったことなどが，基盤モデルの台頭に貢献した。
基礎モデルは，2010 年代後半に深層学習モデルの最新 の波として具体化し始めた(23)。
深層学習に関する先行研究のほとんどと比較して，これらの言語モデルは，自己教師ありの目的（例えば，大規模なテキストコーパスの次の単語を予測する）を使用して，はるかに大規模なウェブソースデータセットで学習する可能性を示した。
これらのアプローチは，word2vec や GloVe のような以前の研究に基づくもので，注釈付きデータ（例えばクラウドソースラベル）を必要とする以前の教師ありアプローチから逸脱している。
<!-- Advances in computer parallelism (e.g., CUDA GPUs) and new developments in neural network architecture (e.g., Transformers), and the increased use of training data with minimal supervision all contributed to the rise of foundation models.
Foundation models began to materialize as the latest wave of deep learning models in the late 2010s.[23]
Relative to most prior work on deep learning, these language models demonstrated the potential of training on much larger web-sourced datasets using self-supervised objectives (e.g. predicting the next word in a large corpus of text).
These approaches, which draw upon earlier works like word2vec and GloVe, deviated from prior supervised approaches that required annotated data (e.g. crowd-sourced labels). -->

2022 年にリリースされた Stable Diffusion と ChatGPT（当初は GPT-3.5 モデルを搭載）は，基礎モデルと生成 AI が広く一般に普及するきっかけとなった。
さらに，2023 年の LLaMA, Llama2, Mistral のリリースは，オープンな基盤モデルが多くの支持[24] と精査を集めることで，基盤モデルがどのようにリリースされるかがより重視されることに貢献した[25]。
<!--The 2022 releases of Stable Diffusion and ChatGPT (initially powered by the GPT-3.5 model) led to foundation models and generative AI entering widespread public discourse. Further, releases of LLaMA, Llama 2, and Mistral in 2023 contributed to a greater emphasis placed on how foundation models are released with open foundation models garnering a lot of support[24] and scrutiny.[25] -->

基盤モデルの構築は，多くの場合，非常にリソース集約的であり，最も高度なモデルでは，膨大なデータセットの取得，キュレーション，処理にかかる費用と，学習に必要な計算能力を賄うために，数億ドルのコストがかかる[2]。
これらのコストは，洗練されたインフラストラクチャ，訓練時間の延長，GPU などの高度なハードウェアの必要性から生じている。
対照的に，既存の基盤モデルを特定のタスクに適合させたり，直接使用したりすることは，事前に訓練された能力を活用するため，はるかにコストがかからず，通常，小規模な課題固有のデータセットで微調整を行うだけで済む。
<!-- Building foundation models is often highly resource-intensive, with the most advanced models costing hundreds of millions of dollars to cover the expenses of acquiring, curating, and processing massive datasets, as well as the compute power required for training.[2]
These costs stem from the need for sophisticated infrastructure, extended training times, and advanced hardware, such as GPUs.
In contrast, adapting an existing foundation model for a specific task or using it directly is far less costly, as it leverages pre-trained capabilities and typically requires only fine-tuning on smaller, task-specific datasets. -->

基盤モデルの初期の例は，OpenAI の GPT シリーズや Google の BERT のような言語モデル（LM）である[3,4]。
テキスト以外にも，画像の DALL-E や Flamingo[5]，音楽の MusicGen[6]，ロボット制御の RT-2[7] など，様々なモダリティの基盤モデルが開発されている。
また，天文学[8]，放射線学[9]，ゲノム学[10]，音楽[11]，コーディング[12]，時系列予測[13]，数学[14]，化学[15] などの分野でも基礎モデルが開発されている。
<!--Early examples of foundation models are language models (LMs) like OpenAI's GPT series and Google's BERT.[3][4]
Beyond text, foundation models have been developed across a range of modalities—including DALL-E and Flamingo[5] for images, MusicGen[6] for music, and RT-2[7] for robotic control.
Foundation models are also being developed for fields like astronomy,[8] radiology,[9] genomics,[10] music,[11] coding,[12] times-series forecasting,[13] mathematics,[14] and chemistry.[15] -->

スタンフォード人間中心人工知能研究所 (HAI) の基盤モデル研究センター (CRFM) は，2021 年 8 月 (16)に「基盤モデル」という用語を作り，「(一般的に大規模な自己教師を使って) 広範なデータで学習され，下流の幅広い課題に適応 (微調整など) 可能なあらゆるモデル」を意味するようになった(17)。
これは，既存の用語が重複するものの，適切ではないという観察に基づいており，「(大規模) 言語モデル」は，(焦点が) 言語だけではないことを考えると狭すぎる。
すなわt 事前学習モデル (pretrained model) は，注目すべき行動がすべて 事前学習後に起こったことを示唆している。
Foundational model ではなく foundation model  という用語が選ばれたのは，fundation model は fundational  (基礎的)  ではない方法で，これらのモデルが fundational principles (基本原理) を提供することを意味するからである。
<!-- The Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) coined the term "foundation model" in August 2021 to mean "any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks".
This was based on their observation that preexisting terms, while overlapping, were not adequate, stating that "'(large) language model' was too narrow given [the] focus is not only language; 'self-supervised model' was too specific to the training objective; and 'pretrained model' suggested that the noteworthy action all happened after 'pretraining.”
The term "foundation model" was chosen over "foundational model” because "foundational" implies that these models provide fundamental principles in a way that "foundation" does not. -->

政府が基盤モデルを規制するにつれ，新たな法的定義が生まれている。
<!-- As governments regulate foundation models, new legal definitions have emerged.-->

* 米国では，「人工知能の安全，確実かつ信頼できる開発および使用に関する大統領令」が，基盤モデルを「広範なデータに基づいて訓練され，一般的に自己監視を使用し，少なくとも数百億のパラメータを含み，広範な文脈にわたって適用可能なAIモデル」と定義している(21)。
* 米国では，Don Beyer 下院議員（バージニア州選出，民主党）と Anbna Eshoo 下院議員（カリフォルニア州選出，民主党）が提案した 2023 年 AI 基盤モデル透明化法(22)  は，基盤モデルを「広範なデータで訓練され，一般的に自己監視を使用し，一般的に少なくとも 1,000,000,000 のパラメータを含み，広範な文脈にわたって適用可能な人工知能モデル」と定義している(21)， また，安全保障，国家経済安全保障，国家公衆衛生もしくは安全，またはそれらの組み合わせに深刻なリスクをもたらす可能性のある課題において，高レベルの性能を示すか，または示すように容易に修正できる。
* 欧州連合 (EU) では，EU に関する欧州議会の交渉見解がある。
AI 法では，基盤モデルを「大規模データで学習され，出力の汎用性を考慮して設計され，幅広い特徴的な課題に適応できる AI モデル」と定義している。
* 英国では，競争市場庁の AI 基盤モデル：
英国では，競争市場庁の AI 基盤モデル：初期報告書[1]は，基盤モデルを 「膨大なデータで訓練され，幅広い課題や業務に適応できる AI 技術の一種」と定義している。

<!--* In the United States, the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence defines a foundation model as "an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts".[21]
* In the United States, the proposed AI Foundation Model Transparency Act of 2023[22] by House Representatives Don Beyer (D, VA) and Anna Eshoo (D, CA) defines a foundation model as "an artificial intelligence model trained on broad data, generally uses self supervision, generally contains at least 1,000,000,000 parameters, is applicable across a wide range of contexts, and exhibits, or could be easily modified to exhibit, high levels of performance at tasks that could pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters."
* In the European Union, the European Parliament's negotiated position on the E.U.
AI Act defines a foundation model as an "AI model that is trained on broad data at scale, is designed for generality of output, and can be adapted to a wide range of distinctive tasks".
* In the United Kingdom, the Competition and Markets Authority's AI Foundation Models:
Initial Report [1] defines foundations model as "a type of AI technology that are trained on vast amounts of data that can be adapted to a wide range of tasks and operations." -->

米国の定義は，基礎モデルの大きさについて言及している唯一のものであり，大きさについては異なっている。
また，Beyer と Eshoo の定義では，基礎モデルは潜在的な危険となるような性能レベルを達成しなければならないと規定している。
対照的に，EU の定義は，モデルが出力の汎用性を考慮して設計されていることを求めている。
どの定義も，基礎モデルは多くの領域で応用される可能性のある広範なデータで訓練されなければならないという点で一致している。
<!-- The United States's definitions are the only ones to make reference to the size of a foundation model, and differ on magnitude.
Beyer and Eshoo's definition also specifies that foundation models must achieve a level of performance as to be a potential danger.
In contrast, the E.U. definition requires the model to be designed for generality of output.
All definitions agree that foundation models must be trained on a broad range of data with potential applications in many domains. -->


1. Competition and Markets Authority (2023). AI Foundation Models: Initial Report. Available at: [https://assets.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_.pdf](https://assets.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_.pdf)
2. Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, and Raymond Perrault, "The AI Index 2023 Annual Report," AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2023.
3. Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). "A Primer in BERTology: What we know about how BERT works". [arXiv:2002.12327](https://arxiv.org/abs/2002.12327).
4. Haddad, Mohammed. "[How does GPT-4 work and how can you start using it in ChatGPT?](https://www.aljazeera.com/news/2023/3/15/how-do-ai-models-like-gpt-4-work-and-how-can-you-start-using-it)". Al Jazeera. Retrieved 20 October 2024.

---

16.  "[Introducing the Center for Research on Foundation Models (CRFM)](https://hai.stanford.edu/news/introducing-center-research-foundation-models-crfm)". Stanford HAI. 18 August 2021. Retrieved 11 June 2022.
17.  Bommasani, Rishi; et al. (18 August 2021). On the Opportunities and Risks of Foundation Models (Report). [arXiv:2108.07258](https://arxiv.org/abs/2108.07258).
18.   "[Reflections on Foundation Models](https://hai.stanford.edu/news/reflections-foundation-models)". Stanford HAI. 18 October 2021. Retrieved 22 May 2023.
19.   Bommasani, Rishi; Liang, Percy (18 October 2021). "[Reflections on Foundation Models](https://crfm.stanford.edu/2021/10/18/reflections.html)". Stanford CRFM. Retrieved 11 December 2023.
20.   Marcus, Gary (11 September 2021). "[Has AI found a new Foundation?](https://thegradient.pub/has-ai-found-a-new-foundation/)". The Gradient. Retrieved 11 December 2023.


### [DALL・E](https://arxiv.org/abs/1605.05396)

<div class="figcenter">
<img src="/2024assets/2025_0110chatGPT_DALLE1.jpg" style="width:33%;">
<Img src="/2024assets/2025_0110chatGPT_DALLE2.jpg" style="width:33%;">
</div>

<p align="center">
<img src="/2024assets/stable_diffusion.png" style="width:44%;">
<!-- <img src="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png" style="width:44%;"><br/> -->
<!-- <img src="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png" alt="sd-pipeline" width="500"/> -->
</p>


<div class="figcenter">
<img src="/2024assets/CLIP.png" style="width:77%;">
</div>
<div class="figcaption">
**CLIP の概要**<br/>
標準的な画像モデルが画像特徴抽出器と線形分類器を同時に学習し，何らかのラベルを予測するのに対し，CLIP は画像符号化器とテキスト符号化器を同時に学習し (画像とテキストの) バッチ学習例の正しいペアリングを予測する。
検証時に，学習したテキスト符号化器は，目標データセットのクラスの名前や説明を埋め込むことで，ゼロ撃の線形分類器を合成する。
<!-- Figure 1. Summary of our approach.
While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples.
At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes. -->
</div>

<div class="figcenter">
<img src="/assets/2015Ronneberger_U-Net_Fig1_ja.svg" style="width:55%;">
</div>


<div class="figcenter">
<img src="/2024assets/perturb_vp.gif" style="width:44%;">
<img src="/2024assets/denoise_vp.gif" style="width:44%;">
<div class="figcaption" style="width:44%;">
左: 連続時間確率過程によるデータのノイズへの摂動<!-- Perturbing data to noise with a continuous-time stochastic process. --><br/>
右: 摂動手順を逆にすることで，ノイズからデータを生成
<!-- Generate data from noise by reversing the perturbation procedure. -->
</div></div>

<div class="figcenter">
<img src="/2024assets/sde_schematic.jpg" style="width:55%;">
<!-- ![](../../assets/img/score/sde_schematic.jpg){.img-fluid .rounded .z-depth-1} -->
</div>
<div class="figcaption">
逆 SDE を解くと，スコアベースの生成モデルが得られる。
データを単純なノイズ分布に変換することは，SDE で達成できる。
各中間時間ステップにおける分布のスコアがわかれば，ノイズからサンプルを生成するために逆 SDE を解くことができる
。
<!-- Solving a reverse SDE yields a score-based generative model.
Transforming data to a simple noise distribution can be accomplished with an SDE.
It can be reversed to generate samples from noise if we know the score of the distribution at each intermediat
e time step. -->
</div>


<div class="figcenter">
<img src="/2024assets/teaser.jpg" style="width:77%;">
<!-- ![](../../assets/img/score/teaser.jpg){.img-fluid .rounded .z-depth-1} -->
<div class="figcaption">
SDE を使用してデータをノイズ分布 (事前分布) に写像し，この SDE を逆にして生成モデリングを行うことができる。
関連する確率フロー ODE を逆にすることもできる。
これにより，SDE と同じ分布からサンプリングする決定論的処理が生成される。
逆時間 SDE と確率フロー ODE はどちらも，スコア関数を推定することで取得できる。
<!-- We can map data to a noise distribution (the prior) with an SDE, and reverse this SDE for generative mode
ling.
We can also reverse the associated probability flow ODE, which yields a deterministic process that samples fro
m the same distribution as the SDE.
Both the reverse-time SDE and probability flow ODE can be obtained by estimating score functions. -->
</div></div>

<Img src="/assets/vae001-2.svg">
<img src="/2023assets/2019Kingma_Welling_vae_cartoon_ja.svg">
<img src="/2024assets/vae.svg">

<!-- <img src="/2024assets/mp4/carracing_vae_compare.mp4">
<img src="/2024assets/mp4/doom_real_vae.mp4"> -->
