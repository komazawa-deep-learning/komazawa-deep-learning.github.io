---
title: 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">
<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 25/Apr/2025<br/>
Appache 2.0 license<br/><br/>
</div>

# 本日のメニュー

1. 数式の読み方
2. 教師あり学習と教師なし学習
3. ロジスティック回帰とサボーとベクターマシン (SVM)
4. Eigneface and Fisherface


### Galileo Galilei (1623) The Assayer (translation by Stillman Drake) より

<div>

Sarsi: 哲学をする際には有名な作家の意見に基づかなければならないという確固たる信念があるように思える。
もしかしたら彼は，哲学とは『イーリアス』や『オーランド・フュリオーソ』のような，ある作家のフィクションであり，そこに書かれていることが真実かどうかが最も重要なことだと思っているのかもしれない。<br/>
Sarsi: 問題はそうではない。
哲学は宇宙という壮大な書物に書かれている。
しかし，まず言語を理解し，その文字を読むことを学ばなければ，その書物を理解することはできない。
<font style="color:navy;font-size:20pt;font-weight:600">この本は数学の言語で書かれており</font>，その文字は三角形や円などの幾何学的図形である，数学がなければ，人は暗い迷宮の中を彷徨うことになる。
</div>

<!-- > In Sarsi I seem to discern the firm belief that in philosophizing one must support oneself upon the opinion of some celebrated author, as if our minds ought to remain completely sterile and barren unless wedded to the reasoning of some other person.
Possibly he thinks that philosophy is a book of fiction by some writer, like the Iliad or Orlando Furioso, productions in which the least important thing is whether what is written there is true.
Well, Sarsi, that is not how matters stand.
Philosophy is written in this grand book, the universe, which stands continually open to our gaze.
But the book cannot be understood unless one first learns to comprehend the language and read the letters in which it is composed.
It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures without which it is humanly impossible to understand a single word of it; without these, one wanders about in a dark labyrinth. -->

上の文章の哲学とは，自然哲学，すなわち，現代では科学のことであろう.

# 実習

* [オリベッティ顔データベースを用いた機械学習実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_0425olivetti_face_detection.ipynb){:target="_blank"}
* [課題提出用フォルダ](https://drive.google.com/drive/u/3/folders/1QrIvRv5WKvx9psO9bUCWaPlqUnWLk52V){:target="_blank"}

---

[今際の際に黒板に書いてあったファインマンの言葉，カリフォルニア工科大学アーカイブ写真](http://archives.caltech.edu/pictures/1.10-29.jpg){:target="_blank"}

### 概念の整理

<div class="figcenter">
<img src="/2025assets/2018Kriegeskorte_fig2_new.svg" style="width:44%;">
<img src='/2025assets/2017Goodfelllow_Fig1_4rev.svg' width="44%"><br/>
左: Kriegeskorte and Douglas (2018) Fig.2 を改変<br/>
右: Goodfellow et al. (2017) Fig.1 を改変
<!-- <img src='/assets/2017Goodfellow_Fig1_4ja.svg' width="33%"><br/> -->
<!-- Goodfellow et al. (2017) Fig.1 を改変 -->
</div>

# 関連用語

- 機械学習 Machine Learning
- データサイエンス Data Science
- AI
- 統計学 Statistics
- 数学
- 情報理論，通信理論


## 数式の読み方と表記

* 関数:
    * $f(x)$: エフ オブ エックス  (この「オブ」を読むことが重要。なぜなら「エフエックス」と読んでしまうと，$f\times x$ の意味だと誤解するから)
    * $P(x)$: ピー オブ エックス (ピーを使う場合には，確率であることが多い)
* $\in$: イン 「含む」ことを意味する ($x\in D$)
* $\partial$: 偏微分の意。統一した読み方は無いが，「パーシャル」とは「ディファレンシャル」と読む人もいる。偏微分については取り上げない。
* $\top$: 行列の転置 (transpose) を表す。
* $\sim$: 「従う」を意味する
* $P(A\|B)$: 「ピー オブ エー ギブン ビー」と読む。縦棒 ($\|$) を条件付きであることを意味する。
    したがって，行頭の数式は，B が与えられた条件で A が起こる確率を意味する
* $\Delta$: 「デルタ」と読むが，差分を表す場合が多い。
* ギリシャ文字: $\alpha,\beta,\gamma,...,\mu,\sigma,\omega$ 統計学の伝統に従って，よく分からないものに使われる場合が多い。よく分からないとは，データ平均を $m$ (または $\bar{X}$) で表し，母集団平均を対応するギリシャ文字 $\mu$ で表す。
同様に，データ分散を $s^2$ で表し，対応する母集団分散を $\sigma^2$ で表す，などである。同じことであるが，$\pi$ は円周率を表すこともあるが，場合によっては，母集団確率を表すこともある。

* ベクトル: 太字の小文字 $\mathbf{x}$
* 行列: 太字の大文字 $\mathbf{W}$



## 機械学習

<div class="figcenter">
<img src="/assets/sklearn_map.svg" style="width:88%"><br/>
scikit-learn の カンペ (cheat sheet) を改変
</div>

- 重回帰，逆行列，線形代数
- 主成分分析，固有値，変分法，
- tSNE
- ロジスティック回帰
- サポートベクトルマシン SVM
<!-- - [番組 nothotdog について](https://komazawa-deep-learning.github.io/nothotdog/){:target="_blannk"}-->
<!-- [nothotdog 体感デモ](https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb)-->


<!-- - [初めての画像認識 <img src="/assets/colab_icon.svg">](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_ResNet50_demo.ipynb){:target="_blank"}-->
<!-- - [機械学習の超簡単デモ 伏線回収バージョン<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0507_3mnists_demo.ipynb){:target="_blank"} -->

<!-- ### 3 つのデータセット: MNIST, Fashion MNIST, KMNIST

- 機械学習分野で頻用されるデータセットとして，手書き数字認識データである MNIST があります。
- MNIST は FAIR (フェイスブック人工知能研究所) 現所長 の Yan LeCun によって作成されました。
NIST とは，アメリカ合衆国版の JIS です。すなわち，標準化機関の手書き数字認識用データセットを 修正した (modified) という意味から MNIST と呼ばれます。
- MNIST は データ数が ７万で，訓練データ数 6 万，テストデータ １ 万からなります。
データは，縦横それぞれ 28 画素からなっています。コンピュータで扱う際に，コンピュータにとってキリの良い 32 画素ではなく，
周囲を切り取ったために，28 画素になっています。
- Fasshion MNIST は， MNIST と同じ画像形式で，ファッション画像，具体的には 10 種類のアパレル画像データです。
- kmnist は日本語のくずし字データセットです。形式は MNIST, Fashion MNIST と同じです。 -->



## 数式の読み方と表記

* 関数:
    * $f(x)$: エフ オブ エックス  (この「オブ」を読むことが重要。なぜなら「エフエックス」と読んでしまうと，$f\times x$ の意味だと誤解するから)
    * $P(x)$: ピー オブ エックス (ピーを使う場合には，確率であることが多い)
* $\in$: イン 「含む」ことを意味する ($x\in D$)
* $\partial$: 偏微分の意。統一した読み方は無いが，「パーシャル」とは「ディファレンシャル」と読む人もいる。偏微分については取り上げない。
* $\top$: 行列の転置 (transpose) を表す。
* $\sim$: 「従う」を意味する
* $P(A\|B)$: 「ピー オブ エー ギブン ビー」と読む。縦棒 ($\|$) を条件付きであることを意味する。
    したがって，行頭の数式は，B が与えられた条件で A が起こる確率を意味する
* $\Delta$: 「デルタ」と読むが，差分を表す場合が多い。
* ギリシャ文字: $\alpha,\beta,\gamma,...,\mu,\sigma,\omega$ 統計学の伝統に従って，よく分からないものに使われる場合が多い。よく分からないとは，データ平均を $m$ (または $\bar{X}$) で表し，母集団平均を対応するギリシャ文字 $\mu$ で表す。
同様に，データ分散を $s^2$ で表し，対応する母集団分散を $\sigma^2$ で表す，などである。同じことであるが，$\pi$ は円周率を表すこともあるが，場合によっては，母集団確率を表すこともある。

* ベクトル: 太字の小文字 $\mathbf{x}$
* 行列: 太字の大文字 $\mathbf{W}$

`to`:$\to$, `mapsto`:$\mapsto$, `rightarrow`:$\rightarrow$ , `Rightarrow`:$\Rightarrow$

# 人工知能 AI とは何か

- 「人工知能の基礎」（小林 一郎）
    - 人の知能，つまり，人が行なう知的作業は，推論，記憶，認識，理解，学習，創造といった現実世界に適応するための能力を指す．
人工の「知能」とは，人の「知能」のある部分を機械に行わせることによって創られる．
- デジタル大辞泉 《artificial intelligence》コンピューターで，記憶・推論・判断・学習など，人間の知的機能を代行できるようにモデル化されたソフトウエア・システム．AI．

シャピロ (Shapiro, Stuart C., 1992) は次の 3 つの分野だと書いている。

1. 計算論的心理学 Computational Psychology:  __人間の知的活動を理解するために人間のように振る舞うコンピュータプログラムを作ること__
1. 計算論的哲学 Computational Philosophy:  __人間レベルの知的活動を計算論的に理解すること。計算論的理解=コンピュータ上に実装可能なモデル__
1. 計算機科学 Advanced Computer Science:  __コンピュータ科学の拡張，発展。現在のコンピュータはプログラムされたことしか実行できないが，人間はプログラムされていなくても勝手に振る舞う。__

* Shapiro, Stuart C. (1992), "Artificial Intelligence", in Stuart C. Shapiro (ed.), Encyclopedia of Artificial Intelligence, 2nd edition (New York: John Wiley & Sons)


# 時代背景

- 18世紀 第 1 次産業革命: <span style="color:Blue">蒸気機関，都市部に大規模工場が出現</span>
- 20世紀初頭 第 2 次産業革命: <span style="color:Blue">電気，オートメーション化，自動車，飛行機，電車による移動手段の変化</span>
- 20世紀後半 第 3 次産業革命: <span style="color:Blue">情報化，コンピュータ化，グローバル化</span>
- 21世紀から 第 4 次産業革命: <span style="color:Blue">AI 人間の能力を越える機械</span>

<!--from [http://bootcamp.lif.univ-mrs.fr:8080/mainpage](http://bootcamp.lif.univ-mrs.fr:8080/mainpage)-->

<center>
<img src='/assets/2009Gray_4th_paradigm.svg' style='width:66%'><br>
Gray (2009) The 4th paradigm より
</center>

# サポートベクターマシン (SVM: Support Vector Machines)

サポートベクターマシン（SVM）では、特に非線形分離可能なデータを取り扱う場合や、ある程度の誤分類が許容される場合に、誤分類を許容するためにスラック変数が導入される。
これらは、各データ点の誤分類の程度を測定し、コストパラメータ（C）によって、マージンの最大化と分類誤りの最小化とのトレードオフを制御する。
<!-- In Support Vector Machines (SVM), slack variables are introduced to allow for misclassifications, especially when dealing with non-linearly separable data or when some degree of error is acceptable. 
They measure the extent of misclassification for each data point, with the cost parameter (C) controlling the trade-off between maximizing the margin and minimizing classification errors.  -->

説明：
* **ハードマージン SVM**：<br/>
ハードマージン SVM は、誤分類のない、データ点を完全に分離する超平面を見つけることを目的としている。
しかし、このアプローチは、ノイズの多いデータや非線形分離可能なデータを扱う場合、過度に厳格になる可能性がある。
* **ソフトマージン SVM**：<br/>
ハードマージン SVM の制限を解決するため、ソフトマージン SVM はスラック変数 (グサイ $\xi$) を導入する。
これらの変数は、一部のデータ点の誤分類を許容し、ハードマージンの場合よりも厳格ではない「マージン」を効果的に作成する。
* **コストパラメータ（C）**：<br/>
コストパラメータ（C）は、マージンの最大化と誤分類エラーの最小化とのトレードオフのバランスを取る上で重要な役割を果たす。
C の値が大きいと誤分類が厳しく罰せられ、過学習を引き起こす可能性がある。
一方、C の値が小さいと誤分類を許容するが、より頑健なモデルとなる可能性がある。
* **スラック変数 (グサイ $\xi$)**<br/>
各データ点には対応するスラック変数が存在し、その点の誤分類の度合いを表す。
値が 0 は正しく分類された点を示し、0 より大きい値は誤分類された点を示し、値が大きいほど誤分類の度合いが大きくなる。
本質的に、スラック変数は SVM モデルに柔軟性を提供し、一部の誤分類を許容することで、より複雑なデータセットに対応し、汎化性能を向上させる。

<!-- Explanation: 
* Hard Margin SVM:<br/>
A hard margin SVM aims to find a hyperplane that perfectly separates data points, with no misclassifications. 
However, this approach can be overly strict, especially when dealing with noisy or non-linearly separable data. 
* Soft Margin SVM:<br/>
To address the limitations of hard margin SVM, soft margin SVM introduces slack variables (ξ). $\zeta$
These variables allow for some data points to be misclassified, effectively creating a "margin" that is not as strict as in the hard margin case. 
* Cost Parameter (C):<br/>
The cost parameter (C) plays a crucial role in balancing the trade-off between maximizing the margin and minimizing misclassification errors. 
A high value of C penalizes misclassifications heavily, potentially leading to overfitting, while a lower value of C allows for more misclassifications but can result in a more robust model. 
* Slack Variable (ξ):$\zeta$<br/>
Each data point has a corresponding slack variable, which represents the degree of misclassification for that point. 
A value of 0 indicates a correctly classified point, while a value greater than 0 indicates a misclassified point, with higher values indicating a greater degree of misclassification. 
In essence, slack variables provide flexibility to the SVM model, allowing it to handle more complex datasets and improve generalization performance by tolerating some errors in the training -->


[Soft Margin SVM: Exploring Slack Variables, the ‘C’ Parameter, and Flexibility](https://pub.aimind.so/soft-margin-svm-exploring-slack-variables-the-c-parameter-and-flexibility-1555f4834ecc){:target="_blank"}

