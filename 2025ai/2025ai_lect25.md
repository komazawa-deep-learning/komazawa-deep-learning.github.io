---
title: "第25回 2025年度開講 駒澤大学 人工知能"
author: "浅川 伸一"
layout: home
---

<div align="right">
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 05/Dec/2025<br/>
Appache 2.0 license<br/>
</div>

<link href="/css/asamarkdown.css" rel="stylesheet">

* [課題提出用フォルダ](https://drive.google.com/drive/u/6/folders/1F9pElOcwOIaSr7QW4PFUlBNAQgQbU4xz){:target="_blank"}
* [TD (時間差)学習, SARSA, 期待 SARSA, Q 学習 と Python 実装 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105Sarsa_Q_learning_expected_sarsa.ipynb){:target="_blank"}

# 本日のお品書き

1. 阿部 雄大さん，特別講義
2. 強化学習 の続き

# キーワード

信用割当問題，エピソード（軌跡，ロールアウト），方針最適化，


## 参考

* [DQN](https://komazawa-deep-learning.github.io/2020pytorch_tutorail_reinforcement_q_learning/){:target="_blank"}
* [REINFORCE.js](https://komazawa-deep-learning.github.io/reinforcejs/){:target="_blank"}

## 実習ファイル

- [ランダム探索  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)
- [方策勾配法  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)
- [SARSA  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)
- [Q学習  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb)

以下のデモは，[OpenAI](https://openai.com/) 提供の強化学習環境 [gym](https://gym.openai.com/) を用いています。

# DeepMind による強化学習

* [DQN: Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236){:target="_blank"}
* [AlphaGo: Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961){:target="_blank"}
* [AlphaGo Zero: Mastering the game of Go without human knowledge](https://www.nature.com/articles/nature24270){:target="_blank"}


<div class="figcenter">
<img src="/assets/2016AlphaGo_Fig1a.svg" style="width:44%;">
<img src="/assets/2016AlphaGo_Fig1b.svg" style="width:49%;"><br/>
AlphaGo の模式図，原著論文より
</div>

<!-- <div class="figcaption"> -->

**図 1. ニューラルネットワークの学習パイプラインとアーキテクチャ**<br/>
**a**  高速ロールアウト方針 $p_\pi$ と教師あり学習 (SL: supervised learning) 方針ネットワーク $p_\sigma$ は，棋盤上の位置データセットにおける人間の専門家の動きを予測するために学習される。
強化学習 (RL: Reinforcement Learning) 方針ネットワーク $p_\rho$ は教師あり学習方針ネットワークに初期化され，方針勾配学習によって方針ネットワークの以前のバージョンに対する結果 (すなわち，より多くのゲームに勝つこと) を最大化するように改善される。
新しいデータ集合は，強化学習方針ネットワークと自己対戦ゲームを行うことによって生成される。
最後に，価値ネットワーク $v_\theta$ が回帰によって訓練され，自己対戦データセットから位置における期待結果（つまり，現在のプレイヤーが勝つかどうか）を予測する。<br/>
**b**  AlphaGo で使用されているニューラルネットワークアーキテクチャの模式図。
方針ネットワークは，碁盤上の位置の表現 $s$ を入力とし，それをパラメータ $\sigma$ （教師あり学習方針ネットワーク）または $\rho$ （強化学習方針ネットワーク）を持つ多層畳み込み層に通し，碁盤上の確率地図によって表される，合理的な手 $a$ に関する確率分布 $p_{\sigma}(a|s)$ または $p_{\rho}(a|s)$ を出力する。
価値ネットワークも同様に，パラメータ $\theta$ を持つ多数の畳み込み層を使用するが，位置 $s'$ における期待結果を予測するスカラ値 $v_{\theta}(s')$ を出力する。
<!-- Figure 1. Neural network training pipeline and architecture.
a, A fast rollout policy pπ and supervised learning (SL) policy network pσ are trained to predict human expert moves in a data set of positions.
A reinforcement learning (RL) policy network pρ is initialized to the SL policy network, and is then improved by policy gradient learning to maximize the outcome (that is, winning more games) against previous versions of the policy network.
A new data set is generated by playing games of self-play with the RL policy network.
Finally, a value network vθ is trained by regression to predict the expected outcome (that is, whether the current player wins) in positions from the self-play data set.
b, Schematic representation of the neural network architecture used in AlphaGo. The policy network takes a representation of the board position s as its input, passes it through many convolutional layers with parameters σ (SL policy network) or ρ (RL policy network), and outputs a probability distribution σpa(|s) or ρpa(|s) over legal moves a, represented by a probability map over the board.
The value network similarly uses many convolutional layers with parameters θ, but outputs a scalar value vθ(s′) that predicts the expected outcome in position s′. -->
<!-- </div> -->

### アルファ碁ゼロにおける強化学習<!-- ### Reinforcement learning in AlphaGo Zero-->

我々の新手法は，パラメータ $\theta$ を持つ深層ニューラルネットワーク $f_{\theta}$ を使用する。
このニューラルネットワークは，位置とその履歴の生の碁盤表現 s を入力とし，手の確率と価値， $(p,v)=f_{\theta}(s)$ の両方を出力する。
手の確率のベクトル $p$ は，各手 $a$ (パスを含む) を選択する確率を表し，$p_a=Pr(a|s)$ である。
価値 $v$ はスカラ評価であり，現在のプレイヤーが位置 $s$ から勝つ確率を推定する。
このニューラルネットワークは，方針ネットワークと価値ネットワーク(12) の両方の役割を 1 つのアーキテクチャにまとめたものである。
ニューラルネットワークは，バッチ正規化(18) と整流非線形 (19) を持つ畳み込み層 (16,17) の多数の残差ブロック(4)  から構成される (「手続き」節参照)。
<!--Our new method uses a deep neural network fθ with parameters θ.
This neural network takes as an input the raw board representation s of the position and its history, and outputs both move probabilities and a value, (p,v)=fθ(s).
The vector of move probabilities p represents the probability of selecting each move a (including pass), pa=Pr(a|s).
The value v is a scalar evaluation, estimating the probability of the current player winning from position s.
This neural network combines the roles of both policy network and value network12 into a single architecture.
The neural network consists of many residual blocks4 of convolutional layers(16,17) with batch normalization18 and rectifier nonlinearities(19) (see Methods) -->

AlphaGoZero のニューラルネットワークは，新しい強化学習アルゴリズムによって，自己対局のゲームから学習される。
各位置 s において，ニューラルネットワーク $f_{\theta}$ によって導かれる MCTS 探索が実行される。
MCTS 探索は，各手を打つ確率 $\pi$ を出力する。
これらの探索確率は通常，ニューラルネットワーク $f_{\theta}(s)$ の生の手確率 $p$ よりもはるかに強い手を選択する。
したがって，MCTS は強力な方針改善演算子と見なすことができる (20,21)。
探索を伴う自己対局の各手を選択するために改善された MCTS に基づく方針を使用し，その後，価値のサンプルとしてゲームの勝者 z を使用するのは，強力な方針評価演算子と見なすことができる。
ニューラルネットワークのパラメータは，手の確率と値 $(p,v)=f_{\theta}(s)$ が，改善された探索確率と自己対局の勝者 $(\pi,z)$ とより密接に一致するように更新される。
これらの新しいパラメータは自己対局の次の反復で使用され，探索をより強力にする。
図 1 は自己対局の訓練パイプラインを示している。
<!-- The neural network in AlphaGo Zero is trained from games of selfplay by a novel reinforcement learning algorithm.
In each position s, an MCTS search is executed, guided by the neural network fθ.
The MCTS search outputs probabilities π of playing each move.
These search probabilities usually select much stronger moves than the raw move probabilities p of the neural network fθ(s); MCTS may therefore be viewed as a powerful policy improvement operator(20,21).
Self-play with search—using the improved MCTS-based policy to select each move, then using the game winner z as a sample of the value—may be viewed as a powerful policy evaluation operator.
The main idea of our reinforcement learning algorithm is to use these search operators repeatedly in a policy iteration procedure22,23: the neural network’s parameters are updated to make the move probabilities and value (p,v)=f_θ(s) more closely match the improved search probabilities and self-play winner (π,z); these new parameters are used in the next iteration of self-play to make the search even stronger.
Figure 1 illustrates the self-play training pipeline. -->

<div class="figcenter">
<img src="/2024assets/2017Silver_AlphaGoZero_fig1_.svg" style="width:44%;">
</div>

**図 1 AlphaGoZero における自己対戦強化学習**<br/>
**a**: プログラムは自分自身と対局 $s_1,\ldots,s_T$ を行う。
各位置 $s_t$ において，最新のニューラルネットワーク $f_\theta$ を用いて MCTS（モンテカルロ木探索）$\alpha_\theta$ が実行される（図 2 参照）。
移動は MCTS によって計算された探索確率に従って，$a_t\sim\pi_t$ で選択される。
終端位置 $s_T$ はゲームの勝者 $z$ を計算するためにゲームのルールに従って得点される。<br/>
**b**, AlphaGoZero におけるニューラルネットワークの学習。
ニューラルネットワークは生の盤面位置 $s_t$ を入力とし，それをパラメータ $\theta$ を持つ多層畳み込み層に通し，手に関する確率分布を表すベクトル $p_t$ と，現在のプレイヤーが位置 $s_t$ で勝利する確率を表すスカラー値 $v_t$ の両方を出力する。
ニューラルネットワークのパラメータ $\theta$ は，探索確率 $\pi_t$ に対する方針ベクトル $p_t$ の類似性を最大化し，予測勝者 $v_t$ とゲーム勝者 $z$ との誤差を最小化するように更新される（式(1)参照）。
新しいパラメータは，$a$ のように，次の自己プレイの繰り返しで使用される。
<!-- Figure 1 Self-play reinforcement learning in AlphaGo Zero.
a, The program plays a game s1, ..., sT against itself.
In each position st, an MCTS (Monte Carlo Tree Search) αθ is executed (see Fig. 2) using the latest neural network fθ.
Moves are selected according to the search probabilities computed by the MCTS, at ∼ πt.
The terminal position sT is scored according to the rules of the game to compute the game winner z.
b, Neural network training in AlphaGo Zero. The neural network takes the raw board position st as its input, passes it through many convolutional layers with parameters θ, and outputs both a vector pt, representing a probability distribution over moves, and a scalar value vt, representing the probability of the current player winning in position st.
The neural network parameters θ are updated to maximize the similarity of the policy vector pt to the search probabilities πt, and to minimize the error between the predicted winner vt and the game winner z (see equation (1)).
The new parameters are used in the next iteration of self-play as in a. -->

MCTS はニューラルネットワーク $f_\theta$ を用いてシミュレーションを行う (図 2)。
探索木の各辺 (s,a) には事前確率 $P(s,a)$，訪問回数 $N(s,a)$，行動価値 $Q(s,a)$ が格納されている。
各シミュレーションはルート状態から開始し，葉ノード s’ に遭遇するまで，$U(s,a)\sim P(s,a)/(1 + N(s,a)$) (文献12, 24) である上限信頼限界 $Q(s,a)+U(s,a)$ を最大にする手を反復的に選択する。
この葉の位置は，事前確率と評価 $(P(s',\cdot),V(s'))=f_\theta(s’)$ の両方を生成するために，ネットワークによって一度だけ展開され評価される。
シミュレーションで通過した各辺 (s,a) は，その訪問回数 N(s,a) をインクリメントするように更新され，その行為価値をこれらのシミュレーションの平均評価に更新する，$Q(s,a)=1/N(s,a)\sum_{s'|s,a\mapsto s'}V(s’)$ ここで s,a→s’ はシミュレーションが最終的に位置 $s$ から移動 $a$ をとって $s’$ に到達したことを示す。
<!-- The MCTS uses the neural network fθ to guide its simulations (see Fig. 2).
Each edge (s,a) in the search tree stores a prior probability P(s,a), a visit count N(s,a), and an action value Q(s,a).
Each simulation starts from the root state and iteratively selects moves that maximize an upper confidence bound Q(s,a)+U(s,a), where U(s,a)∝P(s,a)/(1 + N(s,a)) (refs 12, 24), until a leaf node s' is encountered.
This leaf position is expanded and evaluated only once by the network to generate both prior probabilities and evaluation, (P(s',·),V(s'))=f_\theta(s').
Each edge (s, a) traversed in the simulation is updated to increment its visit count N(s,a), and to update its action value to the mean evaluation over these simulations, Q(s,a)=1/N(s,a)\sum_{}V(s') where s,a→s' indicates that a simulation eventually reached s' after taking move a from position s. -->

<div class="figcenter">
<img src="/2024assets/2017Silver_AlphaGoZero_fig2_.svg" style="width:54%;">
</div>

**図 2. AlphaGo Zero における MCTS**<!-- Figure 2. MCTS in AlphaGo Zero.--><br/>
**a**, 各シミュレーションは，最大行動価値 Q と，保存された事前確率 P とそのエッジの訪問回数 N に依存する上限信頼区間 U を持つエッジを選択することによって木を探索する（これは一度探索されると増分される）。<br/>
**b**, 枝ノードが展開され，関連する位置 s がニューラルネットワーク $P(s,-),V(s))=f_{\theta}(s)$ によって評価される。<br/>
**c**, 行動価値 Q は，その行動の下の部分木内のすべての評価 V の平均を追跡するように更新される。<br/>
**d**, 探索が完了すると，$N^{1/\tau}$ に比例した探索確率 $\pi$ が返される。
ここで，N はルート状態からの各移動の訪問回数，$\tau$ は温度を制御するパラメータ。
<!-- a, Each simulation traverses the tree by selecting the edge with maximum action value Q, plus an upper confidence bound U that depends on a stored prior probability P and visit count N for that edge (which is incremented once traversed).
b, The leaf node is expanded and the associated position s is evaluated by the neural network (P(s,·),V(s))=f_θ(s); the vector of P values are stored in the outgoing edges from s.
c, Action value Q is updated to track the mean of all evaluations V in the subtree below that action.
d, Once the search is complete, search probabilities π are returned, proportional to N^{1/τ}, where N is the visit count of each move from the root state and τ is a parameter controlling temperature.-->

MCTS はニューラルネットワークパラメータ $\theta$ とルート位置 s が与えられると，打つべき手を推奨する探索確率のベクトル $\pi=a_\theta(s)$ を計算し，各手に対する指数化された訪問回数$\pi_{a}\propto N(s,a)^{1/\tau}$ に比例する自己プレイアルゴリズムと見なすことができる。
ここで $\tau$ は温度パラメータである。
<!-- MCTS may be viewed as a self-play algorithm that, given neural network parameters θ and a root position s, computes a vector of search probabilities recommending moves to play, π=α_θ(s), proportional to
the exponentiated visit count for each move, π_a∝N(s,a)^{1/τ}, where τ is a temperature parameter. -->

ニューラルネットワークは，MCTSを用いて各手を打つ自己再生強化学習アルゴリズムによって学習される。
まず，ニューラルネットワークはランダムな重み $\theta_0$ に初期化される。
続く各反復 $i\ge 1$ において，自己対局ゲームが生成される (図 1a)。
各タイム・ステップ $t$ で，ニューラルネットワーク $f_{\theta_{i-1}}$ の前の反復を用いて MCTS 探索 $\pi_t =a_{\theta_{t- i}}(s_t)$ が実行され，探索確率 $\pi_{t}$ をサンプリングすることによって手が打たれる。
ゲームは，両プレイヤーがパスしたとき，探索値が諦めの閾値を下回ったとき，またはゲームが最大長を超えたときにステップ $T$ で終了する。
その後，ゲームは $r_T\in\left[-1,+1\right]$ の最終報酬を与えるように採点される (詳細は方法参照)。
各タイムステップ $t$ のデータは $(s_t,\pi_t, z_t)$ として保存され $z_t=\pm r_T$ はステップ $t$ における現在のプレイヤーから見たゲームの勝者である。
並行して (図1b)，新しいネットワーク・パラメータ $\theta_i$ が，自己対局の最後の反復の全タイム・ステップ間で一様にサンプリングされたデータ $(s,\pi,z)$ から学習される。
ニューラルネットワーク $(p,v)=f_{\theta_{i}}(s)$ は，予測値 v と自己対局の勝者 z との間の誤差を最小化し，ニューラルネットワークの移動確率 $p$ と探索確率 $\pi$ との類似性を最大化するように調整される。
具体的には，パラメータ $\theta$ は，平均二乗誤差と交差エントロピー損失をそれぞれ合計する損失関数lの勾配降下によって調整される：
<!-- The neural network is trained by a self-play reinforcement learning algorithm that uses MCTS to play each move.
First, the neural network is initialized to random weights θ0. At each subsequent iteration i≥1, games of self-play are generated (Fig. 1a).
At each time-step t, an MCTS search π =αθ−t i 1(st) is executed using the previous iteration of neural network θ − f i 1 and a move is played by sampling the search probabilities πt.
A game terminates at step T when both players pass, when the search value drops below a resignation threshold or when the game exceeds a maximum length; the game is then scored to give a final reward of rT ∈ {− 1,+ 1} (see Methods for details).
The data for each time-step t is stored as (st, πt, zt), where zt = ± rT is the game winner from the perspective of the current player at step t.
In parallel (Fig. 1b), new network parameters θi are trained from data (s, π, z) sampled uniformly among all time-steps of the last iteration(s) of self-play.
The neural network = (p, v) fθ (s) i is adjusted to minimize the error between the predicted value v and the self-play winner z, and to maximize the similarity of the neural network move probabilities p to the search probabilities π.
Specifically, the parameters θ are adjusted by gradient descent on a loss function l that sums over the mean-squared error and cross-entropy losses, respectively: -->
$$\tag{1}
(\mathbf{p},v)=f_{\theta}(s) \text{ and } l=(z-v)^{2} - \pi^{\top}n \log\mathbf{p} + c\left\|\theta\right\|^{2}
$$
ここで c は L2 重みの正則化レベルを制御するパラメータである（過学習を防ぐため）。
<!-- where c is a parameter controlling the level of L2 weight regularization (to prevent overfitting). -->

<center>
<div style="text-align: left;width: 88%;background-color: powderblue;">
ムーブ３７:
ムーブ３７とはアルファ碁とイセドル氏との対局において，アルファ碁の打った決定的な３７手目（第四局）のことを指す。
アルファ碁の置いた３７手目の石はとくに評判になったため記憶に新しい。
ムーブ３７の意味は以下のように解釈できる。人類が積み上げてきた経験則である定石が，盤石とは限らないことを示した点にある。
囲碁の世界王者であっても経験に基づいた知識である。すべての可能な石の配置で構成される囲碁空間は膨大が量の人類が築き上げてきた経験の結集であっても，可能な囲碁配置空間の一部に過ぎず結果として偏っていた可能性がある。
このことを示してくれたのがムーブ３７といえる。
確かに千年以上の歳月をかけて人類が探索してきた碁石の可能な配置空間は広大である。
だが可能な囲碁の配置空間は，現在まで我われが探索してきた空間よりも広大であったのであり，アルファ碁なくしては人類が到達し得ない地平が存在したのである。
それまでは，知の地平の存在に気がついていた碩学が存在したとしても，具体例を示すことができず，従って説得力 を持った議論が展開できなかったのである。
</div>
</center>

* Mastering the game of Go without human knowledge

<center>
<div style="text-align: left;width: 88%;background-color: powderblue;">
人工知能の長年の目標は， 困難な領域でも超人的な能力をタブラ・ラサ方式で学習するアルゴリズムである。
最近では AlphaGo が囲碁の世界チャンピオンを破った初めてのプログラムとなった。
AlphaGo の木探索は， 深層ニューラルネットワークを用いて局面の評価と手の選択を行う。
このニューラルネットワークは，人間の熟練した手からの教師付き学習と， 自分自身の競技からの強化学習によって訓練されている。
ここでは， 人間のデータやガイダンス， ゲームルール以外の領域の知識を必要としない， 強化学習のみに基づいたアルゴリズムを導入する。
AlphaGo は自分自身の教師となり AlphaGo 自身の手の選択や AlphaGo のゲームの勝敗を予測するようにニューラルネットワークが学習される。
このニューラルネットワークは， 木探索の強度を向上させ， その結果， より質の高い手の選択が可能となり， 次の反復ではより強力な自己対戦が可能となる。
タブララサからスタートした私たちの新しいプログラム AlphaGo Zero は，既発表のチャンピオンに敗れた AlphaGo に対して 100-0 で勝利するという超人的な成績を達成した。
</div>
</center>

* [David Silver homepage](https://www.davidsilver.uk/)

## AlphaGo Zero<!-- # Case Study: AlphaGo Zero, Lilian Weng のブログ-->

[囲碁](https://en.wikipedia.org/wiki/Go_(game))は、ここ数年まで人工知能分野において極めて難しい問題だった。AlphaGo と AlphaGo Zero は、DeepMind のチームが開発した 2 つのプログラムである。両者とも深層畳み込みニューラルネットワーク（[CNN](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/#cnn-for-image-classification)）とモンテカルロ木探索（MCTS）を組み合わせており、プロ棋士のレベルに達することが実証されている。AlphaGo が人間の専門家による手筋の教師あり学習に依存したのとは異なり、AlphaGo Zero は基本ルール以外の人的知識を用いず、強化学習と自己対局のみによって開発された。
<!--The game of [Go](https://en.wikipedia.org/wiki/Go_(game)) has been an extremely hard problem in the field of Artificial Intelligence for decades until recent years. AlphaGo and AlphaGo Zero are two programs developed by a team at DeepMind. Both involve deep Convolutional Neural Networks ([CNN](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/#cnn-for-image-classification)) and Monte Carlo Tree Search (MCTS) and both have been approved to achieve the level of professional human Go players. Different from AlphaGo that relied on supervised learning from expert human moves, AlphaGo Zero used only reinforcement learning and self-play without human knowledge beyond the basic rules. -->

<div class="figcenter">
<img src="/2025assets/go_config.png" style="width:40%;">
<!-- ![](go_config.png){.center} -->
</div>
<div class="figcaption">

囲碁の碁盤。二人のプレイヤーが、19×19 の碁盤の空いている交点に、黒と白の石を交互に打つ。石の囲みは盤上に残るためには少なくとも一つの開いた点（交点、自由点と呼ばれる）を必要とし、また「生きている」状態を維持するためには少なくとも二つ以上の囲まれた自由点（目と呼ばれる）を必要とする。石は以前に置かれた位置を繰り返し置いてはならない。
<!-- Fig. 10. The board of Go. Two players play black and white stones alternatively on the vacant intersections of a board with 19 x 19 lines. A group of stones must have at least one open point (an intersection, called a "liberty") to remain on the board and must have at least two or more enclosed liberties (called "eyes") to stay "alive". No stone shall repeat a previous position. -->
</div>

上記の強化学習に関する知識を踏まえ AlphaGo Zero の仕組みを見ていく。主要な構成要素は、ゲーム盤の配置 (正確には [ResNet](https://lilianweng.github.io/posts/2017-12-15-object-)) に対して [深層CNN](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/#cnn-for-image-classification) である。このネットワークは 2 つの値を出力する：
<!-- With all the knowledge of RL above, let's take a look at how AlphaGo Zero works. The main component is a deep [CNN](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/#cnn-for-image-classification) over the game board configuration (precisely, a [ResNet](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/#resnet-he-et-al-2015) with batch normalization and ReLU). This network outputs two values: -->

$$
(p,v) = f_\theta(s) 
$$

- $s$: ゲームボードの構成。19×19×17 の積み重ねられた特徴平面。各位置に 17 の特徴。現在のプレイヤー用に 8 つの過去構成（現在を含む）＋対戦相手用に 8 つの過去構成＋色を示す 1 つの特徴（1=黒、0=白）。ネットワークが自身と対戦するため、現在のプレイヤーと対戦相手の色がステップ間で切り替わる。このため色を特にコード化する必要がある
- $p$: $19^2 + 1$ の候補（盤上の $19^2$ の位置に加え、パス）から手を選ぶ確率
- $v$: 現在の設定における勝利確率

<!-- - $s$: the game board configuration, 19 x 19 x 17 stacked feature planes; 17 features for each position, 8 past configurations (including current) for the current player + 8 past configurations for the opponent + 1 feature indicating the color (1=black, 0=white). We need to code the color specifically because the network is playing with itself and the colors of current player and opponents are switching between steps.
- $p$: the probability of selecting a move over 19^2 + 1 candidates ($19^2$ positions on the board, in addition to passing).
- $v$: the winning probability given the current setting. -->

自己対局中、MCTS は行動確率分布 $\pi\sim p(.)$ をさらに改善し、この改善されたポリシーから行動 $a_t$ をサンプリングする。報酬 $z_t$ は現在のプレイヤーが *最終的に* ゲームに勝利するかを示す2 値である。各手番はエピソードタプル $(s_t,\pi_t,z_t)$ を生成し、これは再生記憶に保存される。本記事では紙面の都合上、MCTS の詳細は省略する。興味があれば [原論文 https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf](https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?) を参照されたい。
<!-- During self-play, MCTS further improves the action probability distribution $\pi \sim p(.)$ and then the action $a_t$ is sampled from this improved policy. The reward $z_t$ is a binary value indicating whether the current player *eventually* wins the game. Each move generates an episode tuple $(s_t,\pi_t,z_t)$ and it is saved into the replay memory. The details on MCTS are skipped for the sake of space in this post; please read the original [paper](https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0) if you are interested. -->

<div class="figcenter">
<img src="/2025assets/alphago-zero-selfplay.png" style="width:50%;">
<!-- ![](alphago-zero-selfplay.png){.center} -->
</div>
<div class="figcaption">

図 11. AlphaGo Zero は自己対戦によって学習される一方、MCTS は各ステップで出力方針をさらに改善する。（画像出典：Silver+(2017)の図 1a）
<!-- Fig. 11. AlphaGo Zero is trained by self-play while MCTS improves the output policy further in every step. (Image source: Figure 1a in Silver et al., 2017). -->
</div>

ネットワークは、損失を最小化するために、リプレイメモリ内のサンプルを用いて学習される。<!-- The network is trained with the samples in the replay memory to minimize the loss: -->

$$
\mathcal{L} = (z - v)^2 - \pi^\top \log p + c \| \theta \|^2 
$$

ここで、$c$ は過学習を避けるための L2 ペナルティの強度を制御するハイパーパラメータである。
<!-- where $c$ is a hyperparameter controlling the intensity of L2 penalty to avoid overfitting. -->

AlphaGo Zeroは教師あり学習を排除し、分離されていた方針ネットワークと価値ネットワークを統合することで AlphaGo を簡素化した。その結果、AlphaGo Zero は大幅に短縮された学習時間で、はるかに優れた性能を達成した！ぜひこの 2 つの [論文](https://pdfs.semanticscholar.org/1740/eb993cc8ca81f1e46ddaadce1f917e8000b5.pdf) と [論文](https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0) を並べて読み、違いを比較すると非常に面白い。
<!-- AlphaGo Zero simplified AlphaGo by removing supervised learning and merging separated policy and value networks into one. It turns out that AlphaGo Zero achieved largely improved performance with a much shorter training time! I strongly recommend reading these [two](https://pdfs.semanticscholar.org/1740/eb993cc8ca81f1e46ddaadce1f917e8000b5.pdf) [papers](https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0) side by side and compare the difference, super fun.-->


# ****

- 強化学習というニューラルネットワークモデルがあるわけではない
- 動的で複雑な環境に対処 $\rightarrow$ **強化学習** + DL $\rightarrow$ 一般人工知能への礎

- DQN ATARIのビデオゲーム, [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)
- AlphaGo 囲碁, [https://www.nature.com/articles/nature16961](https://www.nature.com/articles/nature16961)
- AlphaGoZero 囲碁, [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)

# Deep Q Network

<center>

<img src="/assets/2015DQNFig1.svg" width="66%"><br/>
DQNの模式図, 原著論文より
</center>

<!--
- [ギャラガ1](/assets/MOV_0013s.mp4)
- [ギャラガ2](/assets/MOV_0071s.mp4)
-->

- **Q 学習** Q learning に DNN を採用
- CNN が LeNet, [@1998LeCun] そうであったように，強化学習 RL も昔からの技術 [@Sutton_and_Barto1998]
- ではなぜ，今になって囲碁や自動運転に応用できるようになったのか？
  - $\Rightarrow$ コンピュータの能力, データ規模，アルゴリズムの改良, エコシステム(ArXiv, Linux, Git, ROS, AMT, TensorFlow)

<!--
# 強化学習

- 強化学習 $\Rightarrow$ 意思決定
  - **エージェント** agent が **行動**(行為) action をする
  - 行動によって **状態** が変化する
  - **環境** から与えられる **報酬** によって**目標**が決定
- 深層学習: $\Rightarrow$ 表現，表象
  - 教師信号として目標が与えられる
  - 目標を達成するために外部状況の **表現** を獲得

<center>
<font size="+2" color="Green">強化学習 + 深層学習 = 人工知能</font>
</center>

  - 強化学習 $\Rightarrow$ 目標の設定
  - 深層学習 $\Rightarrow$ 内部表象の獲得機構を提供

# 用語の整理

- 教師信号なし **報酬信号** reward signal
- 遅延フィードバック

- **価値** Value
- **行為** Action
- **状態** State
- TD 学習
  - **Sarsa**
  - **Q 学習**
  - **アクタークリティック**
- 報酬 $R_t$: **スカラ値**
  - 時刻 $t$ でエージェントのとった行動を評価する指標
  - エージェントは**累積報酬** cumulative reward の最大化する
  - 報酬仮説: **目標は累積期待報酬の最大化として記述可能**
-->

## DQN 結果

<center>

<img src="/assets/2015Mnih_DQNFig.png" style="width:39%"><br/>
</center>


## YouTube 上でのデモ動画

* ブロック崩し: [https://www.youtube.com/watch?v=V1eYniJ0Rnk](https://www.youtube.com/watch?v=V1eYniJ0Rnk)
* スペースインベーダー: [https://www.youtube.com/watch?v=W2CAghUiofY](https://www.youtube.com/watch?v=W2CAghUiofY)

<!--- packman: [https://www.youtube.com/watch?v=r3pb-ZDEKVg](https://www.youtube.com/watch?v=r3pb-ZDEKVg)
- OpenMind selfplay: [https://www.youtube.com/watch?v=OBcjhp4KSgQ](https://www.youtube.com/watch?v=OBcjhp4KSgQ)

-->
<!--
* DQN の動画 スペースインベーダー

<center>

<video style="width:33%" controls src="/assets/2015Mnih_DQN-Nature_Video1.mp4" type="video/mp4" >
</center>

* DQN の動画 ブロック崩し

<center>

<div>
<video style="width:33%" controls src="/assets/2015Mnih_DQN-Nature_Video2.mp4" type="video/mp4" >
</div>
</center>
-->

<!-- ## なぜ DQN には難しいのか？

<center>
<div>
<video style="width:74%" controls src="/assets/Montezuma.mp4" type="video/mp4" /></br>
**Montezuma**
</div>
</center>

<center>
<video style="width:74%" controls src="/assets/PrivateEye.mp4" type="video/mp4" /></br>
**Private Eye**
</center>

<center>
<video width="39%" autoplay loop markdown="0" controls muted>
  <source src="./assets/Montezuma.mp4">
</video>
<video width="39%" autoplay loop markdown="0" controls muted>
  <source src="./assets/privateEye.mp4">
</video>
 -->

## 人間にはできて強化学習には難しいこと

- Montenzuma's Revenge の動画 [https://www.youtube.com/watch?v=Klxxg9JM5tY](https://www.youtube.com/watch?v=Klxxg9JM5tY)
- Private Eys の動画 [https://www.youtube.com/watch?v=OfyS-Wj1M78](https://www.youtube.com/watch?v=OfyS-Wj1M78)

<!---
## エージェントと環境

- At each step $t$ the agent:
  - Executes action $A_t$
  - Receives observation $O_t$
  - Receives scalar reward $R_t$
- The environment:
  - Receives action $A_t$
  - Emits observation $O_{t+1}$
  - Emits scalar reward $R_{t+1}$
- $t$ increments at env. step
-->

<!--
- **エージェント**: 学習と意思決定を行う主体
  1. **行動** action **$A_t$** を行い
  1. 環境の **観察** observation **$O_t$** を行う
  1. 環境からスカラ値の **報酬** reward **$R_t$** を受け取る
- **環境**: エージェント外部の全て
  1. エージェントから **行為** $A_t$ を受け取り
  1. エージェントに **観察** $O_{t+1}$ を与え
  1. エージェントへ **報酬** $R_{t+1}$ を与える

## エージェントの要素

- **方策** Policy
- **価値関数** Value function
- **モデル** エージェントが持つ環境の表象

## 方策 policy

- **方策** : エージェントの行為
- 決定論的方策: $a=\pi(S)$
- 確率論的方策: $\pi(a|s)=p(A_{t=a}|S_{t=s})$

## 価値関数
- 将来の報酬予測
- 状態評価(良/悪)
- 行為の選択
$$
v_\pi(S)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma R_{t+2} + \gamma^2R_{t+3}+\ldots|S_{t=s}\right\}
$$

## 強化学習のモデル
- 価値ベース
  - 方策:なし
  - 価値関数:あり
- 方策ベース
  - 方策:あり
  - 価値関数:なし
- アクター=クリティック Actor Critic
  - 方策: あり
  - 価値関数: あり

- モデルフリー
  - 方策，価値関数: あり
  - モデル: なし
- モデルベース
  - 方策，価値関数: あり
  - モデル: あり

## 探索と利用のジレンマ Exploration and exploitaion dilemma
- 過去の経験から，一番良いと思う行動ばかりをしていると，さらに良い選択肢を見つけ出すことができない **探索不足**
- 更に良い選択肢ばかり探していると過去の経験が活かせない **過去の経験の利用不足**

## 目標，収益，報酬

- エージェントの目標は累積報酬を最大化すること (報酬仮説)
  - **報酬仮説** Reward Hypothesis
  - 目標: 期待報酬の最大化

- 時刻 $t$ における報酬 $R_t$ : **スカラ値**
- 時刻 $t$ におけるエージェント行為の評価

## 逐次的意思決定 Sequential Decision Making
- 目標 Goal: 総収益を最大化する行動を選択すること
- 行為，行動 Actions は長期的結果
- 収益は遅延することも有る
- 直近の報酬を選ぶよりも，長期的な報酬を考えた方が良い場合がある


## 収益 Return
- **収益** return $G_t$: 割引付き収益
$$
G_t=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}
$$

- 割引率 The discount $\gamma\in\left\{0,1\right\}$ : 現時点から見た将来の報酬を計算するため

- **遅延報酬** delayed reward の評価
- $0$ に近ければ __近視眼的__ 評価
- $1$ に近ければ __将来を見通した__ 評価

## 価値関数 Value Function

- **状態価値関数 $v$ ** と **行動価値関数 $q$ **

- **価値関数** $v(s)$: gives the long-term value of state $s$
- **状態価値関数** $v(s)$ of an MRP is the expected return starting from state $s$
$$
v(s)=\mathbb{E}\left\{G_t|S_{t=2}\right\}
$$

- **状態価値関数** state-value function:
$$
v_\pi(s)=\mathbb{E}_\pi\left\{G_t|S_{t=s}\right\}
$$

# ベルマン期待期待 Bellman Expectation Equation
- **状態価値関数** : 即時報酬と後続状態の割引付き報酬の和に分解できる

$$
v_\pi(s)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma v_\pi(S_{t+1}|S_t=s)\right\}
$$

- **行動価値関数** action-value function:
$$
q_\pi(s,a)=\mathbb{E}_\pi\left\{G_t|S_{t}=s,A_{t}=a\right\}
$$

- **行動価値関数** 同じく分解可能 The action-value function can similarly be decomposed,
$$
q_\pi(s,a)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a\right\}
$$


## 最適価値関数 Optimal Value Function
- 最適状態価値関数
$$
v_{*}(s) = \max_{\pi} v_{\pi}(s)
$$
- 最適行動価値関数
$$
q_{*}(s,a)=\max q_{\pi}(s,a)
$$

- ベルマン方程式 一般に非線形になるので難しい
-->

<!--
- No closed form solution (in general)
- Many iterative solution methods
- 幾つかの解法:
  - 価値反復
  - 方策反復
  - Q 学習
  - sarsa
-->

<!--
  - Value Iteration
  - Policy Iteration
  - Q-learning
  - Sarsa
-->

<!--
# Markov Reward Process
A Markov reward process is a Markov chain with values.

- A Markov Reward Process is a tuple $<S,P,R,\gamma>$
  - $S$ is a finite set of states
  - $P$ is a state transition probability matrix,
  - $P_{ss'}=P\of{S_{t+1}=s'\given{S_t=s}}$
  - $R$ is a reward function, $R_s=\mathbb{E}\BRc{R_{t+1}\given{S_t=s}}$
  - $\gamma$ is a discount factor

# Bellman Equation for MRPs
The value function can be decomposed into two parts:
  - immediate reward $R_{t+1}$
  - discounted value of successor state $\gamma v\of{S_{t+1}}$

$$
\begin{array}{lll}
v\of{s}&=&\mathbb{E}\BRc{G_t\given{S_t=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma\Brc{R_{t+2}+\gamma R_{t+3}+\ldots}\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma G_{t+1}\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma v\of{S_{t+1}\given{S_{t}=s}}}
\end{array}
$$

# Bellman Equation for MRPs

$$
v\of{s}=\mathbb{E}\BRc{R_{t+1}+\gamma v\of{S_{t+1}}\given{S_t=s}}
$$


$$
v\of{s}=R_s +\gamma\sum_{s'\in S} P_{ss'}v\of{s'}
$$

# Solving the Bellman Equation
- The Bellman equation is a linear equation
- It can be solved directly:
$$
\begin{array}{lll}
v &=& R +\gamma Pv\\
\Brc{I - \gamma P}v &=& R\\
v &=& \Brc{I-\gamma P}^{-1}R
\end{array}
$$

- Computational complexity is $O(n^3)$ for $n$ states
- Direct solution only possible for small MRPs
- There are many iterative methods for large MRPs, e.g.
  - Dynamic programming
  - Monte-Carlo evaluation
  - Temporal-Difference learning
-->


<!-- # 価値関数 Value Function -->
<!-- - 将来の報酬予測の関数 -->
<!--   - ある状態である行動を起こすとどれほどの報酬が得られるか -->
<!-- - **Q-値関数** Q-value function : 総期待報酬を得る関数<\!--gives expected total reward-\-> -->
<!--   - 方策 $\pi$ のもとで -->
<!--   - 状態 $s$ で行動 $a$ を行ったとき -->
<!-- $$ -->
<!--   Q^\pi\of{s,a}=\mathbb{E}\BRc{r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots\given{s,a}} -->
<!-- $$   -->

<!--
# 最適価値関数 Optimal Value Functions
- 最大の価値を与える関数
$$
Q^*(s,a)=\max_{\pi}Q^\pi(s,a)=Q^{\pi^*}(s,a)
$$
- 最適価値関数 $Q^*$ が得られれば最適方策 $\pi^*$ を求めることができる
$$
\pi^*(s)=\operatorname{argmax}_aQ^*(s,a)
$$

- 全ての意思決定における最適価値:
$$
\begin{array}{lll}
Q^*(s,a)&=&r_{t+1}+\gamma\max_{a_{t+1}}r_{t+2}+\gamma^2\max_{a_{t+2}}r_{t+3}+\ldots\\
		   &=&r_{t+1}+\gamma\max_{a_{t+1}}Q^*(s_{t+1},a_{t+1})
\end{array}
$$
-->

<!-- from sliver (2016) icml lecture -->
<!--
- **ベルマン方程式** Bellman equation:
$$
Q^*(s,a)=\mathbb{E}_{s'}\left\{r+\gamma\max_{a'}Q^*(s',a')|s,a\right\}.
$$
-->

<!--
# 報酬(収益) Rewards
- 時刻 $t$ における報酬 $R_t$ : **スカラ値**
- 時刻 $t$ におけるエージェント行為の評価Indicates how well agent is doing at step $t$

# Sequential Decision Making
- 目標 Goal: select actions to maximise total future reward
- 行為 Actions はmay have long term consequences
- 収益は遅延することも有る
- 直近の報酬を選ぶよりも，長期的な報酬を考えた方が良い場合がある It may be better to sacrifice immediate reward to gain more long-term reward-
- Examples:
  - A financial investment (may take months to mature)
  - Refuelling a helicopter (might prevent a crash in several hours)
  - Blocking opponent moves (might help winning chances many moves from now)
-->

## RAINBOW, DQN 以降の発展

<center>
<img src="/assets/2017Hassel_RainbowFig1.svg" style="width:47%">
<img src="/assets/2017Hassel_RainbowRes1.svg" style="width:49%"><br/>

Rainbow の性能，
</center>

- カルパシーのブログ [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)

<center>
<img src="/assets/2017Hassel_RainbowFig1.svg" style="width:33%">

<img src="/assets/2017Hassel_Rainbow_fig3.svg" style="width:33%"><br/>
出典: @2018Hessel_Rainbow 左は各アルゴリズム単体の成績，右はアブレーション (ablation:廃止) 実験による性能低下
</center>

DDQN: 二重DQN，prioritized DQN 優先DQN，Dueling DQN 決闘DQN，A3C 非同期アドバンテージ，アクタークリティック, Distributional DQN 分散DQN，Noisy DQN

### 個別のゲームタイトル

<center>
<img src="/assets/Rainbow_appendix_Plots_GamesPub.svg" width="33%">
<img src="/assets/Rainbow_appendix_Plots_GamesAbl.svg" width="33%"><br/>

出典: Hassel+2018 <!--[@2018Hessel_Rainbow]--> の付録より
</center>


| Game               |  DQN|     A3C     |   DDQN | Prior. DDQN | Duel. DDQN | Distrib. DQN | Noisy DQN | Rainbow  |
|:------------------:|-----:|-----------:|-------:|------------:|-----------:|-------------:|----------:|----------:|
| ブロック崩し        |354.5 |**681.9**    |  368.9| 371.6       |      411.6 |        548.7 |   423.3   | 379.5    |
|モンテズーマの復習 | 47.0 |    67.0     |   42.0|        13.0 |       22.0 |        130.0 |     55.0  | **154.0**|
|プライベートアイ   |207.9 | 206.9       | -575.5|  179.0      |      292.6 |     5,717.5  |**5,955.4**| 1,704.4  |
|スペースインベーダー  |1293.8| **15,730.5**| 2628.7| 9,063.0     |    5,993.1 |      6,368.6 |   1,697.2 | 12,629.0 |

Human starts の評価: 訓練中に最高得点を獲得したエージェント・スナップショットから 200 回のテスト・エピソードで平均化した全ゲームの生得点。
DQN, A3C, DDQN, Dueling DDQN, Prioritized DDQN については，公表された得点を掲載。Distributional DQN と Rainbow については，著者自身によるエージェントの評価。
<!-- Human Starts evaluation regime: Raw scores across all games, averaged over 200 testing episodes, from the agent snapshot that obtained the highest score during training.
We report the published scores for DQN, A3C, DDQN, Dueling DDQN, and Prioritized DDQN. For Distributional DQN and Rainbow we report our own evaluations of the agents. -->


|             Game |        DQN  |   DDQN  | Prior. DDQN |    Duel. DDQN | Distrib. DQN |  Noisy DQN |  Rainbow |
|:----------------:|------------:|---------:|------------:|-------------:|-------------:|-----------:|---------:|
| ブロック崩し  |     385.5  |     418.5 |       381.5 |      345.3 |     **612.5**  |      459.1 |    417.5 |
| モンテズーマの復習 |       0.0 |       0.0 |         0.0 |        0.0 |          367.0 |        0.0 |   **384.0**|
| プライベートアイ |     146.7 |     129.7 |       200.0 |      103.0 |  **15,172.9**  |    3,966.0 |    4,234.0 |
| スペースインベーダー |    1692.3 |    2525.5 |     7,696.9 |    6,427.3 |        6,869.1 |    2,145.5 |  **18,789.0**|

No-op starts<br/>


No-op starts の評価: 訓練中に最高得点を得たエージェント・スナップショットから 200 回のテスト・エピソードで平均化した全ゲームの生得点。
DQN, DDQN, Dueling DDQN, Prioritized DDQN については，公表されている得点。Distributional DQN と Rainbow については，著者自身によるエージェントの評価。
A3C については，論文では no-ops 体制での得点が報告されていないため，掲載していない。
<!--
No-op starts evaluation regime: Raw scores across all games, averaged over 200 testing episodes, from the agent snapshot that obtained the highest score during training.
We report the published scores for DQN, DDQN, Dueling DDQN, and Prioritized DDQN. For Distributional DQN and Rainbow we report our own evaluations of the agents. A3C is not listed since the paper did not report the scores for the no-ops regime.-->

<!-- - 価値を $V$ で表せば， $V = \pi(s)$ あるいは $V = Q(s,a)$ である。あるいは $Q(a\vert s) = p(a\vert s)$ と書けば，条件付き確率の表記法ににて，状況 $s$ において，行為 $a$ を行った場合の価値のようにも表記することがある。
- $Q$ が $\theta$ をパラメータとするニューラルネットワークで表されているとすると，以下のように表記すれば，ニューラルネットワークにおける学習と同一の表記となる

$$
\Delta\theta = \alpha\nabla J(\theta) = \frac{\partial}{\partial \theta}J(\theta)
$$

$$
\Delta\theta = \alpha\nabla_\theta \pi(s) = \alpha\nabla \pi(s;\theta) = \frac{\partial}{\partial \theta}\pi(s;\theta)
$$-->

# REINFORCE: Monte Carlo Policy Gradient
<!-- @2018SuttonBartoRL chapt 13.3 -->

<!-- # アタリのゲームによるランキング

- [Atari Games on Atari 2600 Freeway](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)
-->

<center>
<img src="/2022assets/2020-1019paperswithcode_sota_atari_games_on_atari_2600_freeway.svg" style="width:49%"><br/>
</center>

ただし，Q 学習では不安定であった。DQN では， **経験再生** を用いることで，系列間相関と，教師信号の安定性を解決


# 二重 DQN (Double DQN)

[@2015Hasselt_doubleDQN]

最大の Q を求めるときに，自身の Q の計算が入っている，これを 2 つの Q 関数を並行して用いることで解消。

<!--
#### 分散優先体験リプレイ (Distributed Priooritized replay (APE-X))
@2016Schaul_prioritized_replay;@2018Horgan_APE-X-->

<!--
#### 逆強化学習 (Inverse RL)
@1998Russell_inverse_reinforcement_learning;@2000NgRussell_InverseRL

逆とは，価値の推定-->

Q 学習の際に，Q を最大にする行動を使って，Q の値を更新することが行われる。
具体的な更新式の核心部分は以下のとおり:

$$
R_{t+1} + \gamma Q\left(S_{t+1},\arg\max_a Q\left(S_{t+1},a\right)\right).
$$

Q を評価する際に，内部で Q を評価している。Q の評価が不当になる可能性があるので，Q の評価と更新とを別々の Q 関数を用意して，公平性を保証する

# 優先的経験再生 Prioritized replay.

<!-- DQN samples uniformly from the replay buffer. Ideally, we want to sample more frequently those transitions from which there is much to learn.
As a proxy for learning potential, prioritized experience replay (Schaul et al. 2015) samples transitions with probability ptrelative to the last encountered absolute TD error: -->

- DQN では 再生バッファから一様にサンプリングする。
- 理想的には 学習すべきことが多い遷移をより頻繁にサンプリングしたい
- そこで **優先的経験再生** (Schaul+2015) では 最後に遭遇した誤差が大きな系列を優先的にサンプリング

$$
p_t\propto \left|
R_{t+1}+\gamma_{t+1}\max_{a'} Q(s_{t+1},a') - Q(s_t,a_t)
\right|^\omega
$$

$\omega$ はハイパーパラメータで，$\omega=0.5$

# 決闘ネットワーク (Dueling Network)
@2016Wang_dueling。 Q 関数（⾏動価値）を価値関数と⾏動との差 (アドバンテージ) として分岐したニューラルネットワークで表現

<center>
<img src="/assets/2016duelingNet.svg" style="width:33%"><br/>
ドュエリング(決闘) ネットワークの模式図 (Wang+2016)
</center>

<!-- 決闘ネットワークは，価値ベースの RL のために設計されたニューラルネットワークアーキテクチャ。
これは，価値ストリームとアドバンテージストリームという 2 つの計算ストリームを特徴としており，畳み込み符号化器を共有し，特別な集計装置 (アグリゲータ)によってマージされます (Wang et al. 2016)。
これは，アクション値の以下の因数分解に対応しています。
The dueling network is a neural network architecture designed for value based RL.
It features two streams of computation, the value and advantage streams, sharing a convolutional encoder, and merged by a special aggregator (Wang et al. 2016).
This corresponds to the following factorization of action values:-->

$$
Q(s,a) = v(s) + a(s,a) - \frac{\sum_{a'}a(s,a')}{N_{\text{行動}}}
$$

# A3C

### アクタークリティック actor critic

- AC: アクター（行為者） Actor と クリティック (Critic) 批評家。アクターは方策（ポリシー）の改善を行い，クリティックは価値の更新を行う。Q 学習は，アクターとクリティックの両者を含む。
- アドバンテージ: $Q(s,a) - V(s)$ Q の引数は 状態と行為との２つから，報酬を定義，一方 価値関数とは 状況 から報酬を定義なので，この差をアドバンテージと呼ぶ

- Asyncronous Advantage Actor Critic

- A3C とは，アドバンテージ付きの DQN を非同期更新したもの

<!-- #### 逆強化学習 Inverse Reinforcement Learning

@2000NgRussell_InverseRL

観察された行動から報酬関数を推測する。-->

<!-- # マルチステップ強化学習 (Multi-step RL)

@Sutton_and_Barto1998 DQNでは1-stepの報酬を⽤いて，教師データを作成しているが，これをn-stepに拡張することで，学習が促進される場合がある -->

# 分散 DQN あるいはカテゴリカル DQN とも呼ばれる (Categorical DQN)

@2017Bellemare_C51

<!-- We can learn to approximate the distribution of returns instead of the expected return.

Recently Bellemare, Dabney, and Munos (2017) proposed to model such distributions with probability masses placed on a discrete support z, where z is a vector with $N_{\text{atoms}}\in\matcal{N}^{+}\text{atoms}$, defined by $z^i = v_{\min} + (i-1)\frac{v_{\max}-v_{\min}}{V_{\text{atom}}-1}$.
The approximating distribution dt attime t is defined on this support, with the probability mass pi(St;At) on each atom i, such that dt = (z; p(St;At)).
The goal is to update such that this distribution closely matches the actual distribution of returns. -->

期待報酬を離散値ではなく，N 分割したヒストグラムとして表現，


# ノイズネットワーク (Noisy networks)

@2018Fortunato_noisy_networks

<!-- #### 優先体験レプレイ (Prioritized Experience Replay)

- DQN では Experience Reply の活⽤が学習の効率化に寄与していた。
- それは，経験をランダムにサンプルすることで i.i.d. に近づく，および 学習に有⽤なレアな経験の再利⽤性を⾼める。
だが，経験をストアするためには⼤きな記憶領域が必要。
再⽣する記憶には，学習を促進するものとそうではないものがあるはずで，importance sampling によって悪影響を緩和 -->

<!-- The limitations of exploring using $\epsilon$-greedy policies are clear in games such as Montezuma’s Revenge, where many actions must be executed to collect the first reward.
Noisy Nets (Fortunato et al. 2017) propose a noisy linear layer that combines a deterministic and noisy stream, -->

イプシロン 貪欲 ($\epsilon$-グリーディ) な探索を用いる時の効率の悪さを乱数を加えることで軽減。
経験を保持するためには⼤きな記憶領域が必要。
<!--再⽣する記憶には，学習を促進するものとそうではないものがあるはずで，importance samplingによって悪影響を緩和-->
例えば，モンテズーマの復讐では，鍵を探し出して，その鍵を使って局面を打開するまでに要するエピソードの長さから，最初に報酬を得られるまでのエピソード系列が長いので，探索行動の効率が悪い。

# 非同期 強化学習

- 非同期更新については省略します。ですが，複数のエージェントを同時に実行し，その結果を束ねることで性能の改善が見込まれます。下図を見てください。

- A3C, [Gorila](Massively Parallel Methods for Deep Reinforcement Learning), [IMPALA](https://arxiv.org/abs/1802.01561), [APE-X](https://arxiv.org/abs/1803.00933), [NGU](https://arxiv.org/abs/2002.06038) (Never Give Up!)

<center>
<img src="/assets/2018Hogan_APE-X_fig2.svg" style="width:30%">
<img src="/assets/https___qiita-image-store.s3.amazonaws.com_0_144878_9297076e-65e3-9dac-763d-66d646969397.png" style="width:35%">
</center>

- [Never Give Up ICLR 2020 デモビデオ](https://sites.google.com/view/nguiclr2020)

<!-- - モンテズーマ・リベンジ オンライン版 <https://www.retrogames.cz/play_124-Atari2600.php>

<center>
<video width="15%" autoplay loop markdown="0" controls muted>
<source src="assets/openai_atari-reset_git_monte_video.mp4">
</video><br/>
出典: モンテズーマの復讐の解<https://github.com/openai/atari-reset>
</center> -->

# まとめ

- RAINBOW とは，複数の改善手法の全部乗せ。(RAINBOW = DDQN + Prioritized DDQN + Dueling DQN + A3C + Distributional DQN + Noisy DQN)
- アブレーションによる評価から，複数の手法が同時に性能向上に関与している
- 非同期更新 GORILA, APE-X, R2D2, NGU



# Agent57

<!-- # Conclusions and the future-->
Agent57 では，Atari ゲーム 57 ベンチマークの全ゲームで人間以上の性能を持つ，より一般的な知的エージェントの構築に成功した。
Agent57 は，以前のエージェント Never Give Up の上に構築され，エージェントがいつ探索し、いつ利用するか，また，どの時間地平で学習するのが有用かを知るのに役立つ適応的メタコントローラを実体化したものである。
幅広いゲーム課題では，当然これらのトレードオフの選択を変える必要があるため，メタコントローラはこのような選択を動的に適応させる方法を提供している。

Agent57 は，学習時間が長いほど得点が高くなり，計算量の増加に対応することができた。
これにより，Agent57 は強力な一般的成績を達成することができたが，多くの計算と時間がかかりるので，データ効率は確実に向上させることができる。
さらに，Agetn 57 は，Atari 57ゲームのセットでより良い 5 パーセンタイルの成績を示した。
これは，データ効率だけでなく，一般性能の点でも，決して Atari の研究の終わりを意味するものではない。
このことについて，2 つの見解を示す。

1. パーセンタイル間の成績を分析することで，Agent57 アルゴリズムがいかに一般的であるかについて新しい洞察を得ることができる。
Agent57 は 全 57 ゲームの最初のパーセンタイルで強力な結果を達成し，MuZero で示されるように NGU や R2D2 よりも良い平均値と中央値を保持しているが，それでもより高い平均性能を得ることができる。
2. 現在のすべてのアルゴリズムは，いくつかのゲームにおいて最適な性能を達成するには程遠いということである。
そのため，Agent57 が探索，計画，信用割当てに使用する表現を強化することが，使用する上で重要な改善点となるかもしれない。

<!-- With Agent57, we have succeeded in building a more generally intelligent agent that has above-human performance on all tasks in the Atari57 benchmark.
It builds on our previous agent Never Give Up, and instantiates an adaptive meta-controller that helps the agent to know when to explore and when to exploit, as well as what time-horizon it would be useful to learn with.
A wide range of tasks will naturally require different choices of both of these trade-offs, therefore the meta-controller provides a way to dynamically adapt such choices.

Agent57 was able to scale with increasing amounts of computation: the longer it trained, the higher its score got.
While this enabled Agent57 to achieve strong general performance, it takes a lot of computation and time; the data efficiency can certainly be improved.
Additionally, this agent shows better 5th percentile performance on the set of Atari57 games.
This by no means marks the end of Atari research, not only in terms of data efficiency, but also in terms of general performance.
We offer two views on this: firstly, analyzing the performance among percentiles gives us new insights on how general algorithms are.
While Agent57 achieves strong results on the first percentiles of the 57 games and holds better mean and median performance than NGU or R2D2, as illustrated by MuZero, it could still obtain a higher average performance.
Secondly, all current algorithms are far from achieving optimal performance in some games.
To that end, key improvements to use might be enhancements in the representations that Agent57 uses for exploration, planning, and credit assignment. -->


### DQN からの改善
* 二重 DQN
* 優先度付き経験再生
* 決闘 (dueling)
* 分散化

#### 短期記憶
* LSTM, GRU $\rightarrow$ R2D2

#### エピソード記憶
* メモリーネットワーク
* ニューラルエピソディック制御
* トランスフォーマー

#### 探索
* 好奇心
* 内発的動機づけ

#### メタ制御


### エージェント 57 の先祖

2012 年 DeepMind は Atari57 ゲーム群に取り組むために Deep Q network エージェント（DQN）を開発した。
以来，研究コミュニティは DQN の多くの拡張機能や代替機能を開発してきた。
しかし，これらの進歩にもかかわらず，すべての深層強化学習エージェントは 4 つのゲーム で一貫してスコアを出すことができなかった。
4 つのゲーム: モンテズマの復讐，ピットフォール，ソラリス，スキー

モンテズマの復讐 と ピットフォール は，良好な成績を得るために広範な探索が必要となる。
学習における中心的なジレンマは，探索と利用の問題である。
例えば，地元のレストランでいつも同じお気に入りの料理を注文すべきか，それとも，昔からのお気に入りを上回るかもしれない何か新しいものを試すべきなのか，である。
探索活動には，最終的により強力な行動を発見するために必要な情報を収集するために行われる。
このとき，多くの最適でない行動を取ることが含まれる。

ソラリス と スキー では長期的な信用割り当ての問題がある。 ソラリスとスキーでは，エージェントの行動の結果とそれが受け取る報酬を一致させることが困難である。
エージェントは学習に必要なフィードバックを得るために，長い時間スケールで情報を収集しなければならない。

<center>

[エイリアン](https://youtu.be/luZm3jmwGwI?list=PLqYmG7hTraZCHS3JLle_kxwNvImpYVq4z) :<br/>

<iframe width="600" height="320" src="https://www.youtube.com/embed/luZm3jmwGwI?list=PLqYmG7hTraZCHS3JLle_kxwNvImpYVq4z" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<!--
<center>
* ソラリス:<br/> <video width="38%" repeat controls><source src="https://youtu.be/QDb3rmEBTZI"></video>
* スキー:<br/> <video width="38%" repeat controls><source src="https://youtu.be/Lppco8heTxI"></video>
* モンテズマの復讐: <video width="38%" repeat controls><source src="https://youtu.be/M9Yn1kYZb6E"></video>
* ピットフォール: <video width="38%" repeat controls><source src="https://youtu.be/LRRq3BXh0xk"></video>
</center>
-->

<center>

<img src="/assets/2020-1029agent57_1.svg" style="width:49%">
<!--- <img
src="https://kstatic.googleusercontent.com/files/f6b5f285173d4449285a8e812b8385f45c03f7104e1c41370a73e0c8558ff82d6a69e60962dd91c4972c444fd73bc4f98a06b5487eff5a037a37bc42f97cef3b" -->
</center>


### DQN の改善
<!-- # DQN improvements-->

DQN の初期の改良では，二重 DQN，経験の優先再生，決闘アーキテクチャなど，学習効率と安定性が向上した。
これらの変更により，エージェントは経験をより効率的かつ効果的に利用できるようになった。

<!--Early improvements to DQN enhanced its learning efficiency and stability, including double DQN, prioritised experience replay and dueling architecture. These changes allowed agents to make more efficient and effective use of their experience.-->

### 分散エージェント
<!-- ## Distributed agents-->

* 複数のコンピュータ上で同時に実行できる分散型の DQN, Gorila, Ape-X を導入
* エージェントはより迅速に経験を獲得し，経験から学ぶことが可能となった。
* アイデアを迅速に反復することができるようになった。
* Agent57 もまたデータ収集と学習処理と切り離した分散型強化学習エージェント
* 多くのアクターが環境の独立したコピーと相互作用し，優先順位付けされた経験再生バッファの形で中央の「メモリバンク」にデータを供給する。
<!--図 4 に示すように，-->
学習者はこの経験再生バッファから訓練データをサンプリングし，
<!--学習者は，-->これらの経験再生を使って損失関数を構築し，行動やイベントのコストを推定する。
* 損失を最小化することでニューラルネットワークのパラメータを更新する。
* <!--最後に，--> 各アクターは学習者と同じネットワーク・アーキテクチャを共有する。
<!--だが，重み係数の独自のコピーを持ちます。-->
* 学習者の重み係数はアクターに頻繁に送られ，後述するように，アクターは個々の優先順位によって決定された方法で自分の重みを更新する

<!-- Next, researchers introduced distributed variants of DQN, Gorila DQN and ApeX,  that could be run on many computers simultaneously. This allowed agents to acquire and learn from experience more quickly, enabling researchers to rapidly iterate on ideas. Agent57 is also a distributed RL agent that decouples the data collection and the learning processes. Many actors interact with independent copies of the environment, feeding data to a central ‘memory bank’ in the form of a prioritized replay buffer. A learner then samples training data from this replay buffer, as shown in Figure 4, similar to how a person might recall memories to better learn from them.  The learner uses these replayed experiences to construct loss functions, by which it estimates the cost of actions or events. Then, it updates the parameters of its neural network by minimizing losses. Finally, each actor shares the same network architecture as the learner, but with its own copy of the weights. The learner weights are sent to the actors frequently, allowing them to update their own weights in a manner determined by their individual priorities, as we’ll discuss later.  -->

<center>

* [ソラリス](https://youtu.be/QDb3rmEBTZI)<br/>
<iframe width="560" height="320" src="https://www.youtube.com/embed/QDb3rmEBTZI?list=PLqYmG7hTraZBuNkJn6YFhi7TYrAg_NDAr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<center>
<img src="/assets/2020-1029agent57_2.svg" style="width:66%"><br/>
図4: Agent57 の分散設定
</center>


#### 短期記憶<!-- ## Short-term memory-->

エージェントは，過去の観察を考慮に入れて意思決定を行うために，記憶を持つ必要がある。
これにより，エージェントは，現在の観察 (通常は部分的な観察，つまり，エージェントは自分の世界の一部しか見ていない)だけでなく，環境全体についてのより多くの情報を明らかにすることができる過去の観察に基づいて意思決定を行うことができるようになる。
例えば，ある建物内の椅子の数を数えるために，エージェントが部屋から部屋へと移動するタスクを考えてみよ。
記憶を持っていなければ，エージェントはある部屋の観察にしか頼ることができない。
記憶を持っていれば，エージェントは前の部屋の椅子の数を記憶し，現在の部屋で観察している椅子の数を足すだけでタスクを解くことができる。
したがって，記憶の役割は，過去の観察から得られた情報を集約して意思決定プロセスを改善することである。
深層強化学習では，短期記憶として LSTM（Long-Short Term Memory）などのリカレントニューラルネットワークが用いられる。
<!-- Agents need to have memory in order to take into account previous observations into their decision making. This allows the agent to not only base its decisions on the present observation (which is usually partial, that is, an agent only sees some of its world), but also on past observations, which can reveal more information about the environment as a whole. Imagine, for example, a task where an agent goes from room to room in order to count the number of chairs in a building. Without memory, the agent can only rely on the observation of one room. With memory, the agent can remember the number of chairs in previous rooms and simply add the number of chairs it observes in the present room to solve the task. Therefore the role of memory is to aggregate information from past observations to improve the decision making process. In deep RL and deep learning, recurrent neural networks such as Long-Short Term Memory (LSTM) are used as short term memories. -->

記憶と行動とのインターフェースは，自己学習するシステムを構築する上で極めて重要である。
強化学習では，エージェントは，その直接的な行動の価値のみを学習することができるオンポリシー学習者になることができる。
あるいは，それらの行動を実行していなくても最適な行動について学習することができるオフポリシー学習者になることができる。
--- 例えば，ランダムな行動を取っているかもしれないが，可能な限り最高の行動が何であるかを学習することができる。
したがって，オフポリシー学習はエージェントにとって望ましい特性であり，エージェントが自分の環境を徹底的に探索しながら，取るべき最善の行動のコースを学ぶのを助けることができる。
オフポリシー学習と記憶を組み合わせるのは難しいが，異なる行動を実行するときに何を覚えているかを知る必要があるからである。
例えば，リンゴを探しているときに覚えていること（例えば，リンゴがどこにあるか）は，オレンジを探しているときに覚えていることとは異なる。
しかし，オレンジを探していたとしても，将来的にリンゴを探す必要が出てきた場合に備えて，偶然リンゴに出くわしたとしても，リンゴを探す方法を覚えることができる。
メモリとオフポリシー学習を組み合わせた最初の 深層強化学習エージェント は，ディープリカレント Q-Network(DRQN) であった。
さらに最近になって，短期記憶のニューラルネットワークモデルとオフポリシー学習と分散学習を組み合わせた Recurrent Replay Distributed DQN (R2D2)
で Agent57 の系譜に大きな飛躍が発生し，Atari57 では非常に強い平均性能を達成した。
R2D2 は，過去の経験からの学習のための経験再生の仕組みを短期記憶に働きかけるように修正している。
以上のことから，R2D2 は収益性の高い行動を効率的に学習し，報酬のためにそれを利用することができた。
<!--Interfacing memory with behaviour is crucial for building systems that self-learn. In reinforcement learning, an agent can be an on-policy learner, which can only learn the value of its direct actions, or an off-policy learner, which can learn about optimal actions even when not performing those actions – e.g., it might be taking random actions, but can still learn what the best possible action would be.  Off-policy learning is therefore a desirable property for agents, helping them learn the best course of action to take while thoroughly exploring their environment. Combining off-policy learning with memory is challenging because you need to know what you might remember when executing a different behaviour. For example, what you might choose to remember when looking for an apple (e.g., where the apple is located), is different to what you might choose to remember if looking for an orange. But if you were looking for an orange, you could still learn how to find the apple if you came across the apple by chance, in case you need to find it in the future. The first deep RL agent combining memory and off-policy learning was Deep Recurrent Q-Network (DRQN). More recently, a significant speciation in the lineage of Agent57 occurred with Recurrent Replay Distributed DQN (R2D2), combining a neural network model of short-term memory with off-policy learning and distributed training, and achieving a very strong average performance on Atari57.  R2D2 modifies the replay mechanism for learning from past experiences to work with short term memory. All together, this helped R2D2 efficiently learn profitable behaviours, and exploit them for reward. -->


#### エピソード記憶<!-- ## Episodic memory-->

ネバーギブアップ (NGU) は，R2D2 にエピソード記憶という別の形の記憶を補強するように設計された。
NGU  はゲームの新しい部分に遭遇したときにそれを検知し，エージェントが報酬を得るためにその新しい部分を探索することができるようになった。
このため，エージェントの行動 (探索) は，エージェントが学習しようとしている方針 (ゲームで高得点を得ること) から大きく逸脱することになり，ここでも方針逸脱学習が重要な役割を果たす。
NGU は，Atari 57 ベンチマークの導入以来，どのエージェントも得点できなかった Pitfall や，その他の難しい Atari ゲームにおいて，領域知識なしで正の報酬を得た最初のエージェントであった。
残念ながら，NGU は歴史的に「簡単な」ゲームでは性能を犠牲にしたため，平均して R2D2 より性能が劣っている。
<!-- We designed Never Give Up (NGU) to augment R2D2 with another form of memory: episodic memory.
This enables NGU to detect when new parts of a game are encountered, so the agent can explore these newer parts of the game in case they yield rewards.
This makes the agent’s behaviour (exploration) deviate significantly from the policy the agent is trying to learn (obtaining a high score in the game); thus, off-policy learning again plays a critical role here.
NGU was the first agent to obtain positive rewards, without domain knowledge, on Pitfall, a game on which no agent had scored any points since the introduction of the Atari57 benchmark, and other challenging Atari games.
Unfortunately, NGU sacrifices performance on what have historically been the “easier” games and so, on average, underperforms relative to R2D2.
我々は R2D2 を別の記憶形態であるエピソード記憶で補強するために Never Give Up (NGU) を設計した。
これにより NGU はゲームの新しい部分に遭遇したときにそれを検出することができ，それが報酬をもたらす場合には，エージェントはゲームのこれらの新しい部分を探索することができるようになる。
これにより，エージェントの行動（探索）は，エージェントが学習しようとしている方針（ゲームで高得点を得ること）から大きく逸脱してしまいます。
NGU は Atari57 ベンチマークが導入されて以来，誰もポイントを獲得していないゲームである Pitfall や，他の難解な Atari ゲーム で， ドメイン知識なしで，ポジティブな報酬を獲得した最初のエージェントであった。
残念なことに NGU は歴史的に「より簡単な」ゲームでの成績を犠牲にしている。
このため，平均的には R2D2 と比較して低い成績となった。-->


## 直接的な探索を促すための内発的動機づけの方法<!-- # Intrinsic motivation methods to encourage directed exploration-->

最も成功した戦略を発見するためには，エージェントは環境を探索しなければならないが，探索戦略の中には他の戦略よりも効率的なものもある。
DQN では，$\epsilon$ 貪欲(グリーディ) として知られる無方向探索戦略を用いて探索問題を解決しようとした。
イプシロングリーディとは一定の確率（イプシロン）でランダムな行動をとり，そうでなければ現在の最良の行動を選ぶことである。
報酬がない場合，大きな状態行動空間を探索するのに膨大な時間を必要とする。
この限界を克服するために，多くの有向探索戦略が提案されてきた。
これらの中で，ある分野では，新規性を求める行動に対してより密な「内部」報酬を提供することで，エージェントが可能な限り多くの状態を探索し，訪問することを促す内発的動機報酬の開発に焦点を当ててきた。
その中で，我々は 2 つのタイプの報酬を区別した。
第一のタイプの報酬は，長期的な新規性報酬は，訓練期間中，多くのエピソードに渡って多くの状態を訪問することを促す。
第二のタイプの報酬は，短期的な新規性報酬は，短期間（例えば，ゲームの 1エピソード内）に多くの状態を訪問することを促すものである。
<!--In order to discover the most successful strategies, agents must explore their environment–but some exploration strategies are more efficient than others.
With DQN, researchers attempted to address the exploration problem by using an undirected exploration strategy known as epsilon-greedy: with a fixed probability (epsilon), take a random action, otherwise pick the current best action.
However, this family of techniques do not scale well to hard exploration problems: in the absence of rewards, they require a prohibitive amount of time to explore large state-action spaces, as they rely on undirected random action choices to discover unseen states.
In order to overcome this limitation, many directed exploration strategies have been proposed.
Among these, one strand has focused on developing intrinsic motivation rewards that encourage an agent to explore and visit as many states as possible by providing more dense “internal” rewards for novelty-seeking behaviours.
Within that strand, we distinguish two types of rewards: firstly, long-term novelty rewards encourage visiting many states throughout training, across many episodes. Secondly, short-term novelty rewards encourage visiting many states over a short span of time (e.g., within a single episode of a game). -->


### 長時間軸での新規性<!-- ## Seeking novelty over long time scales-->

長期的な新規性報酬は，以前に見たことのない状態がエージェントの生涯で遭遇したときのキッカケであり，訓練中にこれまでに見た状態密度の関数となる。
ある状況に接する頻度が高い場合 (その状態がよく知られていることを示す)，長期的な新規性報酬は低い。その逆もまた然り。
すべての状態が見慣れた状態であれば，エージェントは無方向の探索戦略に頼ることになる。
しかし，高次元空間の密度モデルの学習は，**次元の呪い** のために問題が多い。
実際には，エージェントが深層学習モデルを用いて密度モデルを学習する場合，壊滅的忘却（新しい経験に遭遇すると以前に見た情報を忘れる）や，すべての入力に対して正確な出力を生成することができないことに悩まされる。
例えば，モンテズマの復讐では，指向性のない探索戦略とは異なり，長期的な新規性報酬により，エージェントは人間のベースラインを超えることができる。
しかし，モンテズマの復讐で最高のパフォーマンスを発揮する方法であっても，密度モデルを適切な速度で注意深く訓練する必要がある。
密度モデルが最初の部屋の状態が馴染みのあるものであることを示すとき，エージェントは一貫して馴染みのない領域に到達することができるはずである。
<!-- Long-term novelty rewards signal when a previously unseen state is encountered in the agent’s lifetime, and is a function of the density of states seen so far in training: that is, it’s adjusted by how often the agent has seen a state similar to the current one relative to states seen overall.
When the density is high (indicating that the state is familiar), the long term novelty reward is low, and vice versa.
When all the states are familiar, the agent resorts to an undirected exploration strategy. However, learning density models of high dimensional spaces is fraught with problems due to the curse of dimensionality.
In practice, when agents use deep learning models to learn a density model, they suffer from catastrophic forgetting (forgetting information seen previously as they encounter new experiences), as well as an inability to produce precise outputs for all inputs.
For example, in Montezuma’s Revenge, unlike undirected exploration strategies, long-term novelty rewards allow the agent to surpass the human baseline.
However, even the best performing methods on Montezuma’s Revenge need to carefully train a density model at the right speed: when the density model indicates that the states in the first room are familiar, the agent should be able to consistently get to unfamiliar territory. -->

<center>

[モンテズマの復讐](https://www.youtube.com/watch?v=M9Yn1kYZb6E&list=PLqYmG7hTraZB5YFgejiwDoKBkg50SlY6z):<br/>
<iframe width="600" height="352" src="https://www.youtube.com/embed/M9Yn1kYZb6E?list=PLqYmG7hTraZB5YFgejiwDoKBkg50SlY6z" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>


### 短期間での新規性<!-- ## Seeking novelty over short time scales-->

短期的な新規性報酬は，エージェントが最近の過去に遭遇しなかった状態を探索することを促すために使用することができる。
近年，エピソード記憶のいくつかの特性を模倣したニューラルネットワークが強化学習エージェントの学習を高速化するために使用されている。
エピソード記憶もまた，新しい経験を認識するために重要であると考えられている。
このため，我々はこれらのモデルを適応させ，Never Give Up に短期的な新規性の概念を与えるようにした。
エピソード記憶モデルは（モデルのパラメータを学習したり，適応させたりする必要がなく）その場で適応させることができるノンパラメトリックな密度モデルを素早く学習することができる。
このため，短期的な新規性報酬を計算するための効率的で信頼性の高い候補である。
この場合，報酬の大きさは，エピソード記憶に記録された現在の状態と以前の状態との間の距離を測定することによって決定される。
<!-- Short-term novelty rewards can be used to encourage an agent to explore states that have not been encountered in its recent past.
Recently, neural networks that mimic some properties of episodic memory have been used to speed up learning in reinforcement learning agents.
Because episodic memories are also thought to be important for recognising novel experiences, we adapted these models to give Never Give Up a notion of short-term novelty.
Episodic memory models are efficient and reliable candidates for computing short-term novelty rewards, as they can quickly learn a non-parametric density model that can be adapted on the fly (without needing to learn or adapt parameters of the model).
In this case, the magnitude of the reward is determined by measuring the distance between the present state and previous states recorded in episodic memory.-->

すべての距離の概念が意味のある探索を促進するわけではない。
例えば，多くの歩行者や車両がいる混雑した街をナビゲートする課題を考えると，
あるエージェントが，視覚的なあらゆる微小な変化を考慮に入れた距離の概念を使用するようにプログラムされているとすれば，そのエージェントは，受動的に環境を観察するだけで，静止していることさえも含めて多くの異なる状態を訪れることになる。
このようなシナリオを避けるためには，エージェントが探索にとって重要だと考えられる特徴（たとえば，制御性）を学習し，その特徴についてだけ考慮した距離を計算する必要がある。
このようなモデルはこれまでも探索に用いられてきた。
だが，このエピソード記憶との組み合わせは ネバーギブアップ探索手法の主な進歩の一つであり，これにより Pitfall の性能が人間を凌駕した。
<!-- However, not all notions of distance encourage meaningful forms of exploration.
For example, consider the task of navigating a busy city with many pedestrians and vehicles.
If an agent is programmed to use a notion of distance wherein every tiny visual variation is taken into account, that agent would visit a large number of different states simply by passively observing the environment, even standing still – a fruitless form of exploration.
To avoid this scenario, the agent should instead learn features that are seen as important for exploration, such as controllability, and compute a distance with respect to those features only. Such models have previously been used for exploration, and combining them with episodic memory is one of the main advancements of the Never Give Up exploration method, which resulted in above-human performance in Pitfall!. -->

<center>

[NGU による Pitfall!](https://youtu.be/imAeLt1BPu4?list=PLqYmG7hTraZBgXcetCL9zzd6Cck8S4l4k)<br/>
<iframe width="548" height="323" src="https://www.youtube.com/embed/imAeLt1BPu4?list=PLqYmG7hTraZBgXcetCL9zzd6Cck8S4l4k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

Never Give Up (NGU) は 制御可能な状態に基づく短期的新規性報酬と ランダムネットワーク蒸留を用いた長期的新規性報酬と混合して使用した。
この混合は 長期的新規性が制限されている場合，両方の報酬を掛け合わせることで実現された。
このようにして，短期的な目新しさの報酬の効果は維持される。
だが エージェントが生涯にわたってゲームに慣れてくると，その効果は減衰される可能性がある。
NGU のもう一つの革新的アイデアとしては，NGU は方策族を学習することである。
方策族には，純粋に 利用的 (exploitative) なものから, 高度に探索的 (exploratary) なものまで含まれる。
R2D2 の上に構築することで，アクターは総新規性報酬の重要度の重み付けに基づいて，異なるポリシーでの経験を生成する。
この経験は，方策族内の各重み付けに関して一様に生成される。
<!-- Never Give Up (NGU) used this short-term novelty reward based on controllable states, mixed with a long term novelty reward, using Random Network Distillation.
The mix was achieved by multiplying both rewards, where the long term novelty is bounded.
This way the short-term novelty reward’s effect is preserved, but can be down-modulated as the agent becomes more familiar with the game over its lifetime.
The other core idea of NGU is that it learns a family of policies that range from purely exploitative to highly exploratory. This is achieved by leveraging a distributed setup: by building on top of R2D2, actors produce experience with different policies based on different importance weighting on the total novelty reward. This experience is produced uniformly with respect to each weighting in the family. -->


<center>
<img src="/assets/2017Pathak_fig2.svg" width="77%"><br/>
<div style="text-align: justify;width:88%;background-color:cornsilk">
図 2. 状態 $s_{t}$ のエージェントは，現在の方針 $\pi$ からサンプリングされた行動 $a_{t}$ を実行することで環境と相互作用し，最終的に状態 $s_{t+1}$ になる。
方針 $\pi$ は，環境 $E$ が提供する外発的報酬 ($r^{e}_{t}$) と，我々が提案する内発的好奇心モジュール (ICM) が生成する好奇心に基づく内発的報酬信号 ($r^{i}_{t}$) の和を最適化するように学習されます。
ICM は，状態 $s_{t}$, $s_{t+1}$ を，$a_{t}$ を予測するために学習された特徴量 $\phi(s_{t})$, $\phi(s_{t+1})$ に符号化します (すなわち，逆動力学モデル)。
順方向モデルは，$\phi(s_{t})$ と $a_{t}$ を入力とし $s_{t+1}$ の特徴表現 $\widehat{\phi}(s_{t+1})$ を予測します。
特徴空間での予測誤差は，好奇心に基づく内在的報酬信号として用いられる。
エージェントの行動に影響を与えない，あるいは影響を受けない環境特徴を符号化するインセンティブが $\phi(s_{t})$ にはないため，我々のエージェントの学習された探索戦略は，環境の制御不可能な側面に対してロバストである。

<!-- Figure 2.
The agent in state st interacts with the environment by executing an action $a_{t}$ sampled from its current policy $\pi$ and ends up in the state $s_{t+1}$.
The policy $\pi$ is trained to optimize the sum of the extrinsic reward ($r^{e}_{t}$) provided by the environment $E$ and the curiosity based intrinsic reward signal ($r^{i}_{t}$) generated by our proposed Intrinsic Curiosity Module (ICM).
ICM encodes the states $s_{t}$, $s_{t+1}$ into the features $\phi(s_{t})$, $\phi(s_{t+1})$ that are trained to predict $a_{t}$ (i.e. inverse dynamics model).
The forward model takes as inputs $\phi(s_{t})$ and $a_{t}$ and predicts the feature representation $\widehat{\phi}(s_{t+1})$ of $s_{t+1}$.
The prediction error in the feature space is used as the curiosity based intrinsic reward signal.
As there is no incentive for $\phi(s_{t})$ to encode any environmental features that can not influence or are not influenced by the agent’s actions, the learned exploration strategy of our agent is robust to uncontrollable aspects of the environment.-->
</div>
</center>


<center>
<img src="/assets/2019Jaegle_fig1.jpg" width="77%"><br/>
<div style="text-align: justify;width:88%;background-color:cornsilk">
好奇心は，稀にしか	報酬の得られない環境下での探索を動機づける。
<!-- Novelty can drive exploration in environments with sparse external rewards. -->
上イラストは探索の模式図である。サルは，果物 (報酬) を大きく生茂った枝振りの良い木から探そうとしている。
<!-- An illustration of the benefits of exploration:  -->
報酬にありつくまでに，数多くの枝，すなわち，多くの「状態」を探索せねばならない。
<!-- a monkey is trying to find a piece of fruit (a reward) in a large, densely foliated tree with many branches.  -->
一つ報酬を見つけ出した場合には，他の報酬はすべて捨て去ることをも含意している。
<!-- Typically, the monkey must make many choices and explore many ‘states’ before it receives a single reward.
If the monkey finds novel states rewarding, then it will be encouraged to explore the tree, and it can discover rewarding states that it would otherwise miss. -->
視点の普遍性:
<!-- View invariance: -->
同一状態の異なる見え(リンゴ)は，異なる画像に対応する。
<!-- different views of the same state (e.g. the apple) can correspond to different images.  -->
強化学習が成就するためには，入力画像を対応する状態への関連付けなければならない。
<!-- To effectively drive RL, a system must map images onto their corresponding states. -->
状態普遍性:
<!-- State invariance:  -->
状態が異なれば，新奇性も異なる。果物は通常，高い報酬となり，木々の葉は通常小さく，重なり合って細かな影をなす。
<!-- different states can share features that are indicative of their novelty, for example, reflecting the fact that fruits are usually large and are rarely green while leaves are often small and can take on many shades of green.
A system that can exploit the features shared by different states can drive the monkey to explore states with novel features (e.g. objects with a new size or shade).
-->
source: Jaegle2019+ Fig. 1
</div>
</center>

### メタコントローラ：探索と利用のバランス

Agent57 は 次のような観察に基づいて構築されている。
もしエージェントが，いつ利用 (exploit) するのが良いのか，いつ 探索 (explore) するのが良いのかを学習できるとしたらどうであろうか？
我々は，探索と利用のトレードオフを適応させるメタコントローラの概念と，より長い時間的な信用割当てを必要とするゲームのために調整可能な時間地平線を導入した。
この変更により Agent57 は簡単なゲームでも難しいゲームでも人間レベル以上の成績を得ることができるようになった。
<!-- # Meta-controller: learning to balance exploration with exploitation
Agent57 is built on the following observation: what if an agent can learn when it’s better to exploit, and when it’s better to explore? We introduced the notion of a meta-controller that adapts the exploration-exploitation trade-off, as well as a time horizon that can be adjusted for games requiring longer temporal credit assignment. With this change, Agent57 is able to get the best of both worlds: above human-level performance on both easy games and hard games.-->

具体的には，内在的動機付け方法には 2 つの欠点がある。<!-- Specifically, intrinsic motivation methods have two shortcomings:-->
<!-- * Exploration: Many games are amenable to policies that are purely exploitative, particularly after a game has been fully explored. This implies that much of the experience produced by exploratory policies in Never Give Up will eventually become wasteful after the agent explores all relevant states.
* Time horizon: Some tasks will require long time horizons (e.g. Skiing, Solaris), where valuing rewards that will be earned in the far future might be important for eventually learning a good exploitative policy, or even to learn a good policy at all. At the same time, other tasks may be slow and unstable to learn if future rewards are overly weighted. This trade-off is commonly controlled by the discount factor in reinforcement learning, where a higher discount factor enables learning from longer time horizons. -->

* **探索** 多くのゲームは，特にゲームが完全に探索 explore された後に 純粋に 利用可能 exploit な方針に従順である。
これは NGU での 探索的 (explore) 方策によって生成された経験の多くは，エージェントが関連するすべての状態を探索した後に，最終的に無駄になることを意味している。
* **時間の地平線** Skiing や Solaris のように 遠い将来に得られる報酬を評価することは，最終的に良い 利用可能 (搾取的 exploitive) な政策を学ぶために，あるいは良い政策を全く学ぶために重要かもしれない。
同時に，将来の報酬が過度に重み付けされている場合，他の課題は学習に時間がかかり，不安定になる可能性がある。
このトレードオフは，一般的に強化学習では割引率によって制御され，割引率が高いほど長い時間軸からの学習が可能になる。

このことから，可変長の時間水平線と新規性の重要性を考慮して，異なる方策で生成される経験の量を制御するオンライン適応メカニズムを使用した。
研究者たちは，異なるハイパーパラメータを持つエージェントの集団を訓練する，勾配降下法によってハイパーパラメータの値を直接学習する，ハイパーパラメータの値を学習するために中央集権型バンディットを使用するなど，複数の方法でこれに取り組むことを試みてきた。

我々はバンディットアルゴリズムを使用して，経験を生成するためにエージェントが使用すべき方策を選択した。
具体的には，各アクターに対してスライドウィンドウ型 UCB バンディットを訓練し，そのポリシーが持つべき探索への選好度と時間軸を選択した。

<!-- This motivated the use of an online adaptation mechanism that controls the amount of experience produced with different policies, with a variable-length time horizon and importance attributed to novelty. Researchers have tried tackling this with multiple methods, including training a population of agents with different hyperparameter values, directly learning the values of the hyperparameters by gradient descent, or using a centralized bandit to learn the value of hyperparameters.

We used a bandit algorithm to select which policy our agent should use to generate experience. Specifically, we trained a sliding-window UCB bandit for each actor to select the degree of preference for exploration and time horizon its policy should have. -->

<center>

[NGU によるスキー](https://youtu.be/0_67wNXyOcI?list=PLqYmG7hTraZDp9fZRWVMeGwUupEl7uN8S)<br/>
<iframe width="560" height="323" src="https://www.youtube.com/embed/0_67wNXyOcI?list=PLqYmG7hTraZDp9fZRWVMeGwUupEl7uN8S" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>


### Agent57: すべてをまとめる

Agetn57 を達成するために，我々は以前に開発した探索エージェント Never Give Up をメタコントローラと組み合わせた。
このエージェントは 方策族を探索して学習するために，長期的，および短期的な，内在的動機を混合したものを計算する。
メタコントローラは，エージェントの各アクターが，新しい状態を探索するか，すでに知られていることを利用するかの違いと同様に，短期と長期とのパフォーマンスの間で異なるトレードオフを選択することを可能にする (図4)。
強化学習はフィードバックループを形成する。
選択された行為が訓練データを決定し，メタコントローラはまたエージェントがどのようなデータから学習するかを決定する。

<!-- # Agent57: putting it all together
To achieve Agent57, we combined our previous exploration agent, Never Give Up, with a meta-controller. This agent computes a mixture of long and short term intrinsic motivation to explore and learn a family of policies, where the choice of policy is selected by the meta-controller. The meta-controller allows each actor of the agent to choose a different trade-off between near vs. long term performance, as well as exploring new states vs. exploiting what’s already known (Figure 4). Reinforcement learning is a feedback loop: the actions chosen determine the training data. Therefore, the meta-controller also determines what data the agent learns from. -->

<center>
<img src="/assets/2020deepmind_agent57_fig4.svg" style="width:66%"><br/>
<!-- <img src="https://kstatic.googleusercontent.com/files/a9754efe518fdef7e39f50baeffd0f8348f21d0fd3c919f6ca749799321a9b514134f3a2a6f19d33b9fcf254ccf05847f91da511567e44ffc375a6ccb75b069c"> -->
</center>

<!--
## 結論と今後の展開
Agent57 では Atari57 ベンチマークのすべての課題において人間以上の性能を持つ，より一般的な知的エージェントを構築することに成功した。
このエージェントは，以前のエージェント Never Give Up を ベースに構築されており， 適応的なメタコントローラを実体化している。
広範囲の課題は当然，これらのトレードオフの両方の異なる選択を必要とする。そのためメタコントローラはそのような選択を動的に適応させる方法を提供している。

Agent57 は計算量の増加に伴ってスケーリングすることができた。
これにより Agent57 は強力な一般性能を達成することが可能となった。
多くの計算量と時間を必要とするが，データ効率は確実に改善することができる。
さらに，この Agent57は Atari の 57 ゲームセット全てで 5 パーセンタイル のより良い性能を示した。
これは，データ効率の面だけでなく，一般的な性能の面でも，決して Atari の研究の終わりを意味するものではない。

* 第一に，パーセンタイル間の性能を分析することで，一般的なアルゴリズムがどのようなものであるかについて新たな洞察を得ることができる。
* 第一に，パーセンタイル間の性能を分析することで，一般的な アルゴリズムがどのようなものであるかを知ることができる。
* 第二に，現在のアルゴリズムはすべて，いくつかのゲームで最適なパフォーマンスを達成するには程遠いということです。

そのためには Agent57 が探索，計画，および信用割り当てのために使用する表現を強化することが，使用するための重要な改善点になるかもしれない。 -->





## APE-X
@2018Horgan_APE-X

## TRPO (Trust Region Policy Optimization)
@2015Schulman_trpo

## IMPALA
<https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/>


#### TPRO と PPO
- source: <https://blog.syundo.org/post/20171204-reinforcement-learning-natural-policy-gradient-trpo-ppo/>
- さらに，上のトップ: <https://blog.syundo.org/post/20180115-reinforcement-learning/>

##### TRPO

最適化計算における更新ステップの計算に KL ダイバージェンスによる制約を加えたものが TRPO (Trust Region Policy Optimization) である。
この方法は，を KL ダイバージェンスで拘束しているため，近似的には自然勾配法と同様の手法となる。
TRPO は方策勾配法に限らず，モデルなし学習においても利用することができるが，以下では方策勾配法と組み合わせることを前提に述べる。

さて，$\theta$ でパラメタライズされた方策  $\pi_\theta(a\vert s)$ がある場合，方策勾配が

$$
\hat{g} = \mathbb{E}_\pi\left[ \nabla_\theta \log\pi_\theta(a\vert s)A^{\pi}(s,a)\right]
$$

であった。これは，以下の値 $L_\theta$ の微分値である。

$$
L_\theta(\theta)=\mathbb{E}_\pi\left[\log \pi_\theta(a\vert s)A^{\pi}(s,a)\right]
$$

このとき，更新のステップを制限するために，以下のようにKLダイバージェンスで制約を課して最大化を行う。

$$
\text{maximize}_{x} L_{\theta_{\text{old}}}(\theta)
$$

$$
\text{subject to } D_{KL}(\theta_{\text{old}},\theta)\le\delta
$$

ここで， $\theta_{\text{old}}$ は $\pi_\theta$ におけるパラメタ $\theta$ の前の値である。
また，$D_{KL}$ は確率分布 $\pi_\theta$ と $\pi_{\theta_{\text{old}}}$ の間の KL ダイバージェンスであり， $D_{KL}\max(\theta_{\text{old}},\theta)$ は任意のパラメタの組み合わせに対して，KLダイバージェンスを計算したときの最大値を表す。

実用的には組み合わせが膨大になり，最大値を求めるのは難しいため，制約はヒューリスティックに以下のように平均値で代用する。
$$
\text{maximize}_x L_{\theta_{\text{old}}}(\theta)$$
$$

$$
\text{subject to} D_{KL}^{\max}(\theta_{\text{old}},\theta) \le \delta
$$

ここで， $\bar{D}_{KL}(\theta_{\text{old}},\theta)=\mathbb{E}_{s\sim p}\left[D_{KL}(\pi_\theta(\cdot\vert s),\pi_{\theta_2}(\cdot\vert s)\right]$ である。

ただし，制約において問題を解くのは簡単ではないので，以下のようにソフト制約を使う形に書き下す。

$$
\text{maximize}_x \mathbb{E}_\pi\left[L_{\theta_{\text{old}}}(\theta)-\beta\bar{D}_{KL}(\theta_{\text{old}},\theta)\right]
$$

以上が，TRPO の概要である。


### キーコンセプト ### Key Concepts

* エージェントは **環境** の中で行動する。
環境がある行動に対してどのように反応するかは， 我々が知っているかどうかわからない **モデル** によって定義されます。
エージェントは， 環境の多くの **状態**（$s\in\mathcal{S}$）のうちの 1 つに留まることができ， 多くの **行動**（$a\in\mathcal{A}$）のうちの 1 つを選択して， ある状態から別の状態に切り替えることができます。
エージェントがどの状態に到達するかは， 状態間の遷移確率 ($P$) によって決定される。
行動を起こすと，環境はフィードバックとして **報酬** ($r\in\mathcal{R}$) を与えます。

The agent is acting in an **environment**. How the environment reacts to certain actions is defined by a **model** which we may or may not know.
The agent can stay in one of many **states** ($s \in \mathcal{S}$) of the environment, and choose to take one of many **actions** ($a \in \mathcal{A}$) to switch from one state to another.
Which state the agent will arrive in is decided by transition probabilities between states ($P$).
Once an action is taken, the environment delivers a **reward** ($r \in \mathcal{R}$) as feedback.

モデルは報酬関数と遷移確率を定義しています。
モデルがどのように動作するかを知っている場合と知らない場合があり，これにより 2 つの状況が区別されます。

- **モデルベース**：完全な情報で計画を立てて学習を行う。
環境が完全にわかっている場合 [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP)によって最適解を求めることができます。
- **モデルフリー**：不完全な情報での学習；アルゴリズムの一部として明示的にモデルを学習しようとする。
以下の内容のほとんどは，モデルがわからない場合のシナリオに対応しています。

The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP).
Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL.
This is not the focus of this post though.
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm.
Most of the following content serves the scenarios when the model is unknown.

エージェントの **ポリシー**  $\pi(s)$ は，**総報酬** を最大化することを目的として，ある状態で取るべき最適な行動のガイドラインを提供する。
各状態には，その状態で対応するポリシーを実行することで得られる将来の報酬の期待値を予測する **価値** 関数$V(s)$ が関連付けられています。
言い換えれば，価値関数は，ある状態がどれだけ良いかを定量化します。
強化学習で学習しようとするのは，方策関数と価値関数の両方です。
<!-- The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **the goal to maximize the total rewards**.
Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy.
In other words, the value function quantifies how good a state is.
Both policy and value functions are what we try to learn in reinforcement learning.



<!--
##### PPO

PPO (Proximal Policy Optimization) は方策の目標値をクリッピングすることで，おおまかに方策の更新を制約する方法である。
TRPO では KL ダイバージェンスを制約として利用していたが，PPO では，目的関数を以下の $L^{\text{clip}}$ として，勾配を求める。

$$
L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min(r_t(\theta)\hat{A}_t,\text{clip}(r(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)\right]
$$

ここで，$r_t(\theta)$ は確率の比率であり，

$$
r_t(\theta) = \frac{\pi_\theta(s_t,a_t)}{\pi_{\theta_{\text{old}}}(s_t,a_t)}
$$

である。また，$\text{clip}(r(\theta), 1−\epsilon, 1+\epsilon)$ は $r(\theta)$ が $1−\epsilon} あるいは $1+\epsilon$ を超過しないように制限する関数である。
$\text{clip}(r(\theta), 1−\epsilon , 1+\epsilon)^At$ の グラフと，$L^{\text{CLIP}}$ を以下に示す(John Schulmanらより引用)。
-->


<!-- 
* [実習 オーバーフィッティング，アンダーフィッテング <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_Visit_polynomilal_fittings_demo.ipynb){:target="_blank"}
* [符号化器・復号化器モデル ちはやふる <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}
* [PyTorch による Transfomer 実装 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602Transformer_from_scratch.ipynb){:target="_blank"}
* [chatGPT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0608rinna_chatGPT_demo.ipynb){:target="_blank"} -->

<!-- * WEAVER++, Dell モデルの再現シミュレーション
  - [他言語プライミング課題での事象関連電位 （ERP) のシミュレーション Roelofs, Cortex (2016) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Roelofs_ERP_bilingual_lemret.ipynb){:target="_blank"}
  - [概念バイアス `Conceptual Bias` (Reolofs, 2016) 絵画命名，単語音読，ブロック化，マルチモーダル統合 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2021Roelofs_Conceptual_bias.ipynb){:target="blank"}
  - [2 ステップ相互活性化モデルデモ (Foygell and Dell, 2000) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020ccap_Foygel_Dell2000_2step_interactive_activaition_model_demo.ipynb){:target="_blank"}
  - [WEVER++ デモ 2020-1205 更新 Reolofs(2019) Anomia cueing <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/notebooks/2020ccap_Roelofs2019_Anomia_cueing_demo.ipynb){:target="_blank"} -->

<!-- * [1990 年代の Stroop 効果のシミュレーション<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1110Stroop_1990Cohen_model.ipynb){:target="_blank"}
* [転移学習による Stroop 効果のデモ<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1123Stroop_model.ipynb)

* [画像認識における注意 CAM <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2021_0618CAM_demo.ipynb) -->

<!-- * [リカレントニューラルネットワークによる文処理デモ 青空文庫より，夏目漱石 こころ](https://komazawa-deep-learning.github.io/character_demo.html)
* [CartoonGAN 実習<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0628CartoonGAN_demo.ipynb) -->


<!-- * PyTorch 関連 -->

<!-- * [Pytorch によるニューラルネットワークの構築 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1115PyTorch_buildmodel_tutorial_ja.ipynb){:target="_blank"}
  * [Dataset とカスタマイズと，モデルのチェックポイント，微調整 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0824pytorch_simple_fine_tune_tutorial.ipynb){:target="_blank"}
  * [PyTorch Dataset, DataLoader, Sampler, Transforms の使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0824pytorch_dataset_data_loader_sampler.ipynb){:target="_blank"} -->

<!-- * [オノマトペ関連 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2023_1115onomatope_generator.ipynb){:target="_blank"} -->

<!-- * [Stable-baselines3 を用いた PPO デモ Atari Lunalander 月面着陸 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0619stable_baselines3_demo_LunaLander_V2.ipynb)
* [Stable diffusion デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0707stable_diffusion.ipynb) -->


## 強化学習

未知の環境にある動作主(エージェント)がいて，このエージェントは環境と相互作用することで報酬を得ることができるとする。動作主(エージェント)は，累積報酬を最大化するように行動する必要がある。現実的には，ゲームで高得点を出すロボットや，物理的なアイテムを使って物理的な課題をこなすロボットなどが考えられる。だが，これらに限定されるものではない。
<!-- Say, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment.
The agent ought to take actions so as to maximize cumulative rewards.
In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these. -->

* [REINFORCE.js](https://komazawa-deep-learning.github.io/reinforcejs/){:target="_blank"}

<!-- ## 実習ファイル

- [ランダム探索  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)
- [方策勾配法  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)
- [SARSA  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)
- [Q学習  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb) -->

<!-- 以下のデモは，[OpenAI](https://openai.com/) 提供の強化学習環境 [gym](https://gym.openai.com/) を用いています。 -->

### 強化学習の基本概念<!-- ### Key Concepts -->

<!-- RL の重要な概念を正式に定義しましょう。Now Let's formally define a set of key concepts in RL. -->

* 動作主 (エージェントは) **環境** の中で行動する。
* 環境がある行動に対してどのように反応するかは，我々が知っているかどうかわからない **モデル** によって定義される。
* 動作主 (エージェント) は，環境の多くの **状態**（$s\in\mathcal{S}$）のうちの 1 つに留まることができ，多くの **行動**（$a\in\mathcal{A}$）のうちの 1 つを選択して， ある状態から別の状態に切り替えることができる。
* 動作主 (エージェント) がどの状態に到達するかは，状態間の遷移確率 ($P$) によって決定される。
* 行動を起こすと，環境はフィードバックとして **報酬** ($r\in\mathcal{R}$) を与える。
<!-- The agent is acting in an **environment**. How the environment reacts to certain actions is defined by a **model** which we may or may not know.
The agent can stay in one of many **states** ($s \in \mathcal{S}$) of the environment, and choose to take one of many **actions** ($a \in \mathcal{A}$) to switch from one state to another.
Which state the agent will arrive in is decided by transition probabilities between states ($P$).
Once an action is taken, the environment delivers a **reward** ($r \in \mathcal{R}$) as feedback.-->

* モデルは報酬関数と遷移確率を定義する。
* モデルがどのように動作するかを知っている場合と知らない場合があり，これにより 2 つの状況が区別される。
<!-- The model defines the reward function and transition probabilities.
We may or may not know how the model works and this differentiate two circumstances: -->

  * **モデルベース**
<!-- * **モデルを知る**：完全な情報で計画を立てる、モデルベースの RL を行う。-->
        環境が完全にわかっている場合 [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP) によって最適解を求めることができます。
<!-- アルゴリズム 101 の授業で習った "longest increasing partialence" や "traveling salesmen problem" をまだ覚えていますか？笑
これはこの記事の焦点ではありませんが。-->

  * **モデルフリー**
<!-- * **モデルを知らない**：不完全な情報での学習；モデルフリー RL を行うか，-->
アルゴリズムの一部として明示的にモデルを学習しようとする。
<!-- 以下の内容のほとんどは、モデルがわからない場合のシナリオに対応しています。 -->

<!-- The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP).
Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL.
This is not the focus of this post though.
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm.
Most of the following content serves the scenarios when the model is unknown.-->

* 動作主(エージェント)の **方針(ポリシー)**  $\pi(s)$ は，**総報酬** $G$ を最大化することを目的として，ある状態で取るべき最適な行動のガイドラインを提供する。
* 各状態には，その状態で対応する方針を実行することで得られる将来の報酬の期待値を予測する **価値** 関数 $V(s)$ が関連付けられる。
* 言い換えれば，価値関数は，ある状態がどれだけ良いかを定量化する。
* 強化学習で学習しようとするのは，方針関数 と 価値関数の両方。
<!-- The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **the goal to maximize the total rewards**.
Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy.
In other words, the value function quantifies how good a state is.
Both policy and value functions are what we try to learn in reinforcement learning.-->

<center>
<img src="/assets/RL_algorithm_categorization.png" width="94%"><br/>
<div style="text-align: justify;width:88%;background-color: cornsilk;">
価値，方針(ポリシー)，環境 のいずれをモデル化したいかに基づく強化学習アプローチのまとめ。
(画像出典：[David Silver の強化学習講座](https://youtu.be/2pWv7GOvuf0) より転載)
<!--Fig. 2. Summary of approaches in RL based on whether we want to model the value, policy, or the environment.
(Image source: reproduced from David Silver's RL course [lecture 1](https://youtu.be/2pWv7GOvuf0).)-->
</div>
</center>

* 動作主 (エージェント) と環境の相互作用には $t=1,2,\dots,T$ 時間内の一連の行動と観測された報酬が含まれる。
* この過程で，エージェントは環境に関する知識を蓄積し，最適な政策を学習し，最適な政策を効率的に学習するために，次にどのような行動を取るべきかを決定する。
* 時間ステップ $t$ における状態，行動，報酬をそれぞれ $S_t$, $A_t$, $R_t$ とする。
<!-- このように， インタラクションシーケンスは 1 つの **エピソード** (「試行」または「軌跡」とも呼ばれる) で完全に記述され， 系列は末端の状態 $S_T$ で終了します。 -->
<!-- The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, $t=1, 2, \dots, T$.
During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy.
Let's label the state, action, and reward at time step t as $S_t$, $A_t$, and $R_t$, respectively.
Thus the interaction sequence is fully described by one **episode** (also known as "trial" or "trajectory") and the sequence ends at the terminal state $S_T$:-->

$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$

<!-- RL アルゴリズムの様々なカテゴリーを調べる際によく遭遇する用語。 -->
<!-- Terms you will encounter a lot when diving into different categories of RL algorithms: -->

- **モデルベース**: モデルが既知であるか、アルゴリズムがそれを明示的に学習する。
- **モデルフリー**: 学習時にモデルに依存しない。
- **オンポリシー**<!-- **On-policy** -->: アルゴリズムの学習にターゲットポリシーからの決定論的な結果やサンプルを使用する。
- **オフポリシー**<!-- **Off-policy** -->: ターゲット・方策(ポリシー)ではなく，異なる<!-- ビヘイビア・ -->方策(ポリシー)で生成された上他繊維やエピソード分布で学習する。

<!--- **Model-based**: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.
- **Model-free**: No dependency on the model during learning.
- **On-policy**: Use the deterministic outcomes or samples from the target policy to train the algorithm.
- **Off-policy**: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.-->

#### 遷移と報酬 transition and reward<!-- #### Model: Transition and Reward -->

* モデルとは，環境を記述する実体。
* モデルを用いることで，環境がどのように動作主 (エージェント) と相互作用し，フィードバックを与えるかを学習または推論することができる。
* モデルには，遷移確率関数 $P$ と報酬関数 $R$ の 2 つの主要部分がある。

<!-- The model is a descriptor of the environment.
With the model, we can learn or infer how the environment would interact with and provide feedback to the agent.
The model has two major parts, transition probability function $$P$$ and reward function $R$. -->

* 状態 $s$ にいるとき，次の状態 $s’$ に到達して報酬 $r$ を得るために行動 $a$ をとることを決めたとする。
これは 1 つの **遷移** ステップと呼ばれ， タプル $(s,a,s',r)$ で表される。

<!-- Let's say when we are in state s, we decide to take action a to arrive in the next state s' and obtain reward r.
This is known as one **transition** step, represented by a tuple (s, a, s', r).-->

* 遷移関数 $P$ は，行動 $a$ を起こした後に報酬 $r$ を得て，状態 $s$ から $s’$ に遷移する確率を記録したものである。
ここでは $\mathbb{P}$ を「確率」の記号として用いる。
<!-- The transition function P records the probability of transitioning from state s to s' after taking action a while obtaining reward r.
We use $$\mathbb{P}$$ as a symbol of "probability".-->

$$
P(s', r \vert s, a)  = \mathbb{P} \left[S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a\right]
$$

したがって，状態遷移関数は $P(s', r \vert s, a)$ の関数として定義することができる。
<!-- Thus the state-transition function can be defined as a function of $P(s', r \vert s, a)$: -->

$$
P_{ss'}^a = P(s' \vert s, a) = \mathbb{P} \left[S_{t+1} = s' \vert S_t = s, A_t = a\right] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

報酬関数 $R$ は 1 つの行動 (アクション) によって引き起こされる次の報酬を予測する。
<!-- The reward function R predicts the next reward triggered by one action:-->

$$
R(s,a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s'\in \mathcal{S}} P(s',r|s,a)
$$

#### 方針 (ポリシー) policy<!-- #### Policy -->

* 方策 (ポリシー) とは，動作主 (エージェント) の行動関数 $\pi$ として，状態 $s$ においてどのような行動を取るべきかを示すもの。これは，状態 $s$ から行動 $a$ への写像であり，決定論的なものと確率論的なものがある。
<!-- Policy, as the agent's behavior function $\pi$, tells us which action to take in state s.
It is a mapping from state s to action a and can be either deterministic or stochastic:-->

* 決定論的: $\pi(s) = a$.
* 確率論的: $\pi(a \vert s) = \mathbb{P}_ {\pi} \left[A=a \vert S=s\right]$.

#### 価値関数 value function<!-- #### Value Function -->

* 価値関数は，将来の報酬の予測によって，状態の良さや，状態や行動がどれだけ報われるかを測定するもの。
* 将来の報酬は **リターン** とも呼ばれ，今後の報酬を割引いたものの総和。
* 時刻 $t$ から始まるリターン $G_t$ を計算すると以下のようになる:

<!--Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward.
The future reward, also known as **return**, is a total sum of discounted rewards going forward.
Let's compute the return $G_t$ starting from time t:-->

$$
G_{t} = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

$[0,1]$ の割引係数 $\gamma$  は，将来の報酬に罰則 (ペナルティ) を課すことになる。
<!-- The discounting factor $\gamma \in [0, 1]$ penalize the rewards in the future, because:-->

* 将来の報酬は不確実性が高い可能性がある。
例：株式市場。
* 将来の報酬はすぐに得られるものではない。
例：人間として 5 年後よりも今日楽しいことをしたいと思うかもしれない。
* 割引 (ディスカウント) は数学的な利便性をもたらす。
つまり，リターンを計算するために将来のステップを永遠に追跡する必要がないのです。
* 状態遷移グラフの無限ループを心配する必要はない

<!-- - The future rewards may have higher uncertainty; i.e. stock market.
- The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;).
- Discounting provides mathematical convenience; i.e., we don't need to track future steps forever to compute return.
- We don't need to worry about the infinite loops in the state transition graph.-->

* ある状態 $s$ の **状態値** は， 時間 $t$ でこの状態にある場合の期待リターンであり $S_{t}=s$ である。
<!-- The **state-value** of a state s is the expected return if we are in this state at time t, $S_t = s$: -->

$$
V_{\pi}(s) = \mathbb{E}_ {\pi}[G_t \vert S_t = s]
$$

* 同様に，状態と行動の対としての **行動価値**  $Q$ を以下のように定義する:
<!-- Similarly, we define the **action-value** ("Q-value"; Q as "Quality" I believe?) of a state-action pair as:-->

$$
Q_{\pi}(s, a) = \mathbb{E}_ {\pi}[G_t \vert S_t = s, A_t = a]
$$

* 方針 $pi$ に従うので，可能な行動に対する確率分布と Q 値を利用して，状態値を回復することができる。
<!-- Additionally, since we follow the target policy $\pi$, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value: -->

$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

行動価値と状態価値の際は，**アドバンテージ (advantage)** と呼ばれる
<!-- The difference between action-value and state-value is the action **advantage** function ("A-value"): -->

$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$

#### 最適価値と最適方策<!-- ### Optimal Value and Policy -->

* 最適価値関数は，最大リターンを産む
<!-- The optimal value function produces the maximum return: -->

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

* 最適方策は，最適価値関数によって達成される:
<!-- The optimal policy achieves optimal value functions: -->

$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

* $V_{\pi_{\star}}(s)=V_{\star}(s)$ かつ $Q_{\pi_{\star}}(s,a)=Q_{\star}(s,a)$ である
<!-- And of course, we have $V_{\pi_{\star}}(s)=V_{\star}(s)$ and $Q_{\pi_{\star}}(s, a) = Q_{\star}(s, a)$. -->


#### マルコフ決定過程<!-- ## Markov Decision Processes-->

* 広義には強化学習は，**マルコフ決定過程 (MDP: Markov Decision Process)** の一部である。
    * **マルコフ性 Markov property** とは，将来の状態が現在の状態にのみ依存すること。
    * とりわけ，不確かな状況でのマルコフ決定過程を **POMDP (Partially Observed Markov Decision Process)** と呼ぶ
<!-- In more formal terms, almost all the RL problems can be framed as **Markov Decision Processes** (MDPs).
All states in MDP has "Markov" property, referring to the fact that the future only depends on the current state, not the history: -->

$$
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
$$

逆に言えば，未来と過去とは **条件付き独立** である。
将来の意思決定は現在の状況によって定まる。
<!-- Or in other words, the future and the past are **conditionally independent** given the present, as the current state encapsulates all the statistics we need to decide the future. -->

<center>
<img src="/assets/agent_environment_MDP.png" width="49%"><br/>
Fig. 3. The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton & Barto (2017).
</center>

* マルコフ決定過程 (Markov deicison process) は  $\mathcal{M}=\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, の 5 つの要素で構成される
<!-- A Markov deicison process consists of five elements $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, where the symbols carry the same meanings as key concepts in the [previous](#key-concepts) section, well aligned with RL problem settings:-->

- $\mathcal{S}$: 状態集合<!-- - a set of states; -->
- $\mathcal{A}$: 行動集合 <!-- - a set of actions; -->
- $P$: 遷移関数 <!-- - transition probability function; -->
- $R$: 報酬関数<!-- - reward function; -->
- $\gamma$: 割引率<!--  - discounting factor for future rewards. -->

未知の環境では，$P$ と $R$ とは完全に知ることはできない
<!-- In an unknown environment, we do not have perfect knowledge about $P$ and $R$. -->

<center>
<img src="/assets/mdp_example.jpg" width="66%"><br/>
<div style="text-align: center;width:88%;background-color:cornsilk;">
マルコフ決定過程の例：典型的な仕事の一日
(画像出典: [randomant.net/reinforcement-learning-concepts](https://randomant.net/reinforcement-learning-concepts/))
<!--
Fig. 4. A fun example of Markov decision process: a typical work day.
![MDP example]({{ '/assets/images/mdp_example.jpg' | relative_url }}){: class="center"}
Fig. 4. A fun example of Markov decision process: a typical work day.-->
</div>
</center>

#### ベルマン方程式<!-- ### Bellman Equations -->

ベルマン方程式とは， 価値関数を目先の報酬と割引された将来の価値に分解する一連の方程式を指します。
<!-- Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values. -->

$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

Q 関数とは<!-- Similarly for Q-value, -->

$$
\begin{aligned}
Q(s, a)
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \\
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$

##### 期待ベルマン方程式<!-- ### Bellman Expectation Equations -->

* 再帰的な更新過程は，さらに分解すると，状態値関数と行動値関数の両方で構成される方程式となる。
* 今後の行動ステップを進める際には $\pi$ の方針に沿って $V$ と $Q$ を交互に拡張していく。

<!-- The recursive update process can be further decomposed to be equations built on both state-value and action-value functions.
As we go further in future action steps, we extend V and Q alternatively by following the policy $\pi$.-->

<center>
<img src="/assets/bellman_equation.png" width="60%"><br/>
<div style="text-align: center;width: 88%;background-color: powderblue;">
図 5. ベルマン期待値方程式がどのように状態値関数と行動値関数を更新するかを示す図。
<!--
Fig. 5. Illustration of how Bellman expection equations update state-value and action-value functions.
![Bellman]({{ '/assets/images/bellman_equation.png' | relative_url }}){: style="width: 60%;" class="center"}
-->
</div>
</center>

$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi
} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi
} (s', a')
\end{aligned}
$$

##### 最適ベルマン方程式<!-- ### Bellman Optimality Equations -->

* 方策 (ポリシー) に従った期待値を計算するのではなく，最適値にしか興味がないのであれば，方策 (ポリシー) を使わずに代替更新中の最大リターンにすぐに飛びつくことができる。
<!-- RECAP:  最適値 $V_*$ と $Q_*$ は，得られる最高のリターンであり [ここ](#optimal-value-and-policy) で定義されています。-->
<!-- If we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy.
RECAP: the optimal values $V_*$ and $Q_*$ are the best returns we can obtain, defined [here](#optimal-value-and-policy).-->

$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$

* 上式はベルマンの期待方程式と酷似している。
<!-- Unsurprisingly they look very similar to Bellman expectation equations. -->

* もし環境の完全な情報があれば，この問題は計画問題となり，動的計画法 (DP: dynamic programming) で解くことができる。
* 駄菓子菓子，ほとんどの場合 $P_{ss'}^a$ や $R(s, a)$ が不明であるため，ベルマン方程式を直接適用して MDP を解くことはできない。
* 駄菓子菓子，多くの RL アルゴリズムの理論的基礎を与えている。

<!--
If we have complete information of the environment, this turns into a planning problem, solvable by DP.
Unfortunately, in most scenarios, we do not know $P_{ss'}^a$ or $R(s, a)$, so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms. -->


<!--
```python
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
```

[cartpole 問題](https://www.youtube.com/watch?v=J7E6_my3CHk)を解いてみました

```bash
cd ~/study/2019tensorflow_models.git/research/a3c_blogpost
# python a3c_cartpole.py --train
python a3c_cartpole.py --algorithm=random --max-eps=4000
```
-->


<!-- # [GLUE: General Language Understanding Evaluation](https://gluebenchmark.com/leaderboard){:target="_blank"}

<div class="figure figcenter">
<img src="/2023assets/2019-08-21GLUE_leaderboard.png" width="77%">
<div class="figcaption">

[GLUE: General Language Understanding Evaluation](https://gluebenchmark.com/leaderboard){:target="_blank"}
</div></div>
 -->

<!-- 
### GLUE 下位課題

* CoLA: 入力文が英語として正しいか否かを判定
* SST-2: スタンフォード大による映画レビューの極性判断
* MRPC: マイクロソフトの言い換えコーパス。2 文 が等しいか否かを判定
* STS-B: ニュースの見出し文の類似度を 5 段階で評定
* QQP: 2 つの質問文の意味が等価かを判定
* MNLI: 2 入力文が意味的に含意，矛盾，中立を判定
* QNLI: Q and A
* RTE: MNLI に似た 2 つの入力文の含意を判定
* WNI: ウィノグラッド会話チャレンジ -->

<!-- ### SOTA モデルの特徴

* RoBERTa: BERT の訓練コーパスを巨大 (173GB) にし，ミニバッチサイズを大きした
* XLNet: 順列言語モデル。2 ストリーム注意
* MT-DNN: BERT ベース の転移学習に重きをおいたモデル
* GPT-X: BERT に基づく。人間超えして 2019 年 2 月時点で炎上騒ぎ
* BERT: Transformer に基づく言語モデル。**マスク化言語モデル**  と **次文予測** に基づく **事前訓練**，各下流課題を **ファインチューニング**。
事前訓練されたモデルは一般公開済。
* ELMo: 双方向 RNN による文埋め込み表現
* Transformer: 自己注意に基づく言語モデル。多頭注意，位置符号器.

<img src="/2023assets/2019Liu_mt-dnn.png" width="66%">

<img src="/2023assets/2017Vaswani_Fig2_1ja.svg">
<img src="/2023assets/2017Vaswani_Fig2_2ja.svg">

From Vaswani+2017 transformer Fig. 2
 -->

<!-- # 事前訓練とマルチ課題学習

<center>
<img src="/assets/mt-dnn.png" width="66%"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>

$$
\mathop{attention}\left(Q,K,V\right)=\mathop{dropout}\left(\mathop{softmax}\left(\frac{QK^\top}{\sqrt{d}
}\right)\right)V
$$

<center>
<img src="/assets/2017Vaswani_Fig2_1.svg" width="17%">
<img src="/assets/2017Vaswani_Fig2_2.svg" width="23%"><br/>
From [@2017Vaswani_transformer] Fig. 2
</center>


$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\mathop{head}_ {1},\ldots,\mathop{head}_ {h}\right)W^O
$$

where, $\text{head}_{i} = \text{Attention}\left(QW_i^Q,KW_{i}^K,VW_{i}^V\right)$

The projections are parameter matrices

- $W_i^Q\in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^K \in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$,
- $W^O\in\mathbb{R}^{hd_v\times d_{\mathop{model}}}$. $h=8$
- $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$
\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2
$$

$$
\text{PE}_{(\mathop{pos},2i)} = \sin\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$

$$
\text{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)
$$ -->

<!--
# BERT, GPT, ELMo 事前訓練の違い

- BERT:   トランスフォーマー，マスク化言語モデル，次文予測課題
- GPT:   順方向トランスフォーマー
- ELMo:  双方向 RNN による中間層の連結
-->

<!-- # 多言語対応

<center>
<img src="/assets/2019Lample_Fig1.svg" width="88%"><br/>
From [@2019Lample_Cross-lingual] Fig. 1
</center> -->


<!-- # BERT: ファインチューニング手続きによる性能比較

<center>
<img src="/assets/2019Devlin_mask_method21.jpg" width="66%"><br/>
マスク化言語モデルのマスク化割合の違いによる性能比較
</center>

マスク化言語モデルのマスク化割合は マスクトークン:ランダム置換:オリジナル=80:10:10 だけでなく，
他の割合で訓練した場合の 2 種類下流課題，
MNLI と NER で変化するかを下図 \ref{fig:2019devlin_mask_method21} に示した。
80:10:10 の性能が最も高いが大きな違いがあるわけではないようである。 -->

<!-- # BERT モデルサイズ比較
<center>
<img src="./assets/2019Devlin_model_size20.jpg" style="width:69%"><br/>
</center>
 -->

<!-- # BERT: モデルサイズ比較

<center>
<img src="/assets/2019Devlin_model_size20.jpg" width="59%"><br/>
モデルのパラメータ数による性能比較
</center>

パラメータ数を増加させて大きなモデルにすれば精度向上が期待できる。
下図では，横軸にパラメータ数で MNLI は青と MRPC は赤 で描かれている。
パラメータ数増加に伴い精度向上が認められる。
図に描かれた範囲では精度が天井に達している訳ではない。パラメータ数が増加すれば精度は向上していると認められる。 -->


<!-- # BERT: モデル単方向，双方向モデル比較

<center>
<img src="/assets/2019Devlin_directionality19.jpg" width="59%"><br/>
言語モデルの相違による性能比較
</center>

言語モデルをマスク化言語モデルか次単語予測の従来型の言語モデルによるかの相違による性能比較を
下図 \ref{fig:2019devlin_directionality19} に示した。
横軸には訓練ステップである。訓練が進むことでマスク化言語モデルとの差は 2 パーセントではあるが認められるようである。 -->


<!-- # BERT 事前訓練比較
<center>
<img src="/assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:66%"><br/>
</center>
-->

<!-- # BERT: 事前訓練比較

<center>
<img src="/assets/2019Devlin_Effect_of_Pretraining18.jpg" width="59%"><br/>
事前訓練の効果比較
</center>

図には事前訓練の比較を示しされている。
全ての事前訓練を用いた場合が青，次文訓練を除いた場合が赤，従来型言語モデルで次文予測課題をした場合を黄，
従来型言語モデルで次文予測課題なしを緑で描かれている。4 種類の下流課題は MNLI, QNLI, MRPC, SQuAD である。
下流のファインチューニング課題ごとに精度が分かれるようである。 -->

<!--![](2019document/2019Devlin_BERT_slides.pdf)-->
<!--8. [DistilBERT](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)-->

<!-- # under construction 従来モデルの問題点

BERT の意味，文法表現を知るために，從來モデルである word2vec の単語表現概説しておく。
各単語はワンホット onehot 表現からベクトル表現に変換するモデルを単語埋め込みモデル word embedding models あるいはベクト
ル表現モデル vector representation models と呼ぶ。
下図のように各単語を多次元ベクトルとして表現する。-->

<!-- # 単語埋め込みモデルの問題点

単語の意味が一意に定まらない場合，ベクトル表現モデルでは対処が難しい。
とりわけ多義語の意味を定めることは困難である。

下図の単語「アップル」は果物であるか，IT 企業であるかは，その単語を単独で取り出した場合一意に定める事ができない。

<div class="figcenter">

<img src="/2025assets/2019Devlin_BERT02upper.svg" wisth="74%;"><br/>
単語の意味を一意に定めることができない場合
</div>

単語の多義性解消のために，あるいは単語のベクトル表現を超えて，より大きな意味単位である，句，節，文のベクトル表現を得る努力がなされてきた。適切な普遍文表現ベクトルを得ることができれば，翻訳を含む多くの下流課題にとって有効だと考えられる。
seq2seq モデルは RNN の中間層に文情報が表現されることを利用した翻訳モデルであった

<div class="figcenter">

<img src="/2025assets/2019Devlin_BERT03.svg" wisth="74%;"><br/>
[@2014Sutskever_Sequence_to_Sequence] より
</div> -->

<!-- BERT は上述の從來モデルを凌駕する性能を示した。以下では BERT の詳細を見ていくこととする。 -->

<!-- # BERT: 事前訓練とマルチ課題学習

図は事前訓練と GLUE の各課題に対応するためファインチューニングを示している。事前訓練として図中レキシコンエンコーダと表記されている部分は，単語表現，位置符号器，文情報の 3 種類の信号の合成である。合成された入力信号がトランスフォーマーへ入力され事前訓練が行なわれる。事前訓練語，各課題毎にファインチューニングが施される。

<div class="figcenter">

<img src="/2025assets/mt-dnn.png" wisth="74%;"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</div>

 -->
