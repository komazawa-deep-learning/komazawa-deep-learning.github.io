---
title: "第26回 2025年度開講 駒澤大学 人工知能"
author: "浅川 伸一"
layout: home
---

<div align="right">
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 12/Dec/2025<br/>
Appache 2.0 license<br/>
</div>

<link href="/css/asamarkdown.css" rel="stylesheet">

* [課題提出用フォルダ](https://drive.google.com/drive/u/6/folders/1dWcGZ7dzrhy1c7v4i1eOidGInfYAbNSK){:target="_blank"}

# 本日のお品書き

1. 王 さん，特別講義
2. 強化学習 の続き

# キーワード

Agent 57, 短期記憶，エピソード記憶，経験再生，内発的動機づけ

<img src="/2025assets/Gross-anatomy-of-the-brain-Left-panel-shows-the-major-lobes-of-the-outer-neocortex-layer.ppm">


## 実習ファイル (小川裕太郎さんの本)

<!-- - [ランダム探索  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb){:target="_blank"} -->

- [方針勾配法  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb){:target="_blank"}
- [SARSA  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb){:target="_blank"}
- [Q 学習  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb){:target="_blank"}


# Agent57<!-- # Conclusions and the future-->



Agent57 では，Atari ゲーム 57 ベンチマークの全ゲームで人間以上の性能を持つ，より一般的な知的エージェントの構築に成功した。Agent57 は，以前のエージェント Never Give Up (NGU) の上に構築され，エージェントがいつ探索し、いつ利用するか，また，どの時間地平で学習するのが有用かを知るのに役立つ適応的メタコントローラを実体化したものである。幅広いゲーム課題では，当然これらのトレードオフの選択を変える必要があるため，メタコントローラはこのような選択を動的に適応させる方法を提供している。
<!-- With Agent57, we have succeeded in building a more generally intelligent agent that has above-human performance on all tasks in the Atari57 benchmark.
It builds on our previous agent Never Give Up, and instantiates an adaptive meta-controller that helps the agent to know when to explore and when to exploit, as well as what time-horizon it would be useful to learn with.
A wide range of tasks will naturally require different choices of both of these trade-offs, therefore the meta-controller provides a way to dynamically adapt such choices.-->

<div class="figcenter">
<img src="/assets/2020-1029agent57_1.svg" style="width:49%"><br/>
<!--- <img src="https://kstatic.googleusercontent.com/files/f6b5f285173d4449285a8e812b8385f45c03f7104e1c41370a73e0c8558ff82d6a69e60962dd91c4972c444fd73bc4f98a06b5487eff5a037a37bc42f97cef3b" -->
<div class="figcaption">
画像出典：[DeepMind のブログ](https://deepmind.google/blog/agent57-outperforming-the-human-atari-benchmark/)
</div></div>


Agent57 は，学習時間が長いほど得点が高くなり，計算量の増加に対応することができた。これにより，Agent57 は強力な一般的成績を達成することができたが，多くの計算と時間がかかりるので，データ効率は確実に向上させることができる。さらに，Agetn 57 は，Atari 57ゲームのセットでより良い 5 パーセンタイルの成績を示した。これは，データ効率だけでなく，一般性能の点でも，決して Atari の研究の終わりを意味するものではない。
<!--Agent57 was able to scale with increasing amounts of computation: the longer it trained, the higher its score got. While this enabled Agent57 to achieve strong general performance, it takes a lot of computation and time; the data efficiency can certainly be improved. Additionally, this agent shows better 5th percentile performance on the set of Atari57 games. This by no means marks the end of Atari research, not only in terms of data efficiency, but also in terms of general performance.-->

このことについて，2 つの見解を示す。
1. パーセンタイル間の成績を分析することで，Agent57 アルゴリズムがいかに一般的であるかについて新しい洞察を得ることができる。
Agent57 は 全 57 ゲームの最初のパーセンタイルで強力な結果を達成し，MuZero で示されるように NGU や R2D2 よりも良い平均値と中央値を保持しているが，それでもより高い平均性能を得ることができる。
2. 現在のすべてのアルゴリズムは，いくつかのゲームにおいて最適な性能を達成するには程遠いということである。<br/>
そのため，Agent57 が探索，計画，信用割当てに使用する表現を強化することが，使用する上で重要な改善点となるかもしれない。

<!--We offer two views on this: firstly, analyzing the performance among percentiles gives us new insights on how general algorithms are.
While Agent57 achieves strong results on the first percentiles of the 57 games and holds better mean and median performance than NGU or R2D2, as illustrated by MuZero, it could still obtain a higher average performance.
Secondly, all current algorithms are far from achieving optimal performance in some games.
To that end, key improvements to use might be enhancements in the representations that Agent57 uses for exploration, planning, and credit assignment. -->

### Agent57 における短期記憶

Agent57 は R2D2（Recurrent Replay Distributed DQN） をベースとした分散型強化学習エージェントであり、短期記憶として LSTM を用いた再帰的ネットワークを組み込み、部分観測環境（POMDP）における状態推定を行う。すなわち Agent57 における短期記憶とは LSTM の隠れ状態 $h_t, c_t$ のことである。

### Agent57 のアーキテクチャ

<div class="figcenter">
<img src="/assets/2020-1029agent57_2.svg" style="width:66%"><br/>
<div class="figcaption">
Agent57 の分散設定, 画像出典：[DeepMind のブログ](https://deepmind.google/blog/agent57-outperforming-the-human-atari-benchmark/)
</div></div>

1. Agent57 は R2D2 (再帰再生 分散型 Q-Network: Recurrent Replay Distributed DQN) をベースにしている。R2D2 は以下のような処理を行う：
  * 環境からの入力 (Environment observations) を CNN によって画像処理
  * 画像処理された情報を LSTM を用いて，短期記憶を形成
  * LSTM からの情報を用いて，行動価値 (Q 値) を計算

この LSTM の内部状態が連続時間で保持されることで、以下のような短期的推論が可能となる：
  *	過去の動画フレームを記憶
  * 隠れ状態（部分観測）を推測
  * 一時的な課題構造（探索課題、迷路など）を記憶


2. Agent57 のエピソード記憶 Episodic Memory Module（RND + NH）

Agent57 には「人間のエピソード記憶に相当する」外部モジュールが存在する。これは「長期記憶」に近く、短期記憶（working memory）ではありません。
すなわち，以下のような対応関係がある:
* 短期記憶＝LSTM
* 長期記憶＝RND/NGU 系の報酬モジュール・Replay Buffer

### なぜ Agent57 には短期記憶（LSTM）が必要なのか？

特に Atari 57 の中には 部分観測性が強いゲームが存在する。例としては，
  * モンテズマの復讐：直前の鍵を取ったかどうかが画面に映っていない
  * フロストバイト：自分の位置・スピードの内部状態が必要
  * 迷路系ゲーム：一時的に見えない位置関係を推論し続けないといけない

こうしたゲームは、単純な CNN＋MLP では状態が推定できない。このため Agent57 では LSTM に短期記憶を担わせ、過去の数十〜数百タイムステップを圧縮して保持させている。

### R2D2 の 系列再生 Sequence Replay＝短期記憶を壊さない仕組み

通常の再生は単一ステップのサンプリングを行うが、LSTM の「短期記憶」は遷移をバラバラに切り出すと壊れてしまう。
そこで R2D2／Agent57 以下を導入することで，サンプリング効率の高い学習を可能とした：
  * Stored sequences replay
	  * 過去の挙動を「まとまり（sequence chunk）」として保存
    * LSTM の状態を正しく再構築しながら学習する

<!-- ■ 簡単にまとめると… -->

<!-- Agent57 の短期記憶は LSTM の内部状態である。
	•	環境は部分観測 → LSTM が観測の履歴を圧縮
	•	Replay は sequence 単位 → LSTM の短期記憶を壊さず学習
	•	過去情報を使って探索／長期依存行動を可能にする

この「短期記憶」があったからこそ、Agent57 は Atari57 全ゲームで人間超えを達成しました。 -->

<!-- ## Agent57 の新奇性，好奇心

**Agent57 / NGU（Never Give Up）論文の “Episodic Memory Module” は RND + NH ではなく、
“RND（Random Network Distillation）ではない別の新奇性推定法＋NH（Nearest-Neighbor based Hashing）に近い概念”**が誤って略されて語られることがあるものです。

実際には Agent57 の Episodic Novelty は “RND を使わない、kNN ベースの距離計算” であり、
業界の一部資料で RND + NH と誤記されるケースがあります。

以下、正確な説明をします。

✅ 1. 結論：Agent57 の Episodic Novelty に RND は使われていない

Agent57 の内発的動機づけ（NGU）は次の 2 つで構成されます：
	1.	Episodic Novelty（エピソード内新奇性）
	2.	Life-long Novelty（長期新奇性）

このうち Episodic Noveltyは RND ではなく、
以下の 3 つの要素で構成されています：
	•	Embedding Network（観測の特徴抽出 φ(s)）
	•	kNN-based novelty（k 近傍距離）
	•	Running normalizer（NH: Novelty Hashing / Normalized Hash）
→ 後述する「Normalization Heuristic」に近い概念

つまり、Agent57 の Episodic Novelty は「距離ベース（kNN）方式」であり、RNDは使用していません。

⸻

✅ 2. なぜ “RND + NH” と誤解されやすいのか？

理由は2つあります：

⸻

(A) RND（Random Network Distillation）が “似た用途” を持っていたから

RND は「未知状態に対して prediction error が大きい」性質を利用して新奇性を測る手法。

【RND の基本式】

r_t^{RND} = \| f_\text{target}(s_t) - f_\text{predictor}(s_t) \|^2

これは「未知状態ほど大きい誤差 → 高い intrinsic reward」
という仕組みなので、機能的には “新奇性測定器” の一種。

⸻

(B) NGU（Never Give Up）が RND の改良版と思われがち

NGU の登場は RND と時期が近く、論文タイトルも似ていて、
どちらも探索タスクの性能向上を目的としているため、

「NGU の Episodic Novelty ≈ RND の改良版？」

と誤解されやすい。

実際には NGU は RND ではなく kNN-based novelty を採用。

⸻

(C) “NH” の誤解（Novelty Hashing / Normalization Heuristic）

NGU では episodic novelty を安定化するために Normalization Heuristic
（論文では “NH” と略されることがある）を導入している。

この NH が “ハッシュ” のような振る舞い（値域の正規化）を行うため、
一部の再解説資料で Novelty Hashing（NH） と書かれたことが誤解の原因。

⸻

✅ 3. 正しい：Agent57 Episodic Novelty の公式式（NGU論文より）

Episodic memory モジュールは：

\phi(s_t) = \text{embedding network}(s_t)

エピソード内メモリ M を保持し、

d_t = \min_{m\in M} \| \phi(s_t) - m \|^2

この距離 d を変換して novelty にする：

r^{epi}_t = f(d_t)

ここで f は “running normalization”（これが NH）を含む非線形変換。

ポイント
	•	RND のような prediction error を計算しない
	•	完全に「距離ベース」の新奇性メトリック
	•	エピソード終端で M をリセットすることで “短期記憶” を模倣

⸻

4. 関係性まとめ

|モデル | 種類 | 使う新奇性 | 備考 |
|:---|:---|:---|:---|
|RND | 観測新奇性 | prediction error | 長期ノイズ問題あり|
|NGU Episodic Novelty| 短期新奇性 |kNN距離 + 正規化（NH）|Agent57 で採用|
|NGU Lifetime Novelty| 長期新奇性| 来訪頻度の統計 | RND とは別物|
-->

## Agent57 の エピソード新奇性 episodic novelity

Agent57 の episodic novelty は、次の 3 要素で構成:
1. 特徴埋め込み（embedding network）$\phi(s_t)
2. エピソード内メモリ M との kNN 距離 　「いま見ている状態が episode 内でどれくらい珍しいか」
3. Normalization Heuristic（NH）によるスケーリング 距離値を安定した novelty 値へ変換

Agent57 の Episodic Novelty の最終式（本質形）
$$
r^{epi}_t = f\!\left( \mathrm{sim}\left(\phi(s_t), M\right) \right)
$$
ここで
* kNN-based similarity（距離）
$$
\mathrm{sim}(\phi(s_t), M)= \sum_{i=1}^k \exp\left(-\frac{\|\phi(s_t)-m_i\|^2}{\sigma^2}\right)
$$
*	$m_i$ は M 内の最近傍の埋め込み
* $k$ は最近傍個数（論文では 10）
* $\sigma$ は距離スケーリングパラメータ

$f$ は Normalization Heuristic（NH）

論文では以下のように変換される：
1. 値をエピソード内統計で正規化
2. しきい値より小さい場合はクリップ（新奇性ゼロ扱い）
3. 大きすぎる値も正規化して安定化

非線形性を含むため厳密式を論文は明示しないが、本質は以下のような形：
$$
r^{epi}_t = \max\left(0,\;\frac{\mathrm{sim}(\phi(s_t), M) - \mu_M}{\sigma_M}\right)
$$
* $\mu_M$, $\sigma_M$ は過去の similarity 値の平均・標準偏差
*	正規化後に ReLU（または SoftPlus）でクリップする

1. ステップ別に詳細解説

**Step 1：状態の埋め込み（Embedding network）**<br/>
観測 $s_t$ を CNN/LSTM を通して embedding に変換：$\phi(s_t) \in \mathbb{R}^d$

この埋め込みは：
* 視覚特徴
* エピソード性の距離計算に適した表現
* 	一般化（似た状態を近づけ、異なる状態を離す）

ように学習されている。

**Step 2：エピソード内メモリ M の構築**<br/>
各エピソード開始時：$M = \emptyset$<br/> 
各ステップで $M \leftarrow M \cup \{ \phi(s_t) \}$ → 海馬の「エピソード記憶」とほぼ同じ動作。

**Step 3：kNN 距離による新奇性計算**<br/>
いまの \phi(s_t) と M 内の埋め込みのうち最も近い k 個 を取得（k=10）$d_i = \|\phi(s_t)-m_i\|^2$<br/>
ここで距離をそのまま使うのではなく、距離が近いほど penalize（＝似ているほど novelty を下げる）ための Gaussian kernel を使う：$\mathrm{sim}_i = \exp\left(-\frac{d_i}{\sigma^2}\right)$

そして k 個を加算：$\mathrm{sim}(\phi(s_t), M) = \sum_{i=1}^k \mathrm{sim}_i$

直感
* 既に M に近い埋め込みが多い → sim が大 → 新奇性は低
* どのメモリとも遠い → sim が小 → 新奇性が高い

つまり：“似ているものがないほど novelty が高い” というエピソード内新奇性の定義。

**Step 4：Normalization Heuristic（NH）**<br/>
NGU が最も工夫している部分。
sim 値は環境によってスケールが異なるため、
安定した intrinsic reward にする必要がある。

具体的には：
1. 平均・分散による標準化 $z_t = \frac{\mathrm{sim} - \mu_M}{\sigma_M}$
2. 負の値は 0（“既知状態” は novelty なし）$z_t \leftarrow \max(0, z_t)$
3. 上限を制限（スケーリング）$r^{epi}t = \min(z_t, r{\max})$

これが NH（Normalization Heuristic） と呼ばれる。


**Agent57 論文内の（簡略化された）最終式との対応**

論文では次のような概念式として書かれる：$r^{episodic}(s_t) = f(\mathcal{N}_k(\phi(s_t), M))$<br/>
* $\mathcal{N}_k$：kNN 距離の集合
* $f$：Normalization Heuristic

正確には距離の生値ではなく「Gaussian kernel を通した similarity」を使っている点がポイント。

**Agent57 が episodic novelty を採用した理由**

従来の探索（RND 含む）との比較：

|手法|主原理|問題点|NGU の改善|
|:---|:---|:---|:---|
|ε-greedy|ランダム|効率悪い|directed exploration|
|RND|prediction error|反復訪問で誤作動|“episode 内だけ” 初見性を見る|
|count-based|訪問頻度|連続空間で不適|embedding で連続空間に対応|

つまり NGU は：“海馬のように、1 エピソードの中での初見状態に反応する” → Human-like exploration を実現

4. 全式まとめ（論文より明確化した版）
* $\phi_t = \phi(s_t)*
* $d_i = \|\phi_t - m_i\|^2$
* $\mathrm{sim}_i = \exp(-d_i/\sigma^2)$
* $\mathrm{sim}t = \sum{i=1}^k \mathrm{sim}_i$
* $z_t = \frac{\mathrm{sim}_t - \mu_M}{\sigma_M}$
* $r^{epi}_t = \max(0, z_t)$

## Agent57 におけるメタコントローラ

Agent57 の Meta-controller、とりわけ **UCB（Upper Confidence Bound）による方略選択（探索度 $\beta$， 割引率 $\gamma$ の切り替え）**を説明する。
これは Agent57 の「人間のメタ認知（executive control）」に相当する最も重要な革新点である。

1. Meta-controller とは？（Agent57 の全体観）

Agent57 は単一の方針ではなく、次のように 複数の方策族（policy family）を持つ：
* それぞれ異なる 探索係数 $\beta$（intrinsic reward の重み）
* それぞれ異なる 割引率 $\gamma$（時間スケール）

Agent57 内部には複数の方針が存在する:
* 短期的行動を重視する方策（低 γ）
* 長期的計画を重視する方策（高 γ）
* 好奇心が強い方策（高 β）
* 報酬最適化に集中する方策（低 β）

2. なぜ複数方策が必要なのか？

Atari の 57 ゲームは性質が大きく異なる：
*	短期報酬が重要なゲーム（例：Pong） → 低 γ の方が合理的
* 長期計画が必要なゲーム（例：Montezuma’s Revenge） → 高 γ が必要
* 探索が重要なゲーム → 高 β が必要
* 探索より exploitation が重要なゲーム → 低 β が必要

単一の (β,γ) では 全ゲームで optimal を同時に実現できない。だから “複数方策のポートフォリオ” が必要。

3. Meta-controller の仕事：どの方策を使うか決める<br/>

Meta-controller は step-by-step ではなくエピソード単位で、「次のエピソードではどの (β,γ) の方針で学習・行動するか」を選択する。
その選択基準は：
* その方針が 最近どれくらい上手くいったか（平均リターン）
* まだどれくらい “使っていない” か（不確実性）

これを形式化したものが UCB（Upper Confidence Bound）と呼ばれる。

4. UCB の数式<br/>
方針集合を $\Pi = \{\pi_1, \pi_2, \dots, \pi_K\}$ とする（K=32 など）。
各方方針 $\pi_k$ には使用回数 $n_k$ と平均リターン $\hat{\mu}_k$ がある。

UCB 得点： $\text{UCB}_k=\hat{\mu}_k + c\sqrt{\frac{\ln N}{n_k}}$<br/>
ここで，$N=\sum_k n_k$, c は 探索に対する重み (exploration weight)

次のエピソードで使う方策は 
$$
k^{\star}=\arg\max_k \text{UCB}_k
$$ 
これが meta-controller の中心動作である。
* 第一項：平均リターン（exploitation）　→ 過去に成功した方策を優先
* 第二項：不確実性（exploration）　→ 未試行、または試行回数が少ない方策を積極的に試す

つまり UCB は “成功しつつ、まだ試していない方策をバランスよく選ぶ” というメタ戦略。

5. β（探索強度）の選び方を UCB が最適化する

Agent57 の β は
*	β が大：intrinsic reward（新奇性）が強い → 探索重視
* β が小：外発報酬重視 → exploitation

ゲームによって最適 β は異なる。

Meta-controller は：
1.	β の高い方策を試す → 探索が成功するなら続ける
2.	外発報酬が大事なゲームなら β 小の方策の方がリターンが高くなる
3.	UCB は最終的に “高リターンの β” を選び続ける

6. γ（時間スケール）の選択

γ は discount factor で、
* γ が高い → 将来の報酬を重要視 → 長期計画
* γ が低い → 目先の報酬を重視 → 短期反応

Atari には
* 長期型：Montezuma’s Revenge, Pitfall
* 短期型：Pong, Boxing

などがあるので 最適 γ はゲーム依存。

UCB により：
* 長期ゲーム → 高 γ の方策を優先
* 短期ゲーム → 低 γ の方策を優先

これが自動的に選択される。

7. 心理学，認知科学との対応

Meta-controller は認知科学では，ACC（前帯状皮質），dlPFC（背外側前頭前野）の機能と対応する。

ACC の役割（Expected Value of Control Theory）
* 今どの戦略（探索or exploitation）を選ぶべきか
* どれくらい effort を払うべきか
* 不確実性が高いと探索を増やす

これはまさに UCB の機能と一致：
* exploitation ＝ μ
* exploration ＝ $\sqrt{\log N/n}$

つまり：

Agent57 Meta-controller は「数学化された前頭前野‐ACC の制御回路」と解釈できる。

8. Agent57 は “方策のメタ選択” 

単一の探索戦略ではなく、
* 探索強度（β）× 時間スケール（γ）による 戦略ポートフォリオ を持ち、
* メタレベルの UCB で方針を選ぶ

という階層構造が 人間の “方略選択能力” を模倣している。

その結果：
* Montezuma のような長期依存
* RND や ICM が失敗した sparse reward 問題
* 全 57 ゲームで人間を上回る性能を実現

9. まとめ

Meta-controller（UCB）は Agent57 の中核：
* β（探索強度）を動的に選ぶ
* γ（時間スケール）を動的に選ぶ
* 平均リターン + 不確実性に基づき選択
* ACC/dlPFC の “メタ認知・戦略制御” と対応

式は：
$$
\text{UCB}_k = \hat{\mu}_{k} + c\sqrt{\frac{\ln N}{n_k}}
$$
意味は：
「成功しそうで、かつまだ試していない方策を選ぶ」
Agent57 の汎用性・人間レベル性能の鍵。




### DQN からの改善

* 二重 DQN
* 優先度付き経験再生
* 決闘 (dueling)
* 分散化

#### 短期記憶

* LSTM, GRU $\rightarrow$ R2D2

#### エピソード記憶

* メモリーネットワーク
* ニューラルエピソディック制御
* トランスフォーマー

#### 探索

* 好奇心
* 内発的動機づけ

#### メタ制御


### エージェント 57 の先祖

2012 年 DeepMind は Atari57 ゲーム群に取り組むために Deep Q network エージェント（DQN）を開発した。
以来，研究コミュニティは DQN の多くの拡張機能や代替機能を開発してきた。
しかし，これらの進歩にもかかわらず，すべての深層強化学習エージェントは 4 つのゲーム で一貫してスコアを出すことができなかった。
4 つのゲーム: モンテズマの復讐，ピットフォール，ソラリス，スキー

モンテズマの復讐 と ピットフォール は，良好な成績を得るために広範な探索が必要となる。
学習における中心的なジレンマは，探索と利用の問題である。
例えば，地元のレストランでいつも同じお気に入りの料理を注文すべきか，それとも，昔からのお気に入りを上回るかもしれない何か新しいものを試すべきなのか，である。
探索活動には，最終的により強力な行動を発見するために必要な情報を収集するために行われる。
このとき，多くの最適でない行動を取ることが含まれる。

ソラリス と スキー では長期的な信用割り当ての問題がある。 ソラリスとスキーでは，エージェントの行動の結果とそれが受け取る報酬を一致させることが困難である。
エージェントは学習に必要なフィードバックを得るために，長い時間スケールで情報を収集しなければならない。

<center>

[エイリアン](https://youtu.be/luZm3jmwGwI?list=PLqYmG7hTraZCHS3JLle_kxwNvImpYVq4z) :<br/>

<iframe width="600" height="320" src="https://www.youtube.com/embed/luZm3jmwGwI?list=PLqYmG7hTraZCHS3JLle_kxwNvImpYVq4z" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<!--
<center>
* ソラリス:<br/> <video width="38%" repeat controls><source src="https://youtu.be/QDb3rmEBTZI"></video>
* スキー:<br/> <video width="38%" repeat controls><source src="https://youtu.be/Lppco8heTxI"></video>
* モンテズマの復讐: <video width="38%" repeat controls><source src="https://youtu.be/M9Yn1kYZb6E"></video>
* ピットフォール: <video width="38%" repeat controls><source src="https://youtu.be/LRRq3BXh0xk"></video>
</center>
-->



### DQN の改善
<!-- # DQN improvements-->

DQN の初期の改良では，二重 DQN，経験の優先再生，決闘アーキテクチャなど，学習効率と安定性が向上した。
これらの変更により，エージェントは経験をより効率的かつ効果的に利用できるようになった。

<!--Early improvements to DQN enhanced its learning efficiency and stability, including double DQN, prioritised experience replay and dueling architecture. These changes allowed agents to make more efficient and effective use of their experience.-->

### 分散エージェント
<!-- ## Distributed agents-->

* 複数のコンピュータ上で同時に実行できる分散型の DQN, Gorila, Ape-X を導入
* エージェントはより迅速に経験を獲得し，経験から学ぶことが可能となった。
* アイデアを迅速に反復することができるようになった。
* Agent57 もまたデータ収集と学習処理と切り離した分散型強化学習エージェント
* 多くのアクターが環境の独立したコピーと相互作用し，優先順位付けされた経験再生バッファの形で中央の「メモリバンク」にデータを供給する。
<!--図 4 に示すように，-->
学習者はこの経験再生バッファから訓練データをサンプリングし，
<!--学習者は，-->これらの経験再生を使って損失関数を構築し，行動やイベントのコストを推定する。
* 損失を最小化することでニューラルネットワークのパラメータを更新する。
* <!--最後に，--> 各アクターは学習者と同じネットワーク・アーキテクチャを共有する。
<!--だが，重み係数の独自のコピーを持ちます。-->
* 学習者の重み係数はアクターに頻繁に送られ，後述するように，アクターは個々の優先順位によって決定された方法で自分の重みを更新する

<!-- Next, researchers introduced distributed variants of DQN, Gorila DQN and ApeX,  that could be run on many computers simultaneously. This allowed agents to acquire and learn from experience more quickly, enabling researchers to rapidly iterate on ideas. Agent57 is also a distributed RL agent that decouples the data collection and the learning processes. Many actors interact with independent copies of the environment, feeding data to a central ‘memory bank’ in the form of a prioritized replay buffer. A learner then samples training data from this replay buffer, as shown in Figure 4, similar to how a person might recall memories to better learn from them.  The learner uses these replayed experiences to construct loss functions, by which it estimates the cost of actions or events. Then, it updates the parameters of its neural network by minimizing losses. Finally, each actor shares the same network architecture as the learner, but with its own copy of the weights. The learner weights are sent to the actors frequently, allowing them to update their own weights in a manner determined by their individual priorities, as we’ll discuss later.  -->

<center>

* [ソラリス](https://youtu.be/QDb3rmEBTZI)<br/>
<iframe width="560" height="320" src="https://www.youtube.com/embed/QDb3rmEBTZI?list=PLqYmG7hTraZBuNkJn6YFhi7TYrAg_NDAr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>



#### 短期記憶<!-- ## Short-term memory-->

エージェントは，過去の観察を考慮に入れて意思決定を行うために，記憶を持つ必要がある。
これにより，エージェントは，現在の観察 (通常は部分的な観察，つまり，エージェントは自分の世界の一部しか見ていない)だけでなく，環境全体についてのより多くの情報を明らかにすることができる過去の観察に基づいて意思決定を行うことができるようになる。
例えば，ある建物内の椅子の数を数えるために，エージェントが部屋から部屋へと移動するタスクを考えてみよ。
記憶を持っていなければ，エージェントはある部屋の観察にしか頼ることができない。
記憶を持っていれば，エージェントは前の部屋の椅子の数を記憶し，現在の部屋で観察している椅子の数を足すだけでタスクを解くことができる。
したがって，記憶の役割は，過去の観察から得られた情報を集約して意思決定プロセスを改善することである。
深層強化学習では，短期記憶として LSTM（Long-Short Term Memory）などのリカレントニューラルネットワークが用いられる。
<!-- Agents need to have memory in order to take into account previous observations into their decision making. This allows the agent to not only base its decisions on the present observation (which is usually partial, that is, an agent only sees some of its world), but also on past observations, which can reveal more information about the environment as a whole. Imagine, for example, a task where an agent goes from room to room in order to count the number of chairs in a building. Without memory, the agent can only rely on the observation of one room. With memory, the agent can remember the number of chairs in previous rooms and simply add the number of chairs it observes in the present room to solve the task. Therefore the role of memory is to aggregate information from past observations to improve the decision making process. In deep RL and deep learning, recurrent neural networks such as Long-Short Term Memory (LSTM) are used as short term memories. -->

記憶と行動とのインターフェースは，自己学習するシステムを構築する上で極めて重要である。
強化学習では，エージェントは，その直接的な行動の価値のみを学習することができるオンポリシー学習者になることができる。
あるいは，それらの行動を実行していなくても最適な行動について学習することができるオフポリシー学習者になることができる。
--- 例えば，ランダムな行動を取っているかもしれないが，可能な限り最高の行動が何であるかを学習することができる。
したがって，オフポリシー学習はエージェントにとって望ましい特性であり，エージェントが自分の環境を徹底的に探索しながら，取るべき最善の行動のコースを学ぶのを助けることができる。
オフポリシー学習と記憶を組み合わせるのは難しいが，異なる行動を実行するときに何を覚えているかを知る必要があるからである。
例えば，リンゴを探しているときに覚えていること（例えば，リンゴがどこにあるか）は，オレンジを探しているときに覚えていることとは異なる。
しかし，オレンジを探していたとしても，将来的にリンゴを探す必要が出てきた場合に備えて，偶然リンゴに出くわしたとしても，リンゴを探す方法を覚えることができる。
メモリとオフポリシー学習を組み合わせた最初の 深層強化学習エージェント は，ディープリカレント Q-Network(DRQN) であった。
さらに最近になって，短期記憶のニューラルネットワークモデルとオフポリシー学習と分散学習を組み合わせた Recurrent Replay Distributed DQN (R2D2)
で Agent57 の系譜に大きな飛躍が発生し，Atari57 では非常に強い平均性能を達成した。
R2D2 は，過去の経験からの学習のための経験再生の仕組みを短期記憶に働きかけるように修正している。
以上のことから，R2D2 は収益性の高い行動を効率的に学習し，報酬のためにそれを利用することができた。
<!--Interfacing memory with behaviour is crucial for building systems that self-learn. In reinforcement learning, an agent can be an on-policy learner, which can only learn the value of its direct actions, or an off-policy learner, which can learn about optimal actions even when not performing those actions – e.g., it might be taking random actions, but can still learn what the best possible action would be.  Off-policy learning is therefore a desirable property for agents, helping them learn the best course of action to take while thoroughly exploring their environment. Combining off-policy learning with memory is challenging because you need to know what you might remember when executing a different behaviour. For example, what you might choose to remember when looking for an apple (e.g., where the apple is located), is different to what you might choose to remember if looking for an orange. But if you were looking for an orange, you could still learn how to find the apple if you came across the apple by chance, in case you need to find it in the future. The first deep RL agent combining memory and off-policy learning was Deep Recurrent Q-Network (DRQN). More recently, a significant speciation in the lineage of Agent57 occurred with Recurrent Replay Distributed DQN (R2D2), combining a neural network model of short-term memory with off-policy learning and distributed training, and achieving a very strong average performance on Atari57.  R2D2 modifies the replay mechanism for learning from past experiences to work with short term memory. All together, this helped R2D2 efficiently learn profitable behaviours, and exploit them for reward. -->


#### エピソード記憶<!-- ## Episodic memory-->

ネバーギブアップ (NGU) は，R2D2 にエピソード記憶という別の形の記憶を補強するように設計された。
NGU  はゲームの新しい部分に遭遇したときにそれを検知し，エージェントが報酬を得るためにその新しい部分を探索することができるようになった。
このため，エージェントの行動 (探索) は，エージェントが学習しようとしている方針 (ゲームで高得点を得ること) から大きく逸脱することになり，ここでも方針逸脱学習が重要な役割を果たす。
NGU は，Atari 57 ベンチマークの導入以来，どのエージェントも得点できなかった Pitfall や，その他の難しい Atari ゲームにおいて，領域知識なしで正の報酬を得た最初のエージェントであった。
残念ながら，NGU は歴史的に「簡単な」ゲームでは性能を犠牲にしたため，平均して R2D2 より性能が劣っている。
<!-- We designed Never Give Up (NGU) to augment R2D2 with another form of memory: episodic memory.
This enables NGU to detect when new parts of a game are encountered, so the agent can explore these newer parts of the game in case they yield rewards.
This makes the agent’s behaviour (exploration) deviate significantly from the policy the agent is trying to learn (obtaining a high score in the game); thus, off-policy learning again plays a critical role here.
NGU was the first agent to obtain positive rewards, without domain knowledge, on Pitfall, a game on which no agent had scored any points since the introduction of the Atari57 benchmark, and other challenging Atari games.
Unfortunately, NGU sacrifices performance on what have historically been the “easier” games and so, on average, underperforms relative to R2D2.
我々は R2D2 を別の記憶形態であるエピソード記憶で補強するために Never Give Up (NGU) を設計した。
これにより NGU はゲームの新しい部分に遭遇したときにそれを検出することができ，それが報酬をもたらす場合には，エージェントはゲームのこれらの新しい部分を探索することができるようになる。
これにより，エージェントの行動（探索）は，エージェントが学習しようとしている方針（ゲームで高得点を得ること）から大きく逸脱してしまいます。
NGU は Atari57 ベンチマークが導入されて以来，誰もポイントを獲得していないゲームである Pitfall や，他の難解な Atari ゲーム で， ドメイン知識なしで，ポジティブな報酬を獲得した最初のエージェントであった。
残念なことに NGU は歴史的に「より簡単な」ゲームでの成績を犠牲にしている。
このため，平均的には R2D2 と比較して低い成績となった。-->


## 直接的な探索を促すための内発的動機づけの方法<!-- # Intrinsic motivation methods to encourage directed exploration-->

最も成功した戦略を発見するためには，エージェントは環境を探索しなければならないが，探索戦略の中には他の戦略よりも効率的なものもある。
DQN では，$\epsilon$ 貪欲(グリーディ) として知られる無方向探索戦略を用いて探索問題を解決しようとした。
イプシロングリーディとは一定の確率（イプシロン）でランダムな行動をとり，そうでなければ現在の最良の行動を選ぶことである。
報酬がない場合，大きな状態行動空間を探索するのに膨大な時間を必要とする。
この限界を克服するために，多くの有向探索戦略が提案されてきた。
これらの中で，ある分野では，新規性を求める行動に対してより密な「内部」報酬を提供することで，エージェントが可能な限り多くの状態を探索し，訪問することを促す内発的動機報酬の開発に焦点を当ててきた。
その中で，我々は 2 つのタイプの報酬を区別した。
第一のタイプの報酬は，長期的な新規性報酬は，訓練期間中，多くのエピソードに渡って多くの状態を訪問することを促す。
第二のタイプの報酬は，短期的な新規性報酬は，短期間（例えば，ゲームの 1エピソード内）に多くの状態を訪問することを促すものである。
<!--In order to discover the most successful strategies, agents must explore their environment–but some exploration strategies are more efficient than others.
With DQN, researchers attempted to address the exploration problem by using an undirected exploration strategy known as epsilon-greedy: with a fixed probability (epsilon), take a random action, otherwise pick the current best action.
However, this family of techniques do not scale well to hard exploration problems: in the absence of rewards, they require a prohibitive amount of time to explore large state-action spaces, as they rely on undirected random action choices to discover unseen states.
In order to overcome this limitation, many directed exploration strategies have been proposed.
Among these, one strand has focused on developing intrinsic motivation rewards that encourage an agent to explore and visit as many states as possible by providing more dense “internal” rewards for novelty-seeking behaviours.
Within that strand, we distinguish two types of rewards: firstly, long-term novelty rewards encourage visiting many states throughout training, across many episodes. Secondly, short-term novelty rewards encourage visiting many states over a short span of time (e.g., within a single episode of a game). -->


### 長時間軸での新規性<!-- ## Seeking novelty over long time scales-->

長期的な新規性報酬は，以前に見たことのない状態がエージェントの生涯で遭遇したときのキッカケであり，訓練中にこれまでに見た状態密度の関数となる。
ある状況に接する頻度が高い場合 (その状態がよく知られていることを示す)，長期的な新規性報酬は低い。その逆もまた然り。
すべての状態が見慣れた状態であれば，エージェントは無方向の探索戦略に頼ることになる。
しかし，高次元空間の密度モデルの学習は，**次元の呪い** のために問題が多い。
実際には，エージェントが深層学習モデルを用いて密度モデルを学習する場合，壊滅的忘却（新しい経験に遭遇すると以前に見た情報を忘れる）や，すべての入力に対して正確な出力を生成することができないことに悩まされる。
例えば，モンテズマの復讐では，指向性のない探索戦略とは異なり，長期的な新規性報酬により，エージェントは人間のベースラインを超えることができる。
しかし，モンテズマの復讐で最高のパフォーマンスを発揮する方法であっても，密度モデルを適切な速度で注意深く訓練する必要がある。
密度モデルが最初の部屋の状態が馴染みのあるものであることを示すとき，エージェントは一貫して馴染みのない領域に到達することができるはずである。
<!-- Long-term novelty rewards signal when a previously unseen state is encountered in the agent’s lifetime, and is a function of the density of states seen so far in training: that is, it’s adjusted by how often the agent has seen a state similar to the current one relative to states seen overall.
When the density is high (indicating that the state is familiar), the long term novelty reward is low, and vice versa.
When all the states are familiar, the agent resorts to an undirected exploration strategy. However, learning density models of high dimensional spaces is fraught with problems due to the curse of dimensionality.
In practice, when agents use deep learning models to learn a density model, they suffer from catastrophic forgetting (forgetting information seen previously as they encounter new experiences), as well as an inability to produce precise outputs for all inputs.
For example, in Montezuma’s Revenge, unlike undirected exploration strategies, long-term novelty rewards allow the agent to surpass the human baseline.
However, even the best performing methods on Montezuma’s Revenge need to carefully train a density model at the right speed: when the density model indicates that the states in the first room are familiar, the agent should be able to consistently get to unfamiliar territory. -->

<center>

[モンテズマの復讐](https://www.youtube.com/watch?v=M9Yn1kYZb6E&list=PLqYmG7hTraZB5YFgejiwDoKBkg50SlY6z):<br/>
<iframe width="600" height="352" src="https://www.youtube.com/embed/M9Yn1kYZb6E?list=PLqYmG7hTraZB5YFgejiwDoKBkg50SlY6z" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>


### 短期間での新規性<!-- ## Seeking novelty over short time scales-->

短期的な新規性報酬は，エージェントが最近の過去に遭遇しなかった状態を探索することを促すために使用することができる。
近年，エピソード記憶のいくつかの特性を模倣したニューラルネットワークが強化学習エージェントの学習を高速化するために使用されている。
エピソード記憶もまた，新しい経験を認識するために重要であると考えられている。
このため，我々はこれらのモデルを適応させ，Never Give Up に短期的な新規性の概念を与えるようにした。
エピソード記憶モデルは（モデルのパラメータを学習したり，適応させたりする必要がなく）その場で適応させることができるノンパラメトリックな密度モデルを素早く学習することができる。
このため，短期的な新規性報酬を計算するための効率的で信頼性の高い候補である。
この場合，報酬の大きさは，エピソード記憶に記録された現在の状態と以前の状態との間の距離を測定することによって決定される。
<!-- Short-term novelty rewards can be used to encourage an agent to explore states that have not been encountered in its recent past.
Recently, neural networks that mimic some properties of episodic memory have been used to speed up learning in reinforcement learning agents.
Because episodic memories are also thought to be important for recognising novel experiences, we adapted these models to give Never Give Up a notion of short-term novelty.
Episodic memory models are efficient and reliable candidates for computing short-term novelty rewards, as they can quickly learn a non-parametric density model that can be adapted on the fly (without needing to learn or adapt parameters of the model).
In this case, the magnitude of the reward is determined by measuring the distance between the present state and previous states recorded in episodic memory.-->

すべての距離の概念が意味のある探索を促進するわけではない。
例えば，多くの歩行者や車両がいる混雑した街をナビゲートする課題を考えると，
あるエージェントが，視覚的なあらゆる微小な変化を考慮に入れた距離の概念を使用するようにプログラムされているとすれば，そのエージェントは，受動的に環境を観察するだけで，静止していることさえも含めて多くの異なる状態を訪れることになる。
このようなシナリオを避けるためには，エージェントが探索にとって重要だと考えられる特徴（たとえば，制御性）を学習し，その特徴についてだけ考慮した距離を計算する必要がある。
このようなモデルはこれまでも探索に用いられてきた。
だが，このエピソード記憶との組み合わせは ネバーギブアップ探索手法の主な進歩の一つであり，これにより Pitfall の性能が人間を凌駕した。
<!-- However, not all notions of distance encourage meaningful forms of exploration.
For example, consider the task of navigating a busy city with many pedestrians and vehicles.
If an agent is programmed to use a notion of distance wherein every tiny visual variation is taken into account, that agent would visit a large number of different states simply by passively observing the environment, even standing still – a fruitless form of exploration.
To avoid this scenario, the agent should instead learn features that are seen as important for exploration, such as controllability, and compute a distance with respect to those features only. Such models have previously been used for exploration, and combining them with episodic memory is one of the main advancements of the Never Give Up exploration method, which resulted in above-human performance in Pitfall!. -->

<center>

[NGU による Pitfall!](https://youtu.be/imAeLt1BPu4?list=PLqYmG7hTraZBgXcetCL9zzd6Cck8S4l4k)<br/>
<iframe width="548" height="323" src="https://www.youtube.com/embed/imAeLt1BPu4?list=PLqYmG7hTraZBgXcetCL9zzd6Cck8S4l4k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

Never Give Up (NGU) は 制御可能な状態に基づく短期的新規性報酬と ランダムネットワーク蒸留を用いた長期的新規性報酬と混合して使用した。
この混合は 長期的新規性が制限されている場合，両方の報酬を掛け合わせることで実現された。
このようにして，短期的な目新しさの報酬の効果は維持される。
だが エージェントが生涯にわたってゲームに慣れてくると，その効果は減衰される可能性がある。
NGU のもう一つの革新的アイデアとしては，NGU は方策族を学習することである。
方策族には，純粋に 利用的 (exploitative) なものから, 高度に探索的 (exploratary) なものまで含まれる。
R2D2 の上に構築することで，アクターは総新規性報酬の重要度の重み付けに基づいて，異なるポリシーでの経験を生成する。
この経験は，方策族内の各重み付けに関して一様に生成される。
<!-- Never Give Up (NGU) used this short-term novelty reward based on controllable states, mixed with a long term novelty reward, using Random Network Distillation.
The mix was achieved by multiplying both rewards, where the long term novelty is bounded.
This way the short-term novelty reward’s effect is preserved, but can be down-modulated as the agent becomes more familiar with the game over its lifetime.
The other core idea of NGU is that it learns a family of policies that range from purely exploitative to highly exploratory. This is achieved by leveraging a distributed setup: by building on top of R2D2, actors produce experience with different policies based on different importance weighting on the total novelty reward. This experience is produced uniformly with respect to each weighting in the family. -->


<center>
<img src="/assets/2017Pathak_fig2.svg" width="77%"><br/>
<div style="text-align: justify;width:88%;background-color:cornsilk">
図 2. 状態 $s_{t}$ のエージェントは，現在の方針 $\pi$ からサンプリングされた行動 $a_{t}$ を実行することで環境と相互作用し，最終的に状態 $s_{t+1}$ になる。
方針 $\pi$ は，環境 $E$ が提供する外発的報酬 ($r^{e}_{t}$) と，我々が提案する内発的好奇心モジュール (ICM) が生成する好奇心に基づく内発的報酬信号 ($r^{i}_{t}$) の和を最適化するように学習されます。
ICM は，状態 $s_{t}$, $s_{t+1}$ を，$a_{t}$ を予測するために学習された特徴量 $\phi(s_{t})$, $\phi(s_{t+1})$ に符号化します (すなわち，逆動力学モデル)。
順方向モデルは，$\phi(s_{t})$ と $a_{t}$ を入力とし $s_{t+1}$ の特徴表現 $\widehat{\phi}(s_{t+1})$ を予測します。
特徴空間での予測誤差は，好奇心に基づく内在的報酬信号として用いられる。
エージェントの行動に影響を与えない，あるいは影響を受けない環境特徴を符号化するインセンティブが $\phi(s_{t})$ にはないため，我々のエージェントの学習された探索戦略は，環境の制御不可能な側面に対してロバストである。
<!-- Figure 2.
The agent in state st interacts with the environment by executing an action $a_{t}$ sampled from its current policy $\pi$ and ends up in the state $s_{t+1}$.
The policy $\pi$ is trained to optimize the sum of the extrinsic reward ($r^{e}_{t}$) provided by the environment $E$ and the curiosity based intrinsic reward signal ($r^{i}_{t}$) generated by our proposed Intrinsic Curiosity Module (ICM).
ICM encodes the states $s_{t}$, $s_{t+1}$ into the features $\phi(s_{t})$, $\phi(s_{t+1})$ that are trained to predict $a_{t}$ (i.e. inverse dynamics model).
The forward model takes as inputs $\phi(s_{t})$ and $a_{t}$ and predicts the feature representation $\widehat{\phi}(s_{t+1})$ of $s_{t+1}$.
The prediction error in the feature space is used as the curiosity based intrinsic reward signal.
As there is no incentive for $\phi(s_{t})$ to encode any environmental features that can not influence or are not influenced by the agent’s actions, the learned exploration strategy of our agent is robust to uncontrollable aspects of the environment.-->
</div>
</center>


<center>
<img src="/assets/2019Jaegle_fig1.jpg" width="77%"><br/>
<div style="text-align: justify;width:88%;background-color:cornsilk">
好奇心は，稀にしか	報酬の得られない環境下での探索を動機づける。
<!-- Novelty can drive exploration in environments with sparse external rewards. -->
上イラストは探索の模式図である。サルは，果物 (報酬) を大きく生茂った枝振りの良い木から探そうとしている。
<!-- An illustration of the benefits of exploration:  -->
報酬にありつくまでに，数多くの枝，すなわち，多くの「状態」を探索せねばならない。
<!-- a monkey is trying to find a piece of fruit (a reward) in a large, densely foliated tree with many branches.  -->
一つ報酬を見つけ出した場合には，他の報酬はすべて捨て去ることをも含意している。
<!-- Typically, the monkey must make many choices and explore many ‘states’ before it receives a single reward.
If the monkey finds novel states rewarding, then it will be encouraged to explore the tree, and it can discover rewarding states that it would otherwise miss. -->
視点の普遍性:
<!-- View invariance: -->
同一状態の異なる見え(リンゴ)は，異なる画像に対応する。
<!-- different views of the same state (e.g. the apple) can correspond to different images.  -->
強化学習が成就するためには，入力画像を対応する状態への関連付けなければならない。
<!-- To effectively drive RL, a system must map images onto their corresponding states. -->
状態普遍性:
<!-- State invariance:  -->
状態が異なれば，新奇性も異なる。果物は通常，高い報酬となり，木々の葉は通常小さく，重なり合って細かな影をなす。
<!-- different states can share features that are indicative of their novelty, for example, reflecting the fact that fruits are usually large and are rarely green while leaves are often small and can take on many shades of green.
A system that can exploit the features shared by different states can drive the monkey to explore states with novel features (e.g. objects with a new size or shade).
-->
source: Jaegle2019+ Fig. 1
</div>
</center>

### メタコントローラ：探索と利用のバランス

Agent57 は 次のような観察に基づいて構築されている。
もしエージェントが，いつ利用 (exploit) するのが良いのか，いつ 探索 (explore) するのが良いのかを学習できるとしたらどうであろうか？
我々は，探索と利用のトレードオフを適応させるメタコントローラの概念と，より長い時間的な信用割当てを必要とするゲームのために調整可能な時間地平線を導入した。
この変更により Agent57 は簡単なゲームでも難しいゲームでも人間レベル以上の成績を得ることができるようになった。
<!-- # Meta-controller: learning to balance exploration with exploitation
Agent57 is built on the following observation: what if an agent can learn when it’s better to exploit, and when it’s better to explore? We introduced the notion of a meta-controller that adapts the exploration-exploitation trade-off, as well as a time horizon that can be adjusted for games requiring longer temporal credit assignment. With this change, Agent57 is able to get the best of both worlds: above human-level performance on both easy games and hard games.-->

具体的には，内在的動機付け方法には 2 つの欠点がある。
* **探索** 多くのゲームは，特にゲームが完全に探索 explore された後に 純粋に 利用可能 exploit な方針に従順である。
これは NGU での 探索的 (explore) 方策によって生成された経験の多くは，エージェントが関連するすべての状態を探索した後に，最終的に無駄になることを意味している。
* **時間の地平線** Skiing や Solaris のように 遠い将来に得られる報酬を評価することは，最終的に良い 利用可能 (搾取的 exploitive) な政策を学ぶために，あるいは良い政策を全く学ぶために重要かもしれない。
同時に，将来の報酬が過度に重み付けされている場合，他の課題は学習に時間がかかり，不安定になる可能性がある。
このトレードオフは，一般的に強化学習では割引率によって制御され，割引率が高いほど長い時間軸からの学習が可能になる。

<!-- Specifically, intrinsic motivation methods have two shortcomings:
* Exploration: Many games are amenable to policies that are purely exploitative, particularly after a game has been fully explored. This implies that much of the experience produced by exploratory policies in Never Give Up will eventually become wasteful after the agent explores all relevant states.
* Time horizon: Some tasks will require long time horizons (e.g. Skiing, Solaris), where valuing rewards that will be earned in the far future might be important for eventually learning a good exploitative policy, or even to learn a good policy at all. At the same time, other tasks may be slow and unstable to learn if future rewards are overly weighted. This trade-off is commonly controlled by the discount factor in reinforcement learning, where a higher discount factor enables learning from longer time horizons. -->

このことから，可変長の時間水平線と新規性の重要性を考慮して，異なる方策で生成される経験の量を制御するオンライン適応メカニズムを使用した。
研究者たちは，異なるハイパーパラメータを持つエージェントの集団を訓練する，勾配降下法によってハイパーパラメータの値を直接学習する，ハイパーパラメータの値を学習するために中央集権型バンディットを使用するなど，複数の方法でこれに取り組むことを試みてきた。

我々はバンディットアルゴリズムを使用して，経験を生成するためにエージェントが使用すべき方策を選択した。
具体的には，各アクターに対してスライドウィンドウ型 UCB バンディットを訓練し，そのポリシーが持つべき探索への選好度と時間軸を選択した。

<!-- This motivated the use of an online adaptation mechanism that controls the amount of experience produced with different policies, with a variable-length time horizon and importance attributed to novelty. Researchers have tried tackling this with multiple methods, including training a population of agents with different hyperparameter values, directly learning the values of the hyperparameters by gradient descent, or using a centralized bandit to learn the value of hyperparameters.

We used a bandit algorithm to select which policy our agent should use to generate experience. Specifically, we trained a sliding-window UCB bandit for each actor to select the degree of preference for exploration and time horizon its policy should have. -->

<div class="figcenter">

[NGU によるスキー](https://youtu.be/0_67wNXyOcI?list=PLqYmG7hTraZDp9fZRWVMeGwUupEl7uN8S)<br/>
<iframe width="560" height="323" src="https://www.youtube.com/embed/0_67wNXyOcI?list=PLqYmG7hTraZDp9fZRWVMeGwUupEl7uN8S" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Agent57: すべてをまとめる

Agetn57 を達成するために，我々は以前に開発した探索エージェント Never Give Up をメタコントローラと組み合わせた。
このエージェントは 方策族を探索して学習するために，長期的，および短期的な，内在的動機を混合したものを計算する。
メタコントローラは，エージェントの各アクターが，新しい状態を探索するか，すでに知られていることを利用するかの違いと同様に，短期と長期とのパフォーマンスの間で異なるトレードオフを選択することを可能にする (図4)。
強化学習はフィードバックループを形成する。
選択された行為が訓練データを決定し，メタコントローラはまたエージェントがどのようなデータから学習するかを決定する。

<!-- # Agent57: putting it all together
To achieve Agent57, we combined our previous exploration agent, Never Give Up, with a meta-controller. This agent computes a mixture of long and short term intrinsic motivation to explore and learn a family of policies, where the choice of policy is selected by the meta-controller. The meta-controller allows each actor of the agent to choose a different trade-off between near vs. long term performance, as well as exploring new states vs. exploiting what’s already known (Figure 4). Reinforcement learning is a feedback loop: the actions chosen determine the training data. Therefore, the meta-controller also determines what data the agent learns from. -->

<center>
<img src="/assets/2020deepmind_agent57_fig4.svg" style="width:66%"><br/>
<!-- <img src="https://kstatic.googleusercontent.com/files/a9754efe518fdef7e39f50baeffd0f8348f21d0fd3c919f6ca749799321a9b514134f3a2a6f19d33b9fcf254ccf05847f91da511567e44ffc375a6ccb75b069c"> -->
</center>

<!--
## 結論と今後の展開
Agent57 では Atari57 ベンチマークのすべての課題において人間以上の性能を持つ，より一般的な知的エージェントを構築することに成功した。
このエージェントは，以前のエージェント Never Give Up を ベースに構築されており， 適応的なメタコントローラを実体化している。
広範囲の課題は当然，これらのトレードオフの両方の異なる選択を必要とする。そのためメタコントローラはそのような選択を動的に適応させる方法を提供している。

Agent57 は計算量の増加に伴ってスケーリングすることができた。
これにより Agent57 は強力な一般性能を達成することが可能となった。
多くの計算量と時間を必要とするが，データ効率は確実に改善することができる。
さらに，この Agent57は Atari の 57 ゲームセット全てで 5 パーセンタイル のより良い性能を示した。
これは，データ効率の面だけでなく，一般的な性能の面でも，決して Atari の研究の終わりを意味するものではない。

* 第一に，パーセンタイル間の性能を分析することで，一般的なアルゴリズムがどのようなものであるかについて新たな洞察を得ることができる。
* 第一に，パーセンタイル間の性能を分析することで，一般的な アルゴリズムがどのようなものであるかを知ることができる。
* 第二に，現在のアルゴリズムはすべて，いくつかのゲームで最適なパフォーマンスを達成するには程遠いということです。

そのためには Agent57 が探索，計画，および信用割り当てのために使用する表現を強化することが，使用するための重要な改善点になるかもしれない。 -->





## APE-X
@2018Horgan_APE-X

## TRPO (Trust Region Policy Optimization)
@2015Schulman_trpo

## IMPALA
<https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/>


#### TPRO と PPO
- source: <https://blog.syundo.org/post/20171204-reinforcement-learning-natural-policy-gradient-trpo-ppo/>
- さらに，上のトップ: <https://blog.syundo.org/post/20180115-reinforcement-learning/>

##### TRPO

最適化計算における更新ステップの計算に KL ダイバージェンスによる制約を加えたものが TRPO (Trust Region Policy Optimization) である。
この方法は，を KL ダイバージェンスで拘束しているため，近似的には自然勾配法と同様の手法となる。
TRPO は方策勾配法に限らず，モデルなし学習においても利用することができるが，以下では方策勾配法と組み合わせることを前提に述べる。

さて，$\theta$ でパラメタライズされた方策  $\pi_\theta(a\vert s)$ がある場合，方策勾配が

$$
\hat{g} = \mathbb{E}_\pi\left[ \nabla_\theta \log\pi_\theta(a\vert s)A^{\pi}(s,a)\right]
$$

であった。これは，以下の値 $L_\theta$ の微分値である。

$$
L_\theta(\theta)=\mathbb{E}_\pi\left[\log \pi_\theta(a\vert s)A^{\pi}(s,a)\right]
$$

このとき，更新のステップを制限するために，以下のようにKLダイバージェンスで制約を課して最大化を行う。

$$
\text{maximize}_{x} L_{\theta_{\text{old}}}(\theta)
$$

$$
\text{subject to } D_{KL}(\theta_{\text{old}},\theta)\le\delta
$$

ここで， $\theta_{\text{old}}$ は $\pi_\theta$ におけるパラメタ $\theta$ の前の値である。
また，$D_{KL}$ は確率分布 $\pi_\theta$ と $\pi_{\theta_{\text{old}}}$ の間の KL ダイバージェンスであり， $D_{KL}\max(\theta_{\text{old}},\theta)$ は任意のパラメタの組み合わせに対して，KLダイバージェンスを計算したときの最大値を表す。

実用的には組み合わせが膨大になり，最大値を求めるのは難しいため，制約はヒューリスティックに以下のように平均値で代用する。
$$
\text{maximize}_x L_{\theta_{\text{old}}}(\theta)$$
$$

$$
\text{subject to} D_{KL}^{\max}(\theta_{\text{old}},\theta) \le \delta
$$

ここで， $\bar{D}_{KL}(\theta_{\text{old}},\theta)=\mathbb{E}_{s\sim p}\left[D_{KL}(\pi_\theta(\cdot\vert s),\pi_{\theta_2}(\cdot\vert s)\right]$ である。

ただし，制約において問題を解くのは簡単ではないので，以下のようにソフト制約を使う形に書き下す。

$$
\text{maximize}_x \mathbb{E}_\pi\left[L_{\theta_{\text{old}}}(\theta)-\beta\bar{D}_{KL}(\theta_{\text{old}},\theta)\right]
$$

以上が，TRPO の概要である。


### 基礎概念<!--キーコンセプト ### Key Concepts-->

* エージェントは **環境** の中で行動する。
環境がある行動に対してどのように反応するかは， 我々が知っているかどうかわからない **モデル** によって定義されます。
エージェントは， 環境の多くの **状態**（$s\in\mathcal{S}$）のうちの 1 つに留まることができ， 多くの **行動**（$a\in\mathcal{A}$）のうちの 1 つを選択して， ある状態から別の状態に切り替えることができます。
エージェントがどの状態に到達するかは， 状態間の遷移確率 ($P$) によって決定される。
行動を起こすと，環境はフィードバックとして **報酬** ($r\in\mathcal{R}$) を与えます。

The agent is acting in an **environment**. How the environment reacts to certain actions is defined by a **model** which we may or may not know.
The agent can stay in one of many **states** ($s \in \mathcal{S}$) of the environment, and choose to take one of many **actions** ($a \in \mathcal{A}$) to switch from one state to another.
Which state the agent will arrive in is decided by transition probabilities between states ($P$).
Once an action is taken, the environment delivers a **reward** ($r \in \mathcal{R}$) as feedback.

モデルは報酬関数と遷移確率を定義しています。
モデルがどのように動作するかを知っている場合と知らない場合があり，これにより 2 つの状況が区別されます。

- **モデルベース**：完全な情報で計画を立てて学習を行う。
環境が完全にわかっている場合 [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP)によって最適解を求めることができます。
- **モデルフリー**：不完全な情報での学習；アルゴリズムの一部として明示的にモデルを学習しようとする。
以下の内容のほとんどは，モデルがわからない場合のシナリオに対応しています。

The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP).
Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL.
This is not the focus of this post though.
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm.
Most of the following content serves the scenarios when the model is unknown.

エージェントの **方針(ポリシー)**  $\pi(s)$ は，**総報酬** を最大化することを目的として，ある状態で取るべき最適な行動のガイドラインを提供する。
各状態には，その状態で対応するポリシーを実行することで得られる将来の報酬の期待値を予測する **価値** 関数$V(s)$ が関連付けられています。
言い換えれば，価値関数は，ある状態がどれだけ良いかを定量化します。
強化学習で学習しようとするのは，方策関数と価値関数の両方です。
<!-- The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **the goal to maximize the total rewards**.
Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy.
In other words, the value function quantifies how good a state is.
Both policy and value functions are what we try to learn in reinforcement learning.



<!--
##### PPO

PPO (Proximal Policy Optimization) は方策の目標値をクリッピングすることで，おおまかに方策の更新を制約する方法である。
TRPO では KL ダイバージェンスを制約として利用していたが，PPO では，目的関数を以下の $L^{\text{clip}}$ として，勾配を求める。

$$
L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min(r_t(\theta)\hat{A}_t,\text{clip}(r(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)\right]
$$

ここで，$r_t(\theta)$ は確率の比率であり，

$$
r_t(\theta) = \frac{\pi_\theta(s_t,a_t)}{\pi_{\theta_{\text{old}}}(s_t,a_t)}
$$

である。また，$\text{clip}(r(\theta), 1−\epsilon, 1+\epsilon)$ は $r(\theta)$ が $1−\epsilon} あるいは $1+\epsilon$ を超過しないように制限する関数である。
$\text{clip}(r(\theta), 1−\epsilon , 1+\epsilon)^At$ の グラフと，$L^{\text{CLIP}}$ を以下に示す(John Schulmanらより引用)。
-->

