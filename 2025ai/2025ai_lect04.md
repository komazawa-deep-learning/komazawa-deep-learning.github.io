---
title: 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">
<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 09/May/2025<br/>
Appache 2.0 license<br/><br/>
</div>

# 本日のメニュー

1. 数式の読み方
2. 教師あり学習と教師なし学習
3. ロジスティック回帰とサボーとベクターマシン (SVM)
4. Eigneface and Fisherface

# 分類, 回帰, 次元圧縮, 視覚化

<center>
<img src="/2025assets/sklearn_map_sub1.svg" style="width:69%"><br/>
出典: <http://scikit-learn.org/stable/tutorial/machine_learning_map/> 
</center>


# 統計学と機械学習

## Breiman (2001) の二つの文化

データから結論を得るための統計モデリングの使用には，2 つの文化がある。
* データが所定の確率的データモデルによって生成されたものであると仮定
* アルゴリズムモデルを使用し，データ機構を未知とする

統計学はデータから始まる。
データは，入力変数 x (独立変数) のベクトルが片側に入り，反対側に応答変数 y が出てくるブラックボックスで生成されて
いると考える。
ブラックボックスの中には，予測変数と応答変数を関連付ける機能が備わっており，そのイメージは次のようなものである：
<!-- Statistics starts with data.
Think of the data as being generated bya black box in which a vector of input variables x (independent variables) 
go in one side, and on the other side the response variables y come out.
Inside the black box, nature functions to associate the predictor variables with the response variables, so the pi
cture is like this: -->

<div class="figure figcenter">
<img src="/2025assets/2001Breiman_Two_Cultures_fig1.svg" style="width:49%;"><br/>
<!-- <img src="figures/2001Breiman_Two_Cultures_fig2.svg"><br/>
<img src="figures/2001Breiman_Two_Cultures_fig3.svg"> -->
</div>

データの分析には 2 つのゴールがある：
<!-- There are two goals in analyzing the data:-->

* 予測： 将来の入力変数に対する反応を予測できるようになること；
* 情報：自然が応答変数を入力変数にどのように関連付けるかについての情報を抽出すること。

<!-- * Prediction: To be able to predict what the responses are going to be to future input variables;
* Information: To extract some information about how nature is associating the response variables to the input var
iables. -->

これらの目標に対して，2 つの異なるアプローチがある：
<!--There are two different approaches toward these goals: -->

#### データモデル文化 The Data Modeling Culture

この文化における分析は，ブラックボックス内部の確率的データモデルを仮定することから始まる。
例えば，一般的なデータモデルとして，データは以下のような独立した抽選によって生成されるというものである。
<!-- The analysis in this culture starts with assuming a stochastic data model for the inside of the black box.
For example, a common data model is that data are generated by independent draws from -->
$$
\text{response variables} = f(\text{predictor variables}, \text{random noise, parameters})
$$

データからパラメータの値を推定し、そのモデルを情報収集や予測に利用する。
このように，ブラックボックスはこのように埋められていく：
<!-- The values of the parameters are estimated from the data and the model then used for information and/or predi
ction.
Thus the black box is filled in like this: -->
<div class="figcenter">
<img src="/2025assets/2001Breiman_Two_Cultures_fig2.svg" style="width:49%;">
<div class="figcaption">

* **モデルの検証方法**: 適合度と残差による Yes-No 検定
* **この文化の推定割合**: 全統計学者の 98 ％

<!-- * **Model validation**. Yes–no using goodness-of-fit tests and residual examination.
* **Estimated culture population**. 98% of all statisticians. -->
</div></div>


#### アルゴリズムモデル文化 The Algorithmic Modeling Culture

この文化の分析では，暗箱の中は複雑で未知であると考える。
彼らのアプローチは，関数 $f(x)$ つまり $x$ を操作して応答 $y$ を予測するアルゴリズムを見つけることである。
彼らの暗箱は次のようなものである：
<!-- The analysis in this culture considers the inside of the box complex and unknown.
Their approach is to find a function f(x) — an algorithm that operates on x to predict the responses y.
Their black box looks like this: -->
<div class="figcenter">
<img src="/2025assets/2001Breiman_Two_Cultures_fig3.svg" style="width:49%;"><br/>
<div class="figcaption">

* **モデルの検証方法**： 予測精度によって測定
* **この文化の推定割合**： 統計学者の 2 ％，他分野の多数

<!-- * **Model validation**: Measured bypredictive accuracy.
* **Estimated culture population**: 2% of statisticians, many in other fields. -->
</div></div>

本論文では，統計学の世界ではデータモデルに焦点が当てられていることを論じる：
<!-- In this paper I will argue that the focus in the statistical community on data models has:-->

* 無関係な理論や疑わしい科学的結論を導き出し，
* 統計学者がより適切なアルゴリズム・モデルを使用しないようにし，
* 統計学者がエキサイティングな新しい問題に取り組むことを妨げている

<!-- * Led to irrelevant theory and questionable scientific conclusions;
* Kept statisticians from using more suitable algorithmic models;
* Prevented statisticians from working on exciting new problems; -->


# 独立変数 と 従属変数

- 独立変数 原因
- 従属変数 結果

$$
y = f(x) 
$$

$$
y = a x  + b
$$

# 例： 雇用の未来 Frey and Osborne (2013) 

<center>

<img src="/2025assets/2013Frey_EmploymentFig01.png" style="width:43%">
<img src="/2025assets/2013Frey_EmploymentFig02.png" style="width:43%">

$$
y = f(x) = \frac{1}{1+\exp(-x)}
$$

</center>


---

1. 0.0028 29-1125 Recreational Therapists
2. 0.003 49-1011 First-Line Supervisors of Mechanics, Installers, and Repairers
3. 0.003 11-9161 Emergency Management Directors
4. 0.0031 21-1023 Mental Health and Substance Abuse Social Workers
5. 0.0033 29-1181 Audiologists
6. 0.0035 29-1122 Occupational Therapists
7. 0.0035 29-2091 Orthotists and Prosthetists
8. 0.0035 21-1022 Healthcare Social Workers
9. 0.0036 29-1022 Oral and Maxillofacial Surgeons
10. 0.0036 33-1021 First-Line Supervisors of Fire Fighting and Prevention Workers

693. 0.99 43-4141 New Accounts Clerks
694. 0.99 51-9151 Photographic Process Workers and Processing Machine Operators
695. 0.99 13-2082 Tax Preparers
696. 0.99 43-5011 Cargo and Freight Agents
697. 0.99 49-9064 Watch Repairers
698. 0.99 1 13-2053 Insurance Underwriters
699. 0.99 15-2091 Mathematical Technicians
700. 0.99 51-6051 Sewers, Hand
701. 0.99 23-2093 Title Examiners, Abstractors, and Searchers
702. 0.99 41-9041 Telemarketers


## コンピュータ化しない職業

レクリエーションセラピスト, 整備士・施工士・修理士一級監督士, 危機管理担当役員, 精神保健と物質乱用の
ソーシャルワーカー, 聴覚士, 作業療法士, 義肢装具士と義肢装具士, 医療ソーシャルワーカー
口腔外科・顎顔面外科, 消防職員の第一線監督者

## コンピュータ化する職業

新規口座作成の事務員, 写真加工作業員及び加工機オペレーター, 税務申告書作成者, 貨物および貨物代理店, 
時計修理業者, 保険引受人, 数学技術者, 下水道修理師, 職業診断師,  テレマーケティング


# 統計的仮説検定と機械学習

From a traditional data analytics standpoint, the answer to the above question is simple.

- **Machine Learning** is an algorithm that can learn from data without relying on rules-based programming.
- **Statistical modeling** is a formalization of relationships between variables in the data in the form of mathematical equations.

- **機械学習**: ルールベースのプログラミングではなくデータから学習するアルゴリズム
- **統計的モデリング**: 数学的方程式を用いてデータ内の変数間の関係を定式化

出典 [https://www.kdnuggets.com/2016/11/machine-learning-vs-statistics.html](https://www.kdnuggets.com/2016/11/machine-learning-vs-statistics.html)


|統計学          |    機械学習|
|:--------------|:----------|
|推定            |    学習|
|分類器          |    仮説|
|データ点        |    例，事例|
|回帰           |    教師あり学習|
|分類           |    教師あり学習|
|共変量          |    特徴|
|反応            |   ラベル|

Table: 統計学と機械学習.


- 出典 [https://normaldeviate.wordpress.com/2012/06/12/statistics-versus-machine-learning-5-2/](https://normaldeviate.wordpress.com/2012/06/12/statistics-versus-machine-learning-5-2/)


# まとめ

- 機械学習とデータサイエンスの関係について説明しました。
- いずれも，原因と結果の関係を探る方法

# クイズ

- 教師あり学習と呼ばれる機械学習手法には何が挙げられるでしょうか。
- また，教師なし学習はどうでしょうか。


# 入力信号 (input signals) と教師信号 (teacher signals)

入力データ $x$ が与えられた場合，望ましい出力 $y$ を得るために訓練データを用いて $y=f(x)$ となる関数 $f$ を見つけることを学習と言う。


# 回帰 (regression) と分類 (classification)

出力 (予測すべき値) が連続値の場合を回帰と呼ぶ。
一方，出力が離散値の場合を分類と呼ぶ。

例えば，気温や時刻を予測する場合は連続値を予測することになるので回帰である。
一方，人物を予測するのであれば分類となる。

唯一の例外は，ロジスティック回帰 (logisitic regression) である。
ロジスティック回帰の場合，分類に用いられる確率を与える量が問題となる。
あるカテゴリに分類される確率を 0 から 1 の間の確率として予測することになる。

したがってロジスティック回帰とは，教師信号で与えられる値に近づけるように関数 $f$ を訓練することとなる。
この場合，教師信号が $1$ であれば，そのクラス (あるいはカテゴリ) に属することを意味し，逆に教師信号が $0$ であれば，そのクラスに属さないことを学習することとなる。

顔認識を例に取れば，男女を識別するのであれば, 男である確率と女である確率を予測することとなる。
スマートフォンにおける顔認証では，スマートフォンに登録された人物であるか否かを識別することとなる。
この場合，登録済の人物の顔画像であれば 1 そうでなければ 0 を出力すると考えても良い。

この場合，2 $\times$ 2 の表を考えることができる。
以下の表を参照せよ。

|      |真の値 1| 真の値 0 |
|-----:|:-----:|:-------:|
| 予測 1 | 正解  | 虚報 false alarm | 
|     0 | ミス  | correct rejection |

どのような入力値に対しても 1 と答えることにすれば, 正解は増えるが,虚報も増えてしまう。
反対にどのような入力データに対しても 0 と答えることにすれば, correct rejection は増えるがミスも増える。

性能の良い判別器では，正解と correct rejection が多く,ミスと虚報が少ないことが望まれる。

<center>
<img src="/2025assets/04-02classification_2norm.svg" style="width:33%"><br/>
</center>

|   | 本当は正しい  | 本当は誤り |
|:---|:-----|:---|
|:正しいと判断:| ヒット (TP) ピンク|  虚報 (FP) 青| 縦線の左側
|:誤りと判断:| ミス (FN) 赤| 正しい棄却 (TN) 水色| 縦線の右側
|            |  赤＋ピンクの分布 | 青＋水色の分布  | 


<center>

<img src="/2025assets/04-02classification_2norm.svg" style="width:33%"><br/>
</center>

- 適合率 (precision): $\displaystyle\frac{\text{ヒット(ピンク)}}{\text{ヒット(ピンク)}+\text{虚報(青)}}$ 
- 再現率 (recall): $\displaystyle\frac{\text{ヒット(ピンク)}}{\text{ヒット(ピンク)}+\text{ミス(赤)}}$ 
- 正解率 (accuracy): $\displaystyle\frac{\text{ヒット(ピンク)}+\text{正しい棄却(水色)}}{\text{ヒット(ピンク)}+\text{虚報(青)}+\text{ミス(赤)}+\text{正しい棄却(水色)}}$
- F 値: 適合率と再現率の調和平均 $\displaystyle\frac{2}{\text{正確度}^{-1}+\text{再現率}^{-1}}=2\cdot\frac{\text{正確度}\cdot\text{再現率}}{\text{正確度}+\text{再現率}}$


多値分類の場合，例えば 40 名の人物画像を考える。

ロジット比 (確率比) とは，事象 x の起こる確率と x が起こらない確率との比 $\displaystyle\frac{P(x)}{P(\neg x)}=\frac{P(x)}{1-P(x)}$ のことを言う。
ロジット比の対数を x と置くと，$\displaystyle \log\left(\frac{P(x)}{1−P(x)}\right)=x $ となる。
この式を変形して $\displaystyle P(x)=\frac{1}{1+e^{-x}}$ となる。

この値は，任意の実数 $x\in[-\infty,\infty]\equiv\mathbb{R}$ に対して 0 から 1 の間の値を与える関数となる。

### Galileo Galilei (1623) The Assayer (translation by Stillman Drake) より

<div>

Sarsi: 哲学をする際には有名な作家の意見に基づかなければならないという確固たる信念があるように思える。
もしかしたら彼は，哲学とは『イーリアス』や『オーランド・フュリオーソ』のような，ある作家のフィクションであり，そこに書かれていることが真実かどうかが最も重要なことだと思っているのかもしれない。<br/>
Sarsi: 問題はそうではない。
哲学は宇宙という壮大な書物に書かれている。
しかし，まず言語を理解し，その文字を読むことを学ばなければ，その書物を理解することはできない。
<font style="color:navy;font-size:20pt;font-weight:600">この本は数学の言語で書かれており</font>，その文字は三角形や円などの幾何学的図形である，数学がなければ，人は暗い迷宮の中を彷徨うことになる。
</div>

<!-- > In Sarsi I seem to discern the firm belief that in philosophizing one must support oneself upon the opinion of some celebrated author, as if our minds ought to remain completely sterile and barren unless wedded to the reasoning of some other person.
Possibly he thinks that philosophy is a book of fiction by some writer, like the Iliad or Orlando Furioso, productions in which the least important thing is whether what is written there is true.
Well, Sarsi, that is not how matters stand.
Philosophy is written in this grand book, the universe, which stands continually open to our gaze.
But the book cannot be understood unless one first learns to comprehend the language and read the letters in which it is composed.
It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures without which it is humanly impossible to understand a single word of it; without these, one wanders about in a dark labyrinth. -->

上の文章の哲学とは，自然哲学，すなわち，現代では科学のことであろう.

# 実習

* [オリベッティ顔データベースを用いた機械学習実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_0425olivetti_face_detection.ipynb){:target="_blank"}
* [課題提出用フォルダ](https://drive.google.com/drive/u/3/folders/1QrIvRv5WKvx9psO9bUCWaPlqUnWLk52V){:target="_blank"}

---

[今際の際に黒板に書いてあったファインマンの言葉，カリフォルニア工科大学アーカイブ写真](http://archives.caltech.edu/pictures/1.10-29.jpg){:target="_blank"}

### 概念の整理

<div class="figcenter">
<img src="/2025assets/2018Kriegeskorte_fig2_new.svg" style="width:44%;">
<img src='/2025assets/2017Goodfelllow_Fig1_4rev.svg' width="44%"><br/>
左: Kriegeskorte and Douglas (2018) Fig.2 を改変<br/>
右: Goodfellow et al. (2017) Fig.1 を改変
<!-- <img src='/assets/2017Goodfellow_Fig1_4ja.svg' width="33%"><br/> -->
<!-- Goodfellow et al. (2017) Fig.1 を改変 -->
</div>

# 関連用語

- 機械学習 Machine Learning
- データサイエンス Data Science
- AI
- 統計学 Statistics
- 数学
- 情報理論，通信理論


## 数式の読み方と表記

* 関数:
    * $f(x)$: エフ オブ エックス  (この「オブ」を読むことが重要。なぜなら「エフエックス」と読んでしまうと，$f\times x$ の意味だと誤解するから)
    * $P(x)$: ピー オブ エックス (ピーを使う場合には，確率であることが多い)
* $\in$: イン 「含む」ことを意味する ($x\in D$)
* $\partial$: 偏微分の意。統一した読み方は無いが，「パーシャル」とは「ディファレンシャル」と読む人もいる。偏微分については取り上げない。
* $\top$: 行列の転置 (transpose) を表す。
* $\sim$: 「従う」を意味する
* $P(A\|B)$: 「ピー オブ エー ギブン ビー」と読む。縦棒 ($\|$) を条件付きであることを意味する。
    したがって，行頭の数式は，B が与えられた条件で A が起こる確率を意味する
* $\Delta$: 「デルタ」と読むが，差分を表す場合が多い。
* ギリシャ文字: $\alpha,\beta,\gamma,...,\mu,\sigma,\omega$ 統計学の伝統に従って，よく分からないものに使われる場合が多い。よく分からないとは，データ平均を $m$ (または $\bar{X}$) で表し，母集団平均を対応するギリシャ文字 $\mu$ で表す。
同様に，データ分散を $s^2$ で表し，対応する母集団分散を $\sigma^2$ で表す，などである。同じことであるが，$\pi$ は円周率を表すこともあるが，場合によっては，母集団確率を表すこともある。

* ベクトル: 太字の小文字 $\mathbf{x}$
* 行列: 太字の大文字 $\mathbf{W}$



## 機械学習

<div class="figcenter">
<img src="/assets/sklearn_map.svg" style="width:88%"><br/>
scikit-learn の カンペ (cheat sheet) を改変
</div>

- 重回帰，逆行列，線形代数
- 主成分分析，固有値，変分法，
- tSNE
- ロジスティック回帰
- サポートベクトルマシン SVM
<!-- - [番組 nothotdog について](https://komazawa-deep-learning.github.io/nothotdog/){:target="_blannk"}-->
<!-- [nothotdog 体感デモ](https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb)-->


<!-- - [初めての画像認識 <img src="/assets/colab_icon.svg">](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_ResNet50_demo.ipynb){:target="_blank"}-->
<!-- - [機械学習の超簡単デモ 伏線回収バージョン<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0507_3mnists_demo.ipynb){:target="_blank"} -->

<!-- ### 3 つのデータセット: MNIST, Fashion MNIST, KMNIST

- 機械学習分野で頻用されるデータセットとして，手書き数字認識データである MNIST があります。
- MNIST は FAIR (フェイスブック人工知能研究所) 現所長 の Yan LeCun によって作成されました。
NIST とは，アメリカ合衆国版の JIS です。すなわち，標準化機関の手書き数字認識用データセットを 修正した (modified) という意味から MNIST と呼ばれます。
- MNIST は データ数が ７万で，訓練データ数 6 万，テストデータ １ 万からなります。
データは，縦横それぞれ 28 画素からなっています。コンピュータで扱う際に，コンピュータにとってキリの良い 32 画素ではなく，
周囲を切り取ったために，28 画素になっています。
- Fasshion MNIST は， MNIST と同じ画像形式で，ファッション画像，具体的には 10 種類のアパレル画像データです。
- kmnist は日本語のくずし字データセットです。形式は MNIST, Fashion MNIST と同じです。 -->



## 数式の読み方と表記

* 関数:
    * $f(x)$: エフ オブ エックス  (この「オブ」を読むことが重要。なぜなら「エフエックス」と読んでしまうと，$f\times x$ の意味だと誤解するから)
    * $P(x)$: ピー オブ エックス (ピーを使う場合には，確率であることが多い)
* $\in$: イン 「含む」ことを意味する ($x\in D$)
* $\partial$: 偏微分の意。統一した読み方は無いが，「パーシャル」とは「ディファレンシャル」と読む人もいる。偏微分については取り上げない。
* $\top$: 行列の転置 (transpose) を表す。
* $\sim$: 「従う」を意味する
* $P(A\|B)$: 「ピー オブ エー ギブン ビー」と読む。縦棒 ($\|$) を条件付きであることを意味する。
    したがって，行頭の数式は，B が与えられた条件で A が起こる確率を意味する
* $\Delta$: 「デルタ」と読むが，差分を表す場合が多い。
* ギリシャ文字: $\alpha,\beta,\gamma,...,\mu,\sigma,\omega$ 統計学の伝統に従って，よく分からないものに使われる場合が多い。よく分からないとは，データ平均を $m$ (または $\bar{X}$) で表し，母集団平均を対応するギリシャ文字 $\mu$ で表す。
同様に，データ分散を $s^2$ で表し，対応する母集団分散を $\sigma^2$ で表す，などである。同じことであるが，$\pi$ は円周率を表すこともあるが，場合によっては，母集団確率を表すこともある。

* ベクトル: 太字の小文字 $\mathbf{x}$
* 行列: 太字の大文字 $\mathbf{W}$

`to`:$\to$, `mapsto`:$\mapsto$, `rightarrow`:$\rightarrow$ , `Rightarrow`:$\Rightarrow$

# 人工知能 AI とは何か

- 「人工知能の基礎」（小林 一郎）
    - 人の知能，つまり，人が行なう知的作業は，推論，記憶，認識，理解，学習，創造といった現実世界に適応するための能力を指す．
人工の「知能」とは，人の「知能」のある部分を機械に行わせることによって創られる．
- デジタル大辞泉 《artificial intelligence》コンピューターで，記憶・推論・判断・学習など，人間の知的機能を代行できるようにモデル化されたソフトウエア・システム．AI．

シャピロ (Shapiro, Stuart C., 1992) は次の 3 つの分野だと書いている。

1. 計算論的心理学 Computational Psychology:  __人間の知的活動を理解するために人間のように振る舞うコンピュータプログラムを作ること__
1. 計算論的哲学 Computational Philosophy:  __人間レベルの知的活動を計算論的に理解すること。計算論的理解=コンピュータ上に実装可能なモデル__
1. 計算機科学 Advanced Computer Science:  __コンピュータ科学の拡張，発展。現在のコンピュータはプログラムされたことしか実行できないが，人間はプログラムされていなくても勝手に振る舞う。__

* Shapiro, Stuart C. (1992), "Artificial Intelligence", in Stuart C. Shapiro (ed.), Encyclopedia of Artificial Intelligence, 2nd edition (New York: John Wiley & Sons)


# 時代背景

- 18世紀 第 1 次産業革命: <span style="color:Blue">蒸気機関，都市部に大規模工場が出現</span>
- 20世紀初頭 第 2 次産業革命: <span style="color:Blue">電気，オートメーション化，自動車，飛行機，電車による移動手段の変化</span>
- 20世紀後半 第 3 次産業革命: <span style="color:Blue">情報化，コンピュータ化，グローバル化</span>
- 21世紀から 第 4 次産業革命: <span style="color:Blue">AI 人間の能力を越える機械</span>

<!--from [http://bootcamp.lif.univ-mrs.fr:8080/mainpage](http://bootcamp.lif.univ-mrs.fr:8080/mainpage)-->

<center>
<img src='/assets/2009Gray_4th_paradigm.svg' style='width:66%'><br>
Gray (2009) The 4th paradigm より
</center>

# サポートベクターマシン (SVM: Support Vector Machines)

サポートベクターマシン（SVM）では、特に非線形分離可能なデータを取り扱う場合や、ある程度の誤分類が許容される場合に、誤分類を許容するためにスラック変数が導入される。
これらは、各データ点の誤分類の程度を測定し、コストパラメータ（C）によって、マージンの最大化と分類誤りの最小化とのトレードオフを制御する。
<!-- In Support Vector Machines (SVM), slack variables are introduced to allow for misclassifications, especially when dealing with non-linearly separable data or when some degree of error is acceptable. 
They measure the extent of misclassification for each data point, with the cost parameter (C) controlling the trade-off between maximizing the margin and minimizing classification errors.  -->

説明：
* **ハードマージン SVM**：<br/>
ハードマージン SVM は、誤分類のない、データ点を完全に分離する超平面を見つけることを目的としている。
しかし、このアプローチは、ノイズの多いデータや非線形分離可能なデータを扱う場合、過度に厳格になる可能性がある。
* **ソフトマージン SVM**：<br/>
ハードマージン SVM の制限を解決するため、ソフトマージン SVM はスラック変数 (グサイ $\xi$) を導入する。
これらの変数は、一部のデータ点の誤分類を許容し、ハードマージンの場合よりも厳格ではない「マージン」を効果的に作成する。
* **コストパラメータ（C）**：<br/>
コストパラメータ（C）は、マージンの最大化と誤分類エラーの最小化とのトレードオフのバランスを取る上で重要な役割を果たす。
C の値が大きいと誤分類が厳しく罰せられ、過学習を引き起こす可能性がある。
一方、C の値が小さいと誤分類を許容するが、より頑健なモデルとなる可能性がある。
* **スラック変数 (グサイ $\xi$)**<br/>
各データ点には対応するスラック変数が存在し、その点の誤分類の度合いを表す。
値が 0 は正しく分類された点を示し、0 より大きい値は誤分類された点を示し、値が大きいほど誤分類の度合いが大きくなる。
本質的に、スラック変数は SVM モデルに柔軟性を提供し、一部の誤分類を許容することで、より複雑なデータセットに対応し、汎化性能を向上させる。

<!-- Explanation: 
* Hard Margin SVM:<br/>
A hard margin SVM aims to find a hyperplane that perfectly separates data points, with no misclassifications. 
However, this approach can be overly strict, especially when dealing with noisy or non-linearly separable data. 
* Soft Margin SVM:<br/>
To address the limitations of hard margin SVM, soft margin SVM introduces slack variables (ξ). $\zeta$
These variables allow for some data points to be misclassified, effectively creating a "margin" that is not as strict as in the hard margin case. 
* Cost Parameter (C):<br/>
The cost parameter (C) plays a crucial role in balancing the trade-off between maximizing the margin and minimizing misclassification errors. 
A high value of C penalizes misclassifications heavily, potentially leading to overfitting, while a lower value of C allows for more misclassifications but can result in a more robust model. 
* Slack Variable (ξ):$\zeta$<br/>
Each data point has a corresponding slack variable, which represents the degree of misclassification for that point. 
A value of 0 indicates a correctly classified point, while a value greater than 0 indicates a misclassified point, with higher values indicating a greater degree of misclassification. 
In essence, slack variables provide flexibility to the SVM model, allowing it to handle more complex datasets and improve generalization performance by tolerating some errors in the training -->


[Soft Margin SVM: Exploring Slack Variables, the ‘C’ Parameter, and Flexibility](https://pub.aimind.so/soft-margin-svm-exploring-slack-variables-the-c-parameter-and-flexibility-1555f4834ecc){:target="_blank"}



## ありえないほど (unreasonable) 有能な (effectiveness) 数学

<!-- ガリレイは，宇宙は数学の言葉で書かれていると言いました。以来，数学は神の摂理を知るための道具であり続けています。 -->
<!-- 数学的知識の詳細は不要だが，その精神は理解しておく必要がある。 -->

- 万物は数なり --- ピタゴラス
- 宇宙は数学語で書かれている。数学なしでは迷宮を理解できない --- ガリレイ
- 世界について最も理解不能なことは，それが理解可能なことだ --- アインシュタイン<!--“The most incomprehensible thing about the world is that it is at all comprehensible.”- Albert Einstein, US (German-born) physicist (1879 - 1955)-->
- 微積分は神がこの宇宙を作ったときの言葉である --- ファインマン<!--Calculus was the language that God had used when creating this universe. -->
- 作れなければ理解できたと言えない --- ファインマン

<!-- - All things are number. --- Pythagras
- (The universe) is written in mathematical language,%%and its characters are triangles, circles and other geometric figures, ... without which it is impossible to humanly understand a word; without these one is wandering in a dark labyrinth. --- Galileo Galilei
- What I cannot create, I do not understand. --- [Richard Feynman](https://en.wikiquote.org/wiki/Richard_Feynman)-->


- 若者よ，数学は理解するものではない，ただ慣れるだけだ --- フォン・ノイマン
- 科学は説明しないし，解釈もしない。ただモデルを作るだけである。この場合モデルとは観察された現象を説明する数学(的構成物)である。そのモデルは，ひとえに期待どおり正確であることで正当化される。 --- フォン・ノイマン
- われわれの宇宙はただ単に数学で記述されているだけではない。宇宙は数学である，我々は皆，大きな数学的実態の一部なのだ。--- テグマーク
<!-- ...Our universe isn't just described by math, but that it is math in the sense that we're all parts of a giant mathematical object... --- Max Tegmark -->

<!--
Neumann
  The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.

  Young man, in mathematics you don't understand things. You just get used to them. [von Neumann](https://en.wikiquote.org/wiki/John_von_Neumann)

  any discussion of the nature of intellectual effort in any field is difficult, unless it presupposes an easy, routine familiarity with that field. In mathematics this limitation becomes very severe. ---[von Neumann](https://en.wikiquote.org/wiki/John_von_Neumann)

Neumann
  If one has really technically penetrated a subject, things that previously seemed in complete contrast, might be purely mathematical transformations of each other.

[There's no sense in being precise when you don't even know what you're talking about](https://www.brainyquote.com/quotes/john_von_neumann_137953)
- John von Neumann.

Neumann
  I think that it is a relatively good approximation to truth — which is much too complicated to allow anything but approximations — that mathematical ideas originate in empirics. [John von Neumann](https://en.wikiquote.org/wiki/John_von_Neumann)
-->

<!-- ## 数学

数学というと，心理学徒にとっては，心理統計が真っ先に思い浮かぶでしょう。
ですが，統計的検定のためだけに数学があるわけではなく，むしろ逆だと思っています。 -->


1. [1960 Wigner "Unreasonable Effectiveness of Mathmatics and Natural Sciences"](https://www.maths.ed.ac.uk/~v1ranick/papers/wigner.pdf){:target="_blank"}
2. [1967 Hamming "The Unreasonable Effectiveness of Mathematics"](https://www.tandfonline.com/doi/abs/10.1080/00029890.1980.11994966){:target="_blank"}
3. [2009 Norvig "The Unreasonable Effectiveness of Data"](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/35179.pdf){:target="_blank"}
4. [2015 Karpathy "The Unreasonable Effectiveness of Recurrent Neural Networks"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/){:target="_blank"}
5. 2016 Bangu "On The Unreasonable Effectiveness of Mathematics in the Natural Sciences"
6. [2018 Westhuizen "The Unreasonable Effectiveness of the Forget Gate"](https://arxiv.org/pdf/1804.04849.pdf){:target="_blank"}
7. [2021 Gao "The Unreasonable Effectiveness Of Neural Network Embeddings"](https://medium.com/aquarium-learning/the-unreasonable-effectiveness-of-neural-network-embeddings-93891acad097){:target="_blank"}

<!-- Arthur Lesk in molecular biology, "The Unreasonable Effectiveness of Mathematics in Molecular Biology".[6]
Max Tegmark in physics, "The Mathematical Universe".[8]
Ivor Grattan-Guinness in mathematics, "Solving Wigner's mystery: The reasonable (though perhaps limited) effectiveness of mathematics in the natural sciences".[9]
Vela Velupillai in economics, "The Unreasonable Ineffectiveness of Mathematics in Economics".[10] -->

# 統計学の危機


## [ASA アメリカ統計学会の声明](https://doi.org/10.1080/00031305.2016.1154108){:target="_blank"}

一方で，心理統計で用いられる母集団に対する信頼性は，しばしば疑問が呈されている。
アメリカ統計学会(ASA) では $p$ 値 を用いることに警告を発する宣言を出している。

出典: [ASA Statement on Statistical Significance and P-values](https://amstat.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108){:target="_blank"},
[その和訳](/2023/2016ASA_state_on_p_values_ja){:target="_blank"}

1. **P 値は，データが指定された統計モデルとどの程度相性が悪いかを示すことができる** P-values can indicate how incompatible the data are with a specified statistical model.
2. **P 値は，研究された仮説が真である確率を測定するものではない。そうではなく，データがランダムな偶然だけから，生成された確率を測定するものである** P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
3. **科学的な結論やビジネスや政策の決定は，p 値が特定の閾値を超えたかどうかだけに基づくべきではない** Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.
4. **適切な推論を行うには，完全な報告と透明性が必要である** Proper inference requires full reporting and transparency.
5. **P 値や統計的有意性は，効果の大きさや結果の重要性を測定するものではない** A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.
6. **それ自体では，p 値はモデルや仮説に関する証拠の良い尺度を提供しない。** By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

    * [基礎と応用社会心理学 (BASP)  編集方針 (2014, 2015)](/2023/2015Basic_and_Applied_Social_Psychology_ban_p_values_ja){:target="_blank"}
    * [アメリカ統計学会の声明 2014, 2015](/2023/2016ASA_state_on_p_values_ja){:target="_blank"}
    * [統計学の誤り : 統計的妥当性の「ゴールドスタンダード」である P 値は多くの科学者が想定しているほど信頼できるものではない](/2023/2014Nuzzo_Statistical_errors_ja){:target="_blank"}
    * [統計的有意性を引退させろ](/2023/2019Amrhein_Retire_statistical_significance_ja){:target="_blank"}

では，どうすればよのでしょうか。
おそらく，その答えの一つが機械学習であると考えることもできるのです。

# ソーカル事件と当世流行馬鹿噺 (ファッショナブル・ナンセンス)

* [ソーカル事件](https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%BC%E3%82%AB%E3%83%AB%E4%BA%8B%E4%BB%B6){:target="_blank"},
イアン・ソーカルとジャック・ブリクモン「境界侵犯 --- 量子重力の変形解釈学に向けて」と題したポストモダンの科学批評のパロディーを書き、（もちろん編集者にはそれがパロディーだとは告げず）カルチュラル・スダディーズの雑誌ソーシャル・テクストに投稿した。
ソーシャル・テクスト誌では「サイエンス・ウォーズ」特集号 (1996) でこのパロディー論文を，まじめな学術論文として掲載した。
その三週間後に、リンガ・フランカ誌に寄せた記事でソーカルはこのいたずらを暴露した。
* [知の欺瞞](https://www.amazon.co.jp/dp/4006002610){:target="_blank"} のことを考えてください。
* ただ **騙されない** ようにしたいと願うだけです。


* この授業は，文化，思想，に関する議論をする科目ではありません。
ましてや，文壇，言論界，などに対するいかなるメッセージも含むものではありません。
* ですが，[ソーカル事件](https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%BC%E3%82%AB%E3%83%AB%E4%BA%8B%E4%BB%B6){:target="_blank"}, や [知の欺瞞](https://www.amazon.co.jp/dp/4006002610){:target="_blank"} のことを考えてください。
- ただ **騙されない** ようにしたいと願うだけです。

* ソーカル事件とは： 1996 年，ニューヨーク大学物理学教授アラン・ソーカルは「ソーシャル・テキスト」誌に「境界を越える Towards a transformative hermeneutics of quantum gravity（量子重力の変容的解釈学に向けて）」と題する論文を書いた。
この論文は査読を経て受理され，出版された。
ソーカルは即座に，この論文全体がデマであることを告白した。
極端なポストモダニズムの科学批判のスタイルを暴露し，パロディにするために，狡猾な言葉で書かれた論文だった。
この記事は世界中で一面トップニュースとなり，激しく広範な論争を引き起こした。

黒木(「ソーカル事件」1998，大学の物理教育(2), 25-28) より
「科学者は非専門家の無知には筧容であるべきである．
しかし，権威ある知識人がデタラメを述べていて，さらに，それが多くの人達に影響を与えている（もしくはその危険性が高い）場合においては，科学の専門家はそのことをきちんと指摘すべぎだと思う．(27 ページ)」

* 堀 茂樹 (1998) きみはソーカル事件を知っているか？, 平凡社, 月刊百科, 1998 年 2 月号 No.424, p.14−15 および 3 月号 No.425, p.42−43
* 黒木 玄 (1998) ソーカル事件, 大学の物理教育，Vol.2, 23-28.
* ソーカル，アラン & ブリクモン，ジャン, (田崎，大野，堀 訳) (2012)，[__「知」の欺瞞 ---ポストモダン思想における科学の濫用__](https://www.iwanami.co.jp/book/b255892.html){:target="_blank"}, 岩波現代文庫/学術 261, 岩波書店.


### 訓練データ (training dataset)，テストデータ (test dataset)，検証データ (validation dataset)

* 機械学習では，心理統計で用いられるような 仮説検定を行うこともありますが，むしろ，行わない場合も多いです。
* 理由としては，仮説検定を行うことによりも，モデルの性能を向上させることに主眼があるからという意味合いであろうと考えられます。
* ですが，考え方は母集団統計量の推定と同じような発想をします。すなわち，まだ見ぬ未知のデータに対して精度が良いモデルが優れているモデルと判断されます。
* 訓練データを使ってモデルを作成し，作成したモデルの評価をテストデータを使って評価します。
* このとき，テストデータは訓練には使いません。未知のデータに対しての精度でモデルの性能の優劣を競います。従って，モデルの精度の良いモデルが良いモデルであり，かつ，良いモデルとは，未知のデータに対してより精度が高く動作するモデルとなります。
* この点については，母集団の統計量の優劣を考える心理統計とは異なります。
* 真の母集団という，ありもしない曖昧 (かも知れない) 仮想集団について斟酌するよりも，実際のデータについて精度の優劣でモデルの性能を競うという意味では，実務的な発想と言えるでしょう。
* 機械学習におけるモデルの精度向上を目指したパラメータチューニングのことを **学習** と呼びます。

### 過学習

* モデルのパラメータを学習するときに，同じデータを用いて性能を検証することは，方法論的に間違っていると言えます。
* すでに見たことのある敵をたおせても，真の勇者とは言えません。何度でも生き返ることができる RPG とは違います。
* 見たことのあるデータ （遭遇した経験のあるモンスター）は倒せるでしょう。ですが，それでは 勇者 ではなく チキン です。
* 経験済のデータについては，完璧なスコアを示すことができるでしょう。ですが，まだ見ぬデータに対して有用な予測をすることはできません。
* このような状況を 過学習 (over-learning) あるいは オーバーフィッティング (over-fitting) といいます。
* これを避けるために、（教師あり）機械学習を行う際には，利用可能なデータの一部を テストデータセット `X_test`, `y_test` として用意しておくのが一般的です。
* 一般に k-hold out 法などと呼ばれる手法は，訓練データセットを ｋ 個に分割します。
その上で，k 個に分割した 1 つのデータ群を除いた k-1 群の訓練データを用いてモデルの学習を行います。学習の都度，残しておいたデータを用いて性能を評価します。
* この方法により，最終評価に用いるテストデータを使うこと無くチューニングを行います。
* **なぜ全データを用いないで，データを分割するのか？**
  * 未知の母集団を仮定しないで，モデルの優劣を正当に評価するための方法であるとみなすことができます。

### 回帰と分類

* 機械学習で頻用される手法の分類に **回帰** と **分類** があります。
* 予測すべきデータが連続量の場合は，回帰
* 予測すべきデータが離散量の場合は，分類 と呼ばれます。
* 身長や体重，あるいは，明日の東京都における COVID-19 の感染者数を予測するのであれば 回帰 です。
* 一方，手書き数字認識は，予測すべきデータが 10 分類された各クラスですので 分類 と呼ばれます。
* $\mathbf{y} = \mathbf{Xw} +\mathbf{b}$ などは 線形回帰 と呼ばれます。これは中学校以来の 直線を表す 1 次方程式 $y=ax+b$ と同じ形をしています。
* $y$ を予測すべき量，$x$ を与えられたデータと考えます。
* 傾き slope:$a$ と 切片 intercept:$b$ とを推定する問題が 回帰 です。
* 中学校までの数学の知識では，2 点 $(x_1, y_1)$, $(x_2, y_2)$ が与えられたとき，$a$ と $b$ とは計算して求めることが可能でした。
* では，N 個のデータ $(x_1,y_1),\cdots,(x_n,y_n)$ が与えられたとき，切片 と 傾き とはどう定めたら良いのでしょうか？

#### モデルの精度を測る指標: 精度 (accuracy)，適合度 (precision)，再現率 (recall)，F1 値 (F1 value)

* モデルの精度とは，何か。精度とは，正しく予測できることです。分類課題の場合，
* 正しい予測と誤った予測とには，詳細な検討が必要になりる。
* ここでは，精度 とは，英語で accuracy である。
* 混乱する用語に適合度 precision がある。

* **精度 precision**: 正事例であると予測された事例のうち，正しく評価された事例の割合ある事例が陽性であると分類器が判定した際に，その陽性と判断された事例中の正しい割合。
<!-- This computes the proportion of instances predicted as positives that were correctly evaluated (it measures how right our classifier is when it says that an instance is positive). -->
* **適合度 precision**:
* **再現率 recall**: 分類器がどれだけ正しく正事例を判定できたか<!--This counts the proportion of positive instances that were correctly evaluated (measuring how right our classifier is when faced with a positive instance).-->
* **F1 値 F1-score**: 精度 (precision) と 再現率 (recall) との調和平均<!--This is the harmonic mean of precision and recall, and tries to combine both in a single number-->


| | 真の値 (+) | 真の値 (-) |
|:---|:---|:---|
|予測 (+) | True Positive (ヒット Hit) | False Positive (虚報 False alarm) |
|予測 (-) | False Negative (ミス Miss) | True Negative (正しい棄却 Correct rejection) |


<!-- | | 予測: + | 予測: - |
|---|----|----|
|真の値: + | True Positive (ヒット Hit)| False  Negative (ミス Miss) |
|真の値: - | False Positive (虚報 False alarm)| True Negative (正しい棄却 Correect rejection) | -->


* **問題: 分類問題の精度の指標の一つでもある混同行列の中で正しい組み合わせのものを選べ。ここでは小数第3位以下は切り捨てている。**

| | 真の値 (正 +) | 真の値 (負 -) |
|:---|:---:|:---:|
|分類器の予測 (+) | 13 | 1 |
|分類器の予測 (-) | 2  | 14 |

<!-- | | 予測: + | 予測: - |
|:---|:---:|:---:|
|真の値: + | 13 | 2 |
|真の値: - | 1  | 14 | -->



<!-- <div class="figcenter">
<img src="/assets/cm.svg" style="width:33%;"><br/>
</div> -->

ただし，A は正解率, B は適合率, C は再現率, D は F 値とする。

1. A:$0.866$, B:$0.896$, C:$0.928$, D:$0.900$
2. A:$0.928$, B:$0.900$, C:$0.896$, D:$0.866$
3. A:$0.896$, B:$0.866$, C:$0.900$, D:$0.928$
4. A:$0.900$, B:$0.928$, C:$0.866$, D:$0.896$

<!--
### 解説:
正解は 4

1. 精度 accuracy (13+14)/(13+2+1+14) = 0.900
2. 適合度 precision 13/(13+1) = 0.9286
3. 再現率 recall 13/(13+2) = 0.867
4. F1 (2 * precision * recall)/(precision + recall)

 -->


## 教師あり学習と教師なし学習

* 予測すべき数値に正解が与えられている場合，**教師あり学習 supervised learning** と呼びます。
* 一方，予測すべきデータが与えられていない場合を **教師なし学習 unsupervised learning** と呼びます。
* 手書き数字認識では，正解となるデータが与えられているので，教師あり学習となります。
* 一方で，正解データが与えられていない場合に，入力データを分類したりする場合を 教師なし学習と 呼びます。

以下はすぐに知る必要がない知識です

### 重回帰

中学校以来の直線の方程式 $y = ax + b$ を一般化します。
データ行列を $\mathbf{X}$，予測すべき値を $\mathbf{y}$ とし，推定すべきパラーメータを $\mathbf{W}$ で表します。
重回帰 multiple regression は次式で表されます:

$$
\mathbf{y}=\mathbf{Xw} +\mathbf{b}
$$
ここで $\mathbf{b}$ はバイアス項，中学数学で言えば切片にあたります。

### 主成分分析

データ $\mathbf{X}$ の次元圧縮 dimensionality reduction の方法です。
$\mathbf{X}$ を 係数行列 $\mathbf{w}$ によって変換したデータを $\mathbf{y}$ とします。
$\mathbf{y}$ の分散を最大化する方法として，次のような目的関数を最大化することを考えます:

$$
\mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw} - \lambda\left(\mathbf{w}^\top\mathbf{w}-1\right)
$$

ここで $\lambda$ はラグランジェ (Lagrange) の未定定数 (Lagrange's multiplier) と呼ばれる。
すなわち，主成分分析とは，目的関数である $\mathbf{w}^\top\mathbf{w}$ を最小化する代わりに，制約付き最小化問題を解くことに相当する。
目的とする関数を最小化する代わりに，新たな目的関数を設定して，その新しい目的関数を最小化することで，制約付き最初化を実現する方法である。

この方法を一般化して **変分法** variational methods と呼ぶ。

また，上式を解くことは，$\left|\mathbf{X}-\lambda\mathbf{I}\right|=0$ なる固有方程式を解くことになる。
すなわち，主成分分析とは，データ行列の固有値問題を解くことと同義である。

固有値問題，および 変分法，変分問題は，古くは，オイラーやニュートンによって始められた。
すなわち，惑星の運行を記述する運動方程式の解法として考案されました。
この方法を洗練させたのが，ラグランジェ で解析力学として定式化した。

### ロジスティック回帰

ロジスティック回帰とは 回帰の名前がついていますが，分類 問題を解くための手法である。
ある事象が生起する確率を $p$ とすれば，生起市内確率は $(1-p)$ と表せる。
この確率比のことを **ロジット比** と呼ぶ。
ロジット比の対数が次式に従うことを仮定するのが，ロジスティック回帰である。

$$
\frac{p(x)}{1-p(x)} = e^{x}
$$

上式を解けば，

$$
p(x) = \frac{1}{1+e^{-x}}
$$

この式を **シグモイド関数** sigmoid function と呼ぶ。

<!-- #### 伏線回収

初回の授業で，COVID-19 の感染者数の変動を記述するモデルを紹介しました。
Kermack McKendrick モデルのポイントは 時刻 $t$ における感染者の増加率 $dp/dt$ は その時の感染者の比率と非感染者の比率 の積に比例する
と仮定することでした。

$$
\frac{dp}{dt} = \beta p(t)\left(1-p(t)\right)
$$

上式を高等学校数学風味に書き換えると次式のようになります。

$$
y' = \beta y(1-y)
$$

ここでは $p$ を $y$ と書き換えました。また微分を表す記号を プライム (') にしました。
この式は，高校学校2年生の知識で解くことができます。 -->


### 勾配降下法

重回帰では解析解が存在しました。一方，非線形問題は一般に解析解が存在しません。
その際に，目的関数を繰り返しによって求める方法があります。
**勾配降下法** gradient descent methods はその一つです。
任意の点 $x$ における関数 $f(x)$ の微分が定義されていれば，求める関数の最小値は次式:

$$
\Delta\theta = \eta\frac{\partial f}{\partial\theta}
$$

を逐次計算することで求めることができると仮定します。
ここで $\theta$  はモデルのパラメータ，$f$ は目的関数，$\eta$ は学習率，$\partial$ は **偏微分** partial differential を表します。



<!--
Authors:    J.A. Anderson, A. Pellionisz, E. Rosenfeld (eds.)
Title:      Neurocomputing 2: Directions for Research
Reference:  MIT Press, Cambridge (1990), Massachusetts

### ANNs are some kind of non-linear statistics for amateurs
-->

<!--
## 次の語の示すサイトを訪れ，それぞれどのようなサイトかを調べよ。
いずれも現在のエコシステムとしての役割を果たしている。

1. arXiv: <font color="white">論文置き場</font>
2. Colab:
3. Github: <font color="white">プログラムのソースコード置き場</font>
4. Stack Oerflow: <font color="white">掲示板，ノウハウ集</font>
5. Reddit: <font color="white">掲示板，ただしビッグネーム本人が降臨することがある</font>
-->

<!--
# AI を学ぶ人間のための心構え
- 無知蒙昧から来るブラックボックス的な恐怖を払拭するよう務める(現時点での技術的な裏付けに基づく啓蒙活動)
- 現在の技術から予測できる近未来の展望を語ることを忌避しない(謙遜は美徳ではない)

<center>
<img src="https://blogs-images.forbes.com/markhughes/files/2016/01/Terminator-2-1200x873.jpg" style="width:32%">
<img src="http://zatugaku1128.com/wp-content/uploads/2016/09/%E3%83%89%E3%83%A9%E3%81%88%E3%82%82%E3%82%93.png" style="width:20%"></br>
</center>

未来はどっち？ **It will depend on you.**

# クイズ
* 次の語の組み合わせのうち不適切なものを指摘せよ

1. IBM - Watson - Joapady
2. DeepMind - AlphaGo - 囲碁
3. Google 翻訳 - ペッパー
4. Uber - 自動運転
-->


## ニューラルネットワーク

ニューラルネットワーク (神経回路網 neural networks) とは，神経細胞 (ニューロン) の結合 (ネットワーク) のことです。

クイズのようなダジャレのような話ですが，ANN, BNN, CNN, DNN という省略形で呼ばれています。

* ANN: 人工ニューラルネットワーク
* BNN: 生物学的ニューラルネットワーク
* CNN: 畳み込みニューラルネットワーク。アメリカ合衆国のケーブルテレビの名称でもある。
* DNN: ディープニューラルネットワーク

広義には ニューロンを基本計算単位とした情報処理モデルであると言えます。
**計算** という言葉は，算術演算を意味しません。脳の働き，さまざまな心の作用はすべて 計算である との立場です。
すなわち，我々の知的活動，心的状態は，ニューロンを基本構成単位とする ネットワーク働きとして説明されるという、心，あるいは 脳を理解するためのパラダイム一般をニューラルネットワーク，
あるいは，心 （精神） の計算理論と呼びます。

現在までのところ，ニューラルネットワーク研究では，脳の血流 （とその異常，障害），神経伝達物質の代謝 (とその異常，障害) ，と言った側面にまで
及んでいるわけではありません。

神経細胞は，人間を含む動物が持っていますが，人工ニューラルネットワーク (ANN) では，コンピュータ上で，ニューロンの働きを模倣することにより，複雑な課題を解くことを目指し，
場合によっては，人間以上の性能を示すまでになっています。

ANN は 生物学的ニューラルネットワーク (BNN) にヒントを得て作成されました。
ですが，現在の ANN は BNN に比べて極端な単純化を行った並列情報処理モデルです。

たとえば，スパイキングモデルは計算機科学の分野では重要な位置を占めています。
スパイキングモデルでは 樹状突起による計算や 各ニューロン内の他のプロセス (Gallistel&King2011) あるいは，異なるタイプのニューロンからの関与を考慮しているとは言えません。

通常 ANN では ニューロンの空間構造は プログラミング可能な形で抽象化されています。
ニューロンのスパイク出力はスパイク率のような実数としてモデル化されています。
<!--このレートは、静的な非線形性を介して入力される活性化の加重和としてモデル化されます。-->
このように単純化されているにもかかわらず，ニューラルネットワークは脳の情報処理を理解するための最も重要な方法の 1 つとなっています。

実際のニューロンの活動を調べる 神経科学 と 人間の 知的活動を 材料とする 認知科学 との橋渡しをする 理論モデルとして 中心的な役割を果たすことになると予想します。


* [認知計算論的神経科学 Kriegeskorte and Douglas (2018)](../2018Kriegeskorte_ja.pdf){:target="_blank"} 
* [認知神経科学のためのディープラーニング Storrs and Nikolaus Kriegeskorte (2019)](../2019Storrs_ja.pdf){:target="_blank"} 
* [視覚系のモデルとしての畳み込みニューラルネットワーク: 過去，現在，そして未来 Lindsay (2020)](../2020Lindsay_ja.pdf){:target="_blank"}
* [深層学習の神経科学的基礎: 進化と展望 Yamins and DiCarlo (2016)](../2016Yamins_ja.pdf){:target="_blank"}