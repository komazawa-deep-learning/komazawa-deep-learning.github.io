---
title: 第16回 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\BRc}[1]{\left[#1\right]}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+2" color="navy"><strong>2025 年度開講 駒澤大学 人工知能 I および II</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 26/Sep./2025<br/>
Appache 2.0 license<br/>
</div>

# 第 16 回 後期第 2 回

* [課題提出用フォルダ](https://drive.google.com/drive/u/6/folders/1YGao7jcd4oDk7nva83aUs49-jno6tAde){:target="_blank"}

# 実習ファイル


* [tiktoken の利用 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_0926tiktoken_demo.ipynb){:target="_blank"}
* [オリベッティ顔データセットを用いた系列学習モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025notebooks/2025_0926olivetti_face_series.ipynb){:target="_blank"}


# キーワード

言語モデル，系列予測，フーリエ変換，時系列分析, AR, ARMA, ARIMA


# 言語モデル LM
狭義の言語モデルとしては，次に来る単語 (記号やその他を含めて，トークンとも呼ばれる) を予測モデルのことを言語モデル Language Models と呼ぶ。
ある位置 $t$（ある時刻 $t$ と言い換えても良い）の単語を $x_t$ と書くことにすると，次にくる単語は $x_{t+1}$ と書ける。
もっとも単純な言語モデルとは，ある単語 $x_t$ が与えられたときに，次に来る単語 $x_{t+1}$ を正しく予測することである。
したがって条件付き確率 $\displaystyle P(x_{t+1}|x_{t})$ を最大にするような単語 $x_{t+1}$ を予測するモデルとなる。

## n-gram 言語モデル

- 類似した言語履歴 $h$ について, n-gram 言語モデルは言語履歴 $h$ によって言語が定まることを言う。
- 実用的には n-gram 言語モデルは $n$ 語の単語系列パターンを表現するモデル。
- n-gram 言語モデルでは $n$ の次数増大に従って，パラメータは指数関数的に増大する。
- すなわち高次 n グラム言語モデルのパラメータ推定に必要な言語情報のコーパスサイズは，次数増大に伴って，急激不足する
- Wikipedia からの引用では次式:
$$
p(w_1,\dots,w_m)=\prod_{i=1}^{m} P(w_i\vert w_1,\ldots,w_{i-1})\simeq \prod_{i=1}^{m}p(w_i\vert w_{i-(n-1)},\ldots,w_{i-1})
$$
- 上式では $m$ だが，伝統的に $n$ グラムと呼ぶ$n=1$ であれば直前の 1 つを考慮して次語を予測することとなる。

余談だが，n-グラム (gram)  については，いかの呼び名で呼ばれる：

  * $n=0$: ヌルグラム null-gram
  * $n=1$: ユニグラム uni-gram
  * $n=2$: バイグラム bi-gram
  * $n=3$: トリグラム tri-gram

Manning1999(p.193) によると単語 _gram_ はギリシャ語由来の単語である。従って _gram_ に付ける数接頭辞もギリシャ語である教養を持つべきである。そうだとすると $n=1$: mono-gram, $n=2$: di-gram, $n=4$: tetra-gram が教養です。
$n=3$ はギリシャ，ローマ共通で tri-gram となる。日常会話では $n=4$ をクワッドグラム(ラテン語由来)やフォーグラムと呼ぶことも多い。


# 系列予測モデル


## 自己相関 (AR) 和分 (I) 移動平均 MA モデル

ARIMA モデルとは自己相関 AR, 移動平均 MA, およびその和分 I をあわせた系列予測モデルである。

教科書には p, d, q としてそれぞれ AR, I, MA の次数を決める。
(p,d,q) = (1,0,0) であれば，1 次の自己相関となる。すなわち次式である：
$$
AR(1): y_{t} = \alpha y_{t-1} + c + \epsilon
$$
ここで時刻 $t$ の値 $y_{t}$ を予測する際に，直前の時刻 $t-1$ の値 $y_{t-1}$ の値から予測することになる。
$c$ は切片，$\alpha$ は回帰係数，$\epsilon$ は誤差である。

AR(2) すなわち 2 次の自己相関モデルであれば次式となる。
$$
AR(2): y_{t} = \alpha_{1}y_{t-1} + \alpha_{2}y_{t-2} + c + \epsilon
$$

ただし以下のような制約が存在する。
* AR(1)モデル: $−1<\phi_1<1$<br/>
* AR(2)モデル: $−1<\phi_2<1,\phi_1+\phi_2<1,\phi_2−\phi_1<1$

AR(m) m > 2 の場合制約は複雑になる。

$m$ 次の自己相関モデル AR(m) であれば，次式となる：
$$
y_{t} = \sum_{i=1}^{m}\alpha_{i}y_{i} + c + \epsilon 
$$

ARIMA モデル，$d$ 階差分系列, $y_t-y_{t-d}$ を ARMA モデルで表現するのが ARIMA モデルである。
$$
ARIMA(p,d,q):y_{t} - y_{t-d} = c + \sum_{i=1}^{p}\phi_{i}y_{t-i} + \sum_{i=1}^{q}\theta_{i}\epsilon_{t-1}
$$


## 移動平均モデル

$$
y_t = c + \epsilon_t + \theta_1\epsilon_1+\cdots+\theta_{t-q}\epsilon_{t-q}
= c + \epsilon_t + \sum_{i=1}^{q}\theta_{i}\epsilon_{t-i},
$$
ここで $\epsilon_{t}$ は白色雑音である。上式を q 次の移動平均モデル MA(q) と呼ぶ。

MA(1) であれば次式となる：
$$
MA(1):y_t = c + \epsilon + \theta_1\epsilon_{t-1}
$$

## ARIMA

差分化に自己回帰モデルや移動平均モデルを組み合わると、ARIMA モデルとなる。
ARIMA は AutoRegressive Integrated Moving Average の略語である。(integration は差分化の逆の意味。) 完全なモデルは以下のように書ける：

$$
y^{\prime}_{t}=c+\phi_1 y^{\prime}_{t-1} +\cdots+\phi_{p}y^{\prime}_{t-p} + \theta_{1}\epsilon_{t-1}+\cdots+\theta_{q}\epsilon_{t-q}+\epsilon_{t}
$$
ただし、 $y^{\prime}_{t}$ は差分系列(1 回を超えて差分化されている場合もある)。右辺の「予測変数」には、 $y_{t}$ のラグと誤差のラグの両方があある。これを ARIMA(p,d,q) モデルと呼ぶ。
ただし、

* p= 自己回帰の次数
* d= 1 つ目差分化の次数
* q= 移動平均の次数


##  フーリエ変換

以下の蛇足は覚える必要はないが，覚えておくべき重要な概念は，**関数の直交** である。

前期には，相関係数，一般にピアソンの積率相関係数と呼ばれる量は，２ 変数の共分散を，両変数の標準偏差で除すことで得られる量であった。

$$
\text{相関係数} r_{xy} = \frac{\text{x と y との共分散}}{\text{x の標準偏差}\times\text{y の標準偏差}} = 
\frac{\sum_{i}\left(x_{i}-m_{x}\right)\left(y_{i}-m_{y}\right)^{2}}{\sqrt{\sum_{i}(x_{i}-x_{m})^{2}} \sqrt{\sum_{i}(y_{i}-y_{m})^{2}}}
$$

相関係数が 0 であるとは，共分散が 0 であることであり，このことは ２ 本の高次元ベクトルが直交することであった。

これを拡張して関数の直交を考える。



### 蛇足：三角関数の変換公式

#### 積和変換公式

$n,m$ を整数として，$n\ne m$ であれば<br/>
$$\sin n \sin m = -\frac{1}{2}\BRc{\cos(n+m)-\cos(n-m)}$$

$$\sin n \cos m = \frac{1}{2}\BRc{\sin(n+m)+\sin(n-m)}$$

$$\cos n \sin m = \frac{1}{2}\BRc{\sin(n+m)-\sin(n-m)}$$

$$\cos n \cos m = \frac{1}{2}\BRc{\cos(n+m)+\cos(n-m)}$$

#### 和積変換公式

$$\sin n + \sin m = 2 \sin\frac{n+m}{2} \cos\frac{n-m}{2}$$

$$\sin n - \sin m = 2 \cos\frac{n+m}{2} \sin\frac{n-m}{2}$$

$$\cos n + \cos m = 2 \cos\frac{n+m}{2} \cos\frac{n-m}{2}$$

$$\cos n - \cos m = -2 \sin\frac{n+m}{2} \sin\frac{n-m}{2}$$

<!-- * $$\tan n \pm \tan m = \frac{\sin(n\pm m)}{\cos n \cos m}$$
* $$\tan n \tan m = \frac{\cos(n-m)-\cos(n+m)}{\cos n \cos m}$$
* $$\tan n - \tan m = \frac{\sin(n-m)}{\cos n \cos m}$$
* $$\tan n + \tan m = \frac{\sin(n+m)}{\cos n \cos m}$$
* $$\cot n \pm \cot m = \frac{\sin(m\pm n)}{\sin n \sin m}$$
* $$\cot n \cot m = \frac{\cos(n+m)-\cos(n-m)}{\sin n \sin m}$$
* $$\cot n - \cot m = \frac{\sin(n-m)}{\sin n \sin m}$$
* $$\cot n + \cot m = \frac{\sin(n+m)}{\sin n \sin m}$$
* $$\tan n \cot m = \frac{\cos(n-m)-\cos(n+m)}{\cos n \sin m}$$ -->

$$\begin{aligned}
\int_{-\pi}^{\pi} \cos nx \cos mx \, dx &= \frac{1}{2}\int_{-\pi}^{\pi} \BRc{\cos(n-m)x + \cos(n+m)x} \, dx\\
&=\frac{1}{2(n-m)}\BRc{\sin(n-m)x}_{-\pi}^{\pi}
+ \frac{1}{2(n-m)}\BRc{\sin(n+m)x}_{-\pi}^{\pi}\\
&= 0
\end{aligned}$$


$$\begin{aligned}
\int_{-\pi}^{\pi} \sin nx \sin mx \, dx &= \frac{1}{2}\int_{-\pi}^{\pi} \BRc{\cos(n-m)x - \cos(n+m)x} \, dx\\
&= \frac{1}{2}\BRc{\left.\frac{\sin(n-m)x}{n-m}\right|_{-\pi}^{\pi} - \left.\frac{\sin(n+m)x}{n+m}\right|_{-\pi}^{\pi}}\\
&= 0
\end{aligned}$$

$$\begin{aligned}
\int_{-\pi}^{\pi} \sin nx \cos mx \, dx &= \frac{1}{2}\int_{-\pi}^{\pi} \BRc{\sin(n+m)x + \sin(n-m)x} \, dx\\
&= \frac{1}{2}\BRc{\left.-\frac{\cos(n+m)x}{n+m}\right|_{-\pi}^{\pi} + \left.-\frac{\cos(n-m)x}{n-m}\right|_{-\pi}^{\pi}}\\
\end{aligned}$$


### [DALL・E](https://arxiv.org/abs/1605.05396)
<!-- 
<div class="figcenter">
<img src="/2024assets/2025_0110chatGPT_DALLE1.jpg" style="width:33%;">
<Img src="/2024assets/2025_0110chatGPT_DALLE2.jpg" style="width:33%;">
</div> -->

<p align="center">
<img src="/2024assets/stable_diffusion.png" style="width:44%;">
<!-- <img src="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png" style="width:44%;"><br/> -->
<!-- <img src="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png" alt="sd-pipeline" width="500"/> -->
</p>


<div class="figcenter">
<img src="/2024assets/CLIP.png" style="width:77%;">
</div>
<div class="figcaption">
**CLIP の概要**<br/>
標準的な画像モデルが画像特徴抽出器と線形分類器を同時に学習し，何らかのラベルを予測するのに対し，CLIP は画像符号化器とテキスト符号化器を同時に学習し (画像とテキストの) バッチ学習例の正しいペアリングを予測する。
検証時に，学習したテキスト符号化器は，目標データセットのクラスの名前や説明を埋め込むことで，ゼロ撃の線形分類器を合成する。
<!-- Figure 1. Summary of our approach.
While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples.
At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes. -->
</div>

<div class="figcenter">
<img src="/assets/2015Ronneberger_U-Net_Fig1_ja.svg" style="width:55%;">
</div>


<div class="figcenter">
<img src="/2024assets/perturb_vp.gif" style="width:44%;">
<img src="/2024assets/denoise_vp.gif" style="width:44%;">
<div class="figcaption" style="width:44%;">
左: 連続時間確率過程によるデータのノイズへの摂動<!-- Perturbing data to noise with a continuous-time stochastic process. --><br/>
右: 摂動手順を逆にすることで，ノイズからデータを生成
<!-- Generate data from noise by reversing the perturbation procedure. -->
</div></div>

<div class="figcenter">
<img src="/2024assets/sde_schematic.jpg" style="width:55%;">
<!-- ![](../../assets/img/score/sde_schematic.jpg){.img-fluid .rounded .z-depth-1} -->
</div>
<div class="figcaption">
逆 SDE を解くと，スコアベースの生成モデルが得られる。
データを単純なノイズ分布に変換することは，SDE で達成できる。
各中間時間ステップにおける分布のスコアがわかれば，ノイズからサンプルを生成するために逆 SDE を解くことができる
。
<!-- Solving a reverse SDE yields a score-based generative model.
Transforming data to a simple noise distribution can be accomplished with an SDE.
It can be reversed to generate samples from noise if we know the score of the distribution at each intermediat
e time step. -->
</div>


<div class="figcenter">
<img src="/2024assets/teaser.jpg" style="width:77%;">
<!-- ![](../../assets/img/score/teaser.jpg){.img-fluid .rounded .z-depth-1} -->
<div class="figcaption">
SDE を使用してデータをノイズ分布 (事前分布) に写像し，この SDE を逆にして生成モデリングを行うことができる。
関連する確率フロー ODE を逆にすることもできる。
これにより，SDE と同じ分布からサンプリングする決定論的処理が生成される。
逆時間 SDE と確率フロー ODE はどちらも，スコア関数を推定することで取得できる。
<!-- We can map data to a noise distribution (the prior) with an SDE, and reverse this SDE for generative mode
ling.
We can also reverse the associated probability flow ODE, which yields a deterministic process that samples fro
m the same distribution as the SDE.
Both the reverse-time SDE and probability flow ODE can be obtained by estimating score functions. -->
</div></div>
