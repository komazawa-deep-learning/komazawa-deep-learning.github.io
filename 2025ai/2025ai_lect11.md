---
title: 第11回 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div align="center">
<font size="+2" color="navy"><strong>2025 年度開講 駒澤大学 人工知能 I および II</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 04/Jul./2025<br/>
Appache 2.0 license<br/>
</div>

# 第 11 回

* [課題提出用フォルダ <img src="/2025assets/Google_Drive_icon_2020.svg" style="width:02%">](https://drive.google.com/drive/u/3/folders/1LlRBZUYktTCVXjCT4aadAJo3JuYKC_2-){:target="_blank"}

* ResNet, Inception, DenseNet, ドロップアウト, 転移学習, 微調整, 蒸留

## 実習ファイル

* [ResNet 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0603ResNet_with_Olivetti_faces_.ipynb){:target="_blank"}
* [ニューラルネットワークモデルの定義 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_1028komazawa_neural_networks_primer.ipynb){:target="_blank"}
* [3 層パーセプトロンと確率的勾配降下法のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2021notebooks/2021_0521mlp_Adam_SGD.ipynb){:target="_blank"}
<!-- - [EfficientNet のパラメータ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/drive/1QpKBHsBR5yvEOz2M-pKCUpliDh1XXplS){:target="_blank"} -->

<!-- - [Karapetian+(2023) データを用いた ResNet, LeNet 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_1129ResNet_LeNet_with_Karapetian2023.ipynb){:target="_blank"}
* [AlexNet による Karapetian+(2023) データの転移学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_1122Karapetian_AlexNet_transfer_learning.ipynb){:target="_blank"} -->

* [ソフトマックス関数解題 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1107softmax.ipynb){:target="_blank"}
また，ソフトマックス関数は，エネルギー関数とみなすことも可能である。
- [DETR を用いた領域切り出し  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0625DETR_demo.ipynb){:target="_blank"}
- [フィラデルフィア絵画命名検査課題 PNT を転移学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618pnt_transfer_learning.ipynb){:target="_blank"}
- [データ拡張 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617plot_transforms_demo.ipynb){:target="_blank"}
- [CAM 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb){:target="_blank"}
<!-- - [各画像の画面表示時に日本語キャプションを付与する準備 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/ccap/blob/master/notebooks/2020importing_ccap_from_GitHub.ipynb){:target="_blank"} -->




# 視覚モデルの歴史

人間の視覚処理のモデリングは，Hubel&Wiesel にさかのぼることができる。
Hubel&Wiesel では，視覚野 V1 の単純な細胞の応答特性はエッジの特徴検出として形式化され，複雑な細胞の特性は視野上で空間的に繰り返される一連の操作として概念化された（Hubel&Wiesel1962，translationaly invariant 並進不変）。
この計算原理は，コンピュータビジョンに霊長類の視覚系の特性を取り入れる試みとして Neocognitron（Fukushima1980）に取り入れられた。
さらに HMAX モデルファミリー（Riesenhuber&Poggio1999, Serre+2007）にも影響を与えた。
これらは，今日の特徴検出器とプーリング演算子を交互に用いた物体認識の深層学習モデルとして用いられれている。
(ただし，画像切り分けでは，プーリングを除外する傾向にある。)
AlexNet (Russakovsky+2015) 以前は，ネットワークをどのように組み込むか，あるいは他の方法で訓練するか，明確ではなかった（Olshausen&Field1996, Lowe1999, Torralba&Oliva2003）。
深層ニューラルネットワークを訓練する少なくとも 1 つの方法が示された 。同時に，このような不変
ImageNet 画像認識コンテストで優勝したモデルでは，視覚野 V4 と IT のニューロンの応答を圧倒的によくモデル化した内部「神経」表現を生成することが実証された（Yamins+2013, Cadieu+2014, Yamins+2014）。
ヒトの fMRI や MEG といった，より高度な実験レベルでの説明力の向上が確認された（Khaligh-Razavi&Kriegeskorte2014, Güçlü&van Gerven2015, Cichy+2016）。
<!-- Modeling human visual processing traces back at least to Hubel and Wiesel where response properties of simple cells in visual area V1 were formalized as feature detection of edges and properties of complex cells were conceptualized as a set of operations that were spatially repeated over the visual field (Hubel&Wiesel1962, i.e., translationally invariant).
These computational principles inspired the first models of object recognition, most notably, the Neocognitron (Fukushima1980) and the HMAX model family (Riesenhuber&Poggio1999; Serre+2007), where feature detectors and pooling operators were used in turns to build deep hierarchical models of object recognition.
However, such models lacked robust feature representations as it was not clear at the time how to either build in or otherwise train these networks to learn their spatially-repeated operations from input statistics – particularly for areas beyond visual area V1 (Olshausen&Field1996, Lowe1999, Torralba&Oliva2003).
These issues were first addressed by the AlexNet ANN (Krizhevsky+2012) in that it demonstrated at least one way to train a deep neural network for a large-scale invariant object recognition task (Russakovsky+2015).
Concurrently, deep networks optimized for such invariant object recognition tasks were demonstrated to produce internal "neural" representations that were by far the best models of the responses of neurons in non-human primate visual areas V4 and IT (Yamins+2013, Cadieu+2014, Yamins+2014).
Later work in humans confirmed these gains in explanatory power at the courser experimental level of fMRI and MEG (Khaligh-Razavi&Kriegeskorte2014; Güçlü&van_Gerven2015, Cichy+2016), with detailed measures of behavioral response patterns in both humans and non-human primates (e.g., Rajalingham+2015, Kubilius+2016, Rajalingham+2018), and with non-human primate neural spiking measures from the cortical area V1 (Cadena+2017). -->

## 畳込みニューラルネット(CNN)

<div class="figcenter">
<img src='/assets/imagenet_result2017.png' style='width:74%'><br/>
イメージネットコンテストの結果
<!-- 画像認識の進歩 -->
</div>

深層学習 (ディープラーニング) の中で **畳み込みニューラルネットワーク** CNN と呼ばれるニューラルネットワークについて解説する。

最初に画像処理の概略を述べる CNN が，それまで主流であった従来の手法の性能を凌駕したことはすでに述べました。
CNN の特徴の一つに **エンドツーエンド** と呼ばれる考え方があります。
エンドツーエンドとは，従来手法によるパターン認識システムでは，専門家による手の込んだ詳細な作り込みを必要としていたことと異なり，面倒な作り込みをせずとも性能が向上したことを指します。

エンドツーエンドなニューラルネットワークにより，次のことが実現しました。

- ニューラルネットワークの層ごとに，特徴抽出が行われ，抽出された特徴がより高次の層へと伝達される
- ニューラルネットワークの各層では，比較的単純な特徴から次第に複雑な特徴へと段階的に変化する
- 高次層にみられる特徴は低次層の特徴より大域的，普遍的である
- 高次層のニューロンは，低次層で抽出された特徴を共有している

このことを簡単に説明してみます。

我々人間は，外界を認識するために必要な計算を，生物種としての発生の過程と，個人の発達を通しての経験に基づく認識システムを保持していると見ることができます。
従って我々の視覚認識には化石時代に始まる光の受容器としての眼の進化の歴史と発達を通じた個人の視覚経験が反映された結果でもあります。
人工知能の目標は，この複雑な特徴検出過程をどうやったらコンピュータが獲得できるかということでもあります。
外界を認識するために今日まで考案されてきたモデル（例えば，ニューラルネットワークサポートベクターマシンなどは）は複雑です。
ですがモデルを訓練するための学習方法はそれほど難しくありません。
この意味で画像認識課題が正しく動作するためのポイントは，認識システムが問題を解く事が可能なほど複雑であるかどうかではなく，十分に複雑が視覚環境，すなわち画像認識の場合，外部の艦橋を反映するために十分な量の像データを容易すことができるか否かにあります。
今日の CNN による画像認識性能の向上は，簡単な計算方法を用いて複雑な外部環境に適応できる認識システムを構築する方法が確立したからであると言うことが可能です。

下図<!--[fig:2012Ng_01](#fig:2012Ng_01){reference-type="ref"reference="fig:2012Ng_01"} -->に画像処理の例を挙げました。

<center>
<img src="/assets/2012Ng_ML_and_AI_01.png" style="width:66%">
</center>
<!-- 図[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref"reference="fig:2012Ng_01"} -->

<center>
<img src='/assets/2013LeCun-tutorial-icml_15.svg' style="width:66%"><br/>

**LeCun (2013) より**
</center>

## LeNet5 (LeCun1998)

* **LeNet**. Yann LeCun (現 Facebook AI 研究所所長)による CNN 実装
 [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf){:target="_blank"} 手書き数字認識

<center>
<img src="/assets/1998LeNet5.png" width="84%"><br/>
<div style="text-align:left; width:77%;background-color:cornsilk">

LeNet5 の論文より改変
</div>
</center>

- 畳込層とプーリング層（発表当初はサブサンプリング）との繰り返し
  - 畳込とプーリングは<font color="green">局所結合</font>
- MNIST を用いた１０種類の手書き文字認識
- 最終２層は全結合層をつなげて最終層１０ニューロン，最終層の各ニューロンの出力がそれぞれの数字（０から９までの１０種）に対応する



## AlexNet

<img src="/2023assets/alex_net_block_diagram.png"><br/>


## 畳み込み演算

<center>
<img src="/assets/dmoulin_gif/full_padding_no_strides.gif" style="width:33%">
<img src="/assets/dmoulin_gif/same_padding_no_strides_transposed.gif" style="width:33%"><br/>
<div style="text-align=:left; width:66%; background-color:cornsilk">

左:入力層 5x5青，出力層緑，カーネルサイズ3x3, フルパディング，ストライド=1.<br/>
右:入力層 5x5青，出力層緑，カーネルサイズ3x3, フルパディング，ストライド=1. トランスポーズド畳み込み
</div>
<img src="/assets/dmoulin_gif/numerical_max_pooling.gif" style="width:33%">
<img src="/assets/dmoulin_gif/numerical_average_pooling.gif" style="width:33%"><br/>
<div style="text-align=:left; width:66%; background-color:cornsilk">

左: 最大値プーリング。
右: 平均値プーリング
</div>
<div style="text-align=:left; width:44%; background-color:cornsilk">
Dmoulin and Visin (2020) より
</div>

<img src="/assets/dmoulin_gif/padding_strides.gif" style="width:33%">
<img src="/assets/dmoulin_gif/padding_strides_odd.gif" style="width:33%">
<img src="/assets/dmoulin_gif/padding_strides_odd_transposed.gif" style="width:33%"><br/>
<div style="text-align=:left; width:44%; background-color:cornsilk">

左: padding_strides, 中:padding_strides_odd, 右:padding_stride_transposed
</div>
<img src="/assets/dmoulin_gif/same_padding_no_strides.gif" style="width:33%">
<img src="/assets/dmoulin_gif/same_padding_no_strides_transposed.gif" style="width:33%">
<div style="text-align=:left; width:44%; background-color:cornsilk">

右:same_padding_no_strides, 左: same_padding_no_strides_transposed
</div>
<img src="/assets/dmoulin_gif/arbitrary_padding_no_strides.gif" style="width:33%">
<img src="/assets/dmoulin_gif/arbitrary_padding_no_strides_transposed.gif" style="width:33%">
<div style="text-align=:left; width:44%; background-color:cornsilk">
右:arbitrary padding no strides, 左: artibtrary padding no stride transposed
</div>
</center>

<div class="figcenter">

<iframe src="/conv-demo/index.html" width="140%" height="640px;" style="border:none;"></iframe>
</div>


## HMAX 最大値プーリング Riesenhuber&Poggio(1999)

<div class="figcenter">
<img src="/assets/1999Riesenhuber_Poggio_fig2.svg" style="width:49%">
<div class="figcaption" style="width:66%;">

<font style="font-weight:bold">モデルのスケッチ</font><br/>
単純細胞から作られた複雑細胞の古典的なモデルを拡張したもので，線形演算 (福島の表記法では `S` ユニット，テンプレート・マッチング 図中の実線) と非線形演算 (`C`プーリングユニット，最大値 MAX 演算を行う 図中破線) を持つ層の階層で構成。
細胞入力の最大値を選択，その値を用いてセルを駆動する非線形の MAX 演算は複雑細胞に対して，線形入力の合計とは異なりモデルの特性の鍵となる概念である。
この 2 種類の操作は 異なる位置にチューニングされた求心性結合をプールすることでパターン特異性と並進不変性を，また異なるスケールにチューニングされた求心性結合をプールすることで、スケール不変性をもたらした (図示せず)。<br/>
Riesenhuber&Poggio(1999) Fig. 2 より
</div></div>


<div class="figcenter">
<img src="/assets/1999Riesenhuber_Poggio_fig3a.svg" style="width:44%">
<img src="/assets/1999Riesenhuber_Poggio_fig3b.svg" style="width:44%"><br/>
<div class="figcaption" style="width:99%">

MAX 機構 高度に非線形な形状調整の特性。<br/>
「最適」特徴を決定するために考案された「単純化手順」を用いて得られた下側頭葉細胞の応答（選好刺激に対する反応が等しくなるように正規化された反応)。
実験では，もともと細胞は「水のボトル」の画像 (一番左の物体) に非常に強い反応を示した。
その後，刺激を単色の輪郭に単純化したところ，細胞の発火が増加し，さらに，楕円を支える棒からなるパドルのような物体に変化した。
この物体が強い反応を引き起こすのに対し，棒や楕円だけではほとんど反応しなかった。
Riesenhuber&Poggio(1999) Fig 3A.
</div></div>

実験とモデルの比較。
白棒はの実験用ニューロンの反応を示す。
黒と灰色の棒は 選好刺激の 幹-楕円 の基部の遷移に合わせてチューニングしたモデル細胞の反応を示している。
モデル細胞は 直上図に示したモデルを簡略化したもの。
受容野の各位置に 2 種類の S1 特徴があり，それぞれが遷移領域の左側または右側にチューンしていて，その出力が C1 ユニットに入力され MAX 関数 (黒棒) または SUM 関数 (灰色棒) を用いてプールされている。
モデル細胞は 実験ニューロンの 選好刺激が受容野内にあるときに反応が最大になるよう，C1 ユニットに接続されていた。

図 3. MAX 機構の高度に非線形な形状チューニング特性。
(a) `最適` 特徴を決定するために考案された `単純化手続`(26) を用いて得られた，実験的に観察された IT 細胞の応答 (好ましい刺激に対する応答が 1 に等しくなるように正規化された応答)。
実験では，細胞はもともと「水瓶」 (一番左の物体) の画像にかなり強く反応した。
その後，刺激は単色の輪郭に「単純化」され，細胞の発火が増加し，さらに楕円を支える棒からなるパドルのような物体に変化した。
この物体は強い反応を引き起こしたが，棒や楕円だけではほとんど全く反応を起こさなかった (図は許可を得て使用)。
(b) 実験とモデルの比較。
白棒は (a) の実験ニューロンの反応。
黒棒と灰色棒は，優先刺激の幹-楕円底遷移に同調させたモデルニューロンの反応を示す。
このモデルニューロンは，図 2 に示したモデルの単純化された版の最上部にあり，受容野の各位置に 2 種類の S1 特徴のみが存在し，それぞれが遷移領域の左側または右側に同調し，MAX 関数 (黒棒グラフ) または SUM 関数 (灰色棒グラフ) のいずれかを用いてそれらをプールする C1 ユニットに供給される。
モデルニューロンは，実験ニューロンの好ましい刺激がその受容野にあるときに，その反応が最大になるように，これらの C1 ユニットに接続された。
<!-- Fig. 3. Highly nonlinear shape-tuning properties of the MAX mechanism.
(a) Experimentally observed responses of IT cells obtained using a `simplification procedure`(26) designed to determine `optimal` features (responses normalized so that the response to the preferred stimulus is equal to 1).
In that experiment, the cell originally responded quite strongly to the image of a `water bottle` (leftmost object).
The stimulus was then `simplified` to its monochromatic outline, which increased the cell’s firing, and further, to a paddle-like object consisting of a bar supporting an ellipse.
Whereas this object evoked a strong response, the bar or the ellipse alone produced almost no response at all (figure used by permission).
(b) Comparison of experiment and model.
White bars show the responses of the experimental neuron from (a).
Black and gray bars show the response of a model neuron tuned to the stem-ellipsoidal base transition of the preferred stimulus.
The model neuron is at the top of a simplified version of the model shown in Fig. 2, where there were only two types of S1 features at each position in the receptive field, each tuned to the left or right side of the transition region, which fed into C1 units that pooled them using either a MAX function (black bars) or a SUM function (gray bars).
The model neuron was connected to these C1 units so that its response was maximal when the experimental neuron’s preferred stimulus was in its receptive field. -->


<!-- MAX 機構に対する追加的な間接的支持は，IT 細胞の好ましい特徴，つまり細胞を駆動するための刺激成分を決定するために，「単純化手順」(26 )または「複雑性減少」(27) を用いた研究から得られている。 -->
MAX 機構に対する追加的な間接的支持は，IT 細胞の好ましい特徴，つまり細胞を駆動するための刺激成分を決定するために，「単純化手順」または「複雑性減少」を用いた研究から得られている。
これらの研究では一般的に，IT 細胞の高度に非線形な同調を発見している (図 3a)。
このような同調は MAX 応答関数 (図 3b 黒棒) と一致する。
線形モデル (図 3b 灰色棒) では，入力画像のわずかな変化に対す るこの強い応答の変化を再現できないことに注意。
<!--Additional indirect support for a MAX mechanism comes from studies using a `simplification procedure`(26) or `complexity reduction`(27) to determine the preferred features of IT cells, that is, the stimulus components that are responsible for driving the cell.
These studies commonly find a highly nonlinear tuning of IT cells (Fig. 3a).
Such tuning is compatible with the MAX response function (Fig. 3b, black bars).
Note that a linear model (Fig. 3b, gray bars) could not reproduce this strong change in response for small changes in the input image.-->





## 畳み込み演算を利用したニューラルネットワーク

<div align="center">
<!--<img src='https://komazawa-deep-learning.github.io/assets/2012AlexNet.svg" style="width:94%">-->
<img src="https://komazawa-deep-learning.github.io/assets/Neocognitron.svg" style="width:74%">
<img src="https://komazawa-deep-learning.github.io/assets/Fukushima.jpeg" style="width:24%"><br>
ネオコグニトロンの概略図(Fukushima, 1979)<br>
</div>


## LeNet5 (LeCun, 1998)
<center>
<img src="https://komazawa-deep-learning.github.io/assets/1998LeCun_Fig2_CNN.svg" style='width:94%'><br>
LeCun (1998) より
</center>

## AlexNet (Krizensky, et al., 2012)

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNet.svg" style="width:94%"><br/>
Krzensky et al (2012) より
</center>

## GooLeNet (Inception) (Szegedy et. al, 2014)

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2014Szegedy_GoogLeNet.svg" style='width:99%'><br/>
</center>

<!-- <center>
<img src='https://komazawa-deep-learning.github.io/assets/2013Uijings_Selective_Search_Fig1.svg' style='width:94%'><br>
空間ピラミッド (2015) より
</center>



<div align="center" style="width:94%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/full_padding_no_strides.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/same_padding_no_strides_transposed.gif" style="width:33%"><br/>
	<div align="left" style="width:66%">
		左:入力層 5x5青，出力層緑，カーネルサイズ3x3, フルパディング，ストライド=1.
		右:入力層 5x5青，出力層緑，カーネルサイズ3x3, フルパディング，ストライド=1. トランスポーズド畳み込み
	</div>
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/numerical_max_pooling.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/numerical_average_pooling.gif" style="width:33%"><br/>
	<div align="left" style="width:66%">
		左: 最大値プーリング。
		右: 平均値プーリング
	</div>
	<div align="left" style="width:66%">
		Dmoulin and Visin (2020) より
	</div>
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/padding_strides.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/padding_strides_odd.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/padding_strides_odd_transposed.gif" style="width:33%"><br/>
	<div align="left" style="width:66%">
		左: padding_strides, 中:padding_strides_odd, 右:padding_stride_transposed
	</div>
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/same_padding_no_strides.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/same_padding_no_strides_transposed.gif" style="width:33%">
    <div align="left" style="width:66%">
	 右:same_padding_no_strides, 左: same_padding_no_strides_transposed
	</div>
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/arbitrary_padding_no_strides.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/arbitrary_padding_no_strides_transposed.gif" style="width:33%">
    <div align="left" style="width:66%">
	 右:arbitrary padding no strides, 左: artibtrary padding no stride transposed
	</div>
</div>
-->


### イメージネットコンテスト，アレックスネットの出力にみる問題点

<div align="center" style="width:89%">
	<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNetResult0.svg" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNetResult.svg" style="width:33%">
	<div align="left" style="width:66%">
	アレックスネットの結果: 画像のすぐ下の英単語は正解ラベルを表す。Krizensky et. al (2012) Fig. 4 より。
	ピンク色は正解ラベルの確率を表す。ブルーは不正解ラベル判断確率を表している。
	チェリーが正解であるが，画像を見る限り，第一回答候補のダルマチアンを正解だと考えても問題は無いと考えられる。
</div>
</div>
















HMO の記述

## 2.2 階層的モジュラー最適化によるハイスループットスクリーニング
<!--2.2 High-Throughput Screening via Hierarchical Modular Optimization-->

我々の目標は、N 個の範囲内で、多様な画像に対する神経反応を効果的にモデル化できるモデルを見つけることである。このための基本戦略は、スクリーニングタスク [28] に対してハイスループット最適化を実行することである。物体認識問題を困難にする側面を十分に代表するスクリーニングタスクを選択することで、一般的に適用可能なネットワークアーキテクチャを見つけることができるはずである。スクリーニングセットとして、自然な背景に配置された日常的な物体の 36 種類の 3 次元メッシュモデルのいずれかを含む 125 枚の画像からなる 4500 枚の合成画像セットを作成した。評価対象としたスクリーニング課題は、36 方向物体認識であった。最大相関分類器（MCC）を、3 分割交差検証された 50%/50% の学習/検査分割を用いて学習させ、検査分類の正解率をスクリーニングの目的関数とした。
<!-- Our goal is to find models within N that are effective at modeling neural responses to a wide variety of images. To do this, our basic strategy is to perform high-throughput optimization on a screening task [28]. By choosing a screening task that is sufficiently representative of the aspects that make the object recognition problem challenging, we should be able to find network architectures that are generally applicable. For our screening set, we created a set of 4500 synthetic images composed of 125 images each containing one of 36 three-dimensional mesh models of everyday objects, placed on naturalistic backgrounds. The screening task we evaluated was 36-way object recognition. We trained Maximum Correlation Classifiers (MCC) with 3-fold cross-validated 50%/50% train/test splits, using testing classification percent-correct as the screening objective function.  -->


N は非常に大きな空間であるため、広大な可能性の中から、スクリーニングセットで高い性能を発揮する視覚表現を生成するパラメータ設定を特定することは困難である。我々は、階層的モジュラー最適化（HMO）と呼ぶ新しい手法を適用することで、この問題に対処した。HMO 最適化手順の直感的な考え方は、全体的な問題の一部に特化している単一スタックの成分を混合して作成することで、優れたマルチスタック異種ネットワークが見つかるというものである。これを実現するために、我々は適応型ハイパーパラメータブースティングの一種を実装した。これは、最適化のラウンドをブースティングと階層的スタッキングでインターリーブするものである。
<!-- Because N is a very large space, determining among the vast space of possibilities which parameter setting(s) produce visual representations that are high performing on the screening set, is a challenge. We addressed this by applying a novel method we call Hierarchical Modular Optimization (HMO). The intuitive idea of the HMO optimization procedure is that a good multi-stack heterogeneous network will be found by creating mixtures of single-stack components each of which specializes in a portion of an overall problem. To achieve this, we implemented a version of adaptive hyperparameter boosting, in which rounds of optimization are interleaved with boosting and hierarchical stacking.  -->

<div class="figcenter">
<img src="/2025assets/2013Yamins_HMO_fig2.png" style="width:77%"><br/>
<div class="figcaption" style="width:77%;">

**図 2**（A）階層的モジュール最適化（HMO）は、物体認識性能のためにニューラルネットワークを効率的に最適化する機構である。HMO の直感的なアイデアは、全体の問題の一部を専門とする単一スタック成分の混合を作成することで、優れたマルチスタック異種ネットワークを見つけることができるというものである。この過程では、まず、適応ブーストとハイパーパラメータ最適化を交互に繰り返す手法を用いて、単一スタック（非異種）畳み込みニューラルネットワークの空間において、補完的な性能勾配を特定する。この過程で特定された成分は、2  番目の畳み込み層を用いて非線形に組み合わされ、複合出力モデルが生成される。<br/> 
(B) 上：HMO 過程で特定された 2 つの補完的な成分に関連する  36 種の混同行列。左下：上部のパネルの混同行数表を生成した単一スタックモデルが導出された 2 つの最適化軌跡。第 2 ラウンドの最適化基準（赤点）は、第 1 ラウンドの誤差（青点）を基準に定義された。右下：第 1 ラウンドと第 2 ラウンドのネットワークを組み合わせた異種モデルによる混同行数。<!-- Figure 2: (A) The Hierarchical Modular Optimization is a mechanism for efficiently optimizing neural networks for object recognition performance. The intuitive idea of HMO is that a good multistack hetergenous network will be found by creating mixtures of single-stack components each of which specializes in a portion of an overall problem. The process first identifies complementary performance gradients in the space of single-stack (non-heterogenous) convolutional neural networks by using version of adaptive boosting interleaved with hyperparameter optimization. The components identified in this process are then composed nonlinearly using a second convolutional layer to produce a combined output model.<br/> 
(B) Top: the 36-way confusion matrices associated with two complementary components identified in the HMO process. Bottom Left: The two optimization trajectories from which the single-stack models were drawn that produced the confusion matrices in the top panels. The optimization criterion for the second round (red dots) was defined relative to the errors of the first round (blue dots). Bottom Right: The confusion matrix of the heterogenous model produced by combining the round 1 and round 2 networks. -->
</div></div>

具体的には、$N\in\mathcal{N}$ と $S$ がスクリーニング刺激セットであると仮定する。$E$ は 2 値分類正しさ指標であり、スクリーニング課題の予測が正しかったか間違っていたかに応じて、各刺激画像 $s$ に 1 または 0 を割り当てる。$\text{score}(N,S)=\sum_{s\in\mathcal{S}} N(F(s))$。$\text{score}(N,S)$ を最大化する $N$ を効率的に見つけるために、HMO 手順は次の手順に従う。
1. 最適化: 単一スタックネットワークのクラス内でスコア関数を最適化し、N 内のネットワークの最適化軌跡を取得する (図 2A 左)。ここで使用する最適化手順は、[1] で説明されている Hyperparameter Tree Parzen Estimator である。この手順は、離散パラメータと連続パラメータを含む大規模なパラメータ空間で有効である。
2. ブースティング: ステップ 1 で探索したネットワーク集合を弱学習器の集合とみなし、標準的なブースティングアルゴリズム（Adaboost）を適用して、誤りパターンが相補的なネットワーク $N_{11},\ldots,N_{1l_{1}}$ を複数特定する（図 2A右）。
3. 結合: マルチスタックネットワーク $N_1 =\oplus_{i}N_{1i}$ を形成し、すべての $s\in\mathcal{S}$ について $E(N_{1}(s))$ を評価する。
4. 誤りに基づく重み付け変更: ステップ 1 を繰り返し、スコアリングの重み付けを変更して、$j$ 番目の刺激 $s_j$ の重みを、$N_{1}$ が $s_{j}$ において正しい場合は 0， それ以外の場合は 1 とする。つまり、$N$ に対して最適化される性能関数は$\sum_{s\in\mathcal{S}}E(N_{1}(s))\cdot E(N(s))$ になる。得られた最適化軌跡の結果に対して手順2を繰り返してモデル $N_{21},\ldots,N_{2k2}$ を取得し、手順 3 を繰り返す。手順1、2、3 は K 回繰り返される。

<!-- Specifically, suppose that $N\in\mathcal{N}$ and $S$ is a screening stimulus set. Let $E$ be the binary-valued classification correctness indicator, assigning to each stimulus image s 1 or 0 according to whether the screening task prediction was right or wrong. Let $\text{score}(N,S)=\sum_{s\in\mathcal{S}} N(F(s))$: To efficiently find $N$ that maximizes $\text{score}(N,S)$, the HMO procedure follows these steps:
1. Optimization: Optimize the score function within the class of single-stack networks, obtaining an optimization trajectory of networks in N (fig 2A, left). The optimization procedure that we use is Hyperparameter Tree Parzen Estimator, as described in [1]. This procedure is effective in large parameter spaces that include discrete and continuous parameters.
2. Boosting: Consider the set of networks explored during step 1 as a set of weak learners, and apply a standard boosting algorithm (Adaboost) to identify some number of networks $N_{11},\ldots,N_{1l_{1}}$ whose error patterns are complementary (fig 2A, right).
3. Combination: Form the multi-stack network $N_1 =\otimes _{i}N_{1i}$ and evaluate $E(N_{1}(s))$ for all $s\in\mathcal{S}$.
4. Error-based Reweighting: Repeat step 1, but reweight the scoring to give the $j$-th stimulus $s_j$ weight 0 if $N_{1}$ is correct in $s_{j}$ , and 1 otherwise. That is, the performance function to be optimized for $N$ is now $\sum_{s\in\mathcal{S}}E(N_{1}(s))\cdot  E(N(s))$: Repeat the step 2 on the results of the optimization trajectory obtained to get models $N_{21},\ldots,N_{2k2}$ , and repeat step 3. Steps 1, 2, 3 are repeated K times. -->

