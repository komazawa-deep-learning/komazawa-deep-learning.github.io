---
title: 第15回 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+2" color="navy"><strong>2025 年度開講 駒澤大学 人工知能 I および II</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br/>
Date: 19/Sep./2025<br/>
Appache 2.0 license<br/>
</div>

# 第 15 回 後期第 1 回

* [課題提出用フォルダ](https://drive.google.com/drive/u/6/folders/12-BemcY1fFty0nFGQXwkFafek6Bjd8RY){:target="_blank"}

# 実習ファイル

* [<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2025noteboks/2025_0919demo.ipynb){:target="_blank"}

# 用語

* [Google Colab](https://colab.research.google.com/){:target="_blank"}, Google Colab とは，Python の実行環境を対話的にした ipython をブラウザで動作させる Jupyter notebook に対して，クラウド環境を提供したもの。
したがって，ブラウザ があれば PC のみならずスマートフォンでも動作する。
* Python: [TIOBE index](https://www.tiobe.com/tiobe-index/){:target="_blank"} によれば Python のシェアは圧倒的
  * Numpy, Pandas, Matplotlib, scikit-learn などのライブラリを用いている。
  * 機械学習，人工知能のライブラリとしては PyTorch, TensorFlow, Flax など。本授業では PyTorch を用いる。
  <img src="https://viso.ai/wp-content/uploads/2023/02/pytorch-vs-tensorflow-popularity-comparison.png" width="88%;">
* ニューラルネットワークモデル
* 深層ニューラルネットワークモデル
* 再帰型ニューラルネットワークモデル
* 転移学習
* 教師あり学習，自己教師あり学習
* 損失関数，誤差関数
* 勾配降下法
* コサイン類似度，相関係数
- 拡散モデル Diffusion Models
<!-- - 変分自己符号化器モデル (VAE: Variational Auto-Encoders)
- 変分下限 (ELBO: Evidence Lower BOund) [ビデオ ](https://www.youtube.com/watch?v=jugUBL4rEIM){:target="_blank"} -->

---

* [基盤モデル Fundation Models](https://arxiv.org/abs/2108.07258){:target="_blank"}, Gary Marcus による[批判](https://crfm.stanford.edu/commentary/2021/10/18/marcus-davis.html){:target="_blank"}
* 言語モデル
* 系列予測
* ワンホットベクトル
* 系列予測
* BERT, Transformer, GPT, 注意
* 世界モデル
* 強化学習



<!-- # 教材 -->

<!-- * [宇宙人の夢: アートシーンの出現](https://shinasakawa.github.io/2022/2021Snell_clip-art_ja){:target="_blank"} -->
<!-- * [CLIP デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2020_1014CLIP_ja_basics.ipynb){:target="_blank"} -->

<!-- * [畳み込みニューラルネットワークの事前訓練済モデルによる中間表現の可視化 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_1024CNN_layer_visualization.ipynb){:target="_blank"}-->
<!--* [特徴点検出アルゴリズム 画像フィルタ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_visit_feature_extractions_demo.ipynb){:target="_blank"}-->
<!-- * [DOG などのフィルタと Harr 特徴による顔検出 a.k.a ビオラ＝ジョーンズ アルゴリズム <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0528edge_and_face_detection_algorithm_not_cnn.ipynb){:target="_blank"}
<!-- - [LeNet PyTorch <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0528LeNet_pytorch.ipynb){:target="_blank"} -->
<!-- - [データ拡張 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617plot_transforms_demo.ipynb) -->

<!-- - [DeepDream 実習 <img src="/assets/colab_icon.svg"> ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb){:target="_blank"}-->
<!-- - [CartoonGAN 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0628CartoonGAN_demo.ipynb){:target="_blank"} -->
<!-- - [CycleGAN 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0625_CycleGAN_demo.ipynb){:target="_blank"} -->

<!-- - [VAE の実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit_vae_demo.ipynb){:target="_blank"} -->

<!-- * [Stable diffusion <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0714stable_diffusion.ipynb){:target="_blank"} -->
<!-- * [Stable Diffusion <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2025_0110stable_diffusion.ipynb){:target="_blank"} -->
<!-- * [Diffusion Models from Scratch <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2025_0110_02diffusion_models_from_scratch.ipynb){:target="_blank"} -->

<img src="https://github.com/Nixtla/nixtla/blob/main/nbs/img/timegpt_archi.png?raw=true" width="88%;"><br/>
from [About TimeGPT](https://nixtlaverse.nixtla.io/nixtla/docs/getting-started/introduction.html)

<!-- ### ありえないほど (unreasonable) 有能な (effectiveness) 数学 -->

<!-- ガリレイは，宇宙は数学の言葉で書かれていると言いました。以来，数学は神の摂理を知るための道具であり続けています。 -->
<!-- 数学的知識の詳細は不要だが，その精神は理解しておく必要がある。 -->

<!-- - 万物は数なり --- ピタゴラス
- 宇宙は数学語で書かれている。数学なしでは迷宮を理解できない --- ガリレイ -->
<!--- 世界について最も理解不能なことは，それが理解可能なことだ --- アインシュタイン-->
<!--“The most incomprehensible thing about the world is that it is at all comprehensible.”- Albert Einstein, US (German-born) physicist (1879 - 1955)-->
<!-- - 微積分は神がこの宇宙を作ったときの言葉である --- ファインマン -->
<!--Calculus was the language that God had used when creating this universe. -->
<!-- - 作れなければ理解できたと言えない --- ファインマン -->

<!-- - All things are number. --- Pythagras
- (The universe) is written in mathematical language,%%and its characters are triangles, circles and other geometric figures, ... without which it is impossible to humanly understand a word; without these one is wandering in a dark labyrinth. --- Galileo Galilei
- What I cannot create, I do not understand. --- [Richard Feynman](https://en.wikiquote.org/wiki/Richard_Feynman)-->

<!-- - 若者よ，数学は理解するものではない，ただ慣れるだけだ --- フォン・ノイマン -->
<!-- - 科学は説明しないし，解釈もしない。ただモデルを作るだけである。この場合モデルとは観察された現象を説明する数学(的構成物)である。そのモデルは，ひとえに期待どおり正確であることで正当化される。 --- フォン・ノイマン -->
<!-- - われわれの宇宙はただ単に数学で記述されているだけではない。宇宙は数学である，我々は皆，大きな数学的実態の一部なのだ。--- テグマーク -->
<!-- ...Our universe isn't just described by math, but that it is math in the sense that we're all parts of a giant mathematical object... --- Max Tegmark -->

<!--
Neumann
  The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.

  Young man, in mathematics you don't understand things. You just get used to them. [von Neumann](https://en.wikiquote.org/wiki/John_von_Neumann)

  any discussion of the nature of intellectual effort in any field is difficult, unless it presupposes an easy, routine familiarity with that field. In mathematics this limitation becomes very severe. ---[von Neumann](https://en.wikiquote.org/wiki/John_von_Neumann)

Neumann
  If one has really technically penetrated a subject, things that previously seemed in complete contrast, might be purely mathematical transformations of each other.

[There's no sense in being precise when you don't even know what you're talking about](https://www.brainyquote.com/quotes/john_von_neumann_137953)
- John von Neumann.

Neumann
  I think that it is a relatively good approximation to truth — which is much too complicated to allow anything but approximations — that mathematical ideas originate in empirics. [John von Neumann](https://en.wikiquote.org/wiki/John_von_Neumann)
-->

<!-- ## 数学

数学というと，心理学徒にとっては，心理統計が真っ先に思い浮かぶでしょう。
ですが，統計的検定のためだけに数学があるわけではなく，むしろ逆だと思っています。 -->

<!-- 1. [1960 Wigner "Unreasonable Effectiveness of Mathmatics and Natural Sciences"](https://www.maths.ed.ac.uk/~v1ranick/papers/wigner.pdf){:target="_blank"}
2. [1967 Hamming "The Unreasonable Effectiveness of Mathematics"](https://www.tandfonline.com/doi/abs/10.1080/00029890.1980.11994966){:target="_blank"}
3. [2009 Norvig "The Unreasonable Effectiveness of Data"](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/35179.pdf){:target="_blank"}
4. [2015 Karpathy "The Unreasonable Effectiveness of Recurrent Neural Networks"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/){:target="_blank"}
5. 2016 Bangu "On The Unreasonable Effectiveness of Mathematics in the Natural Sciences"
6. [2018 Westhuizen "The Unreasonable Effectiveness of the Forget Gate"](https://arxiv.org/pdf/1804.04849.pdf){:target="_blank"}
7. [2021 Gao "The Unreasonable Effectiveness Of Neural Network Embeddings"](https://medium.com/aquarium-learning/the-unreasonable-effectiveness-of-neural-network-embeddings-93891acad097){:target="_blank"} -->

<!-- Arthur Lesk in molecular biology, "The Unreasonable Effectiveness of Mathematics in Molecular Biology".[6]
Max Tegmark in physics, "The Mathematical Universe".[8]
Ivor Grattan-Guinness in mathematics, "Solving Wigner's mystery: The reasonable (though perhaps limited) effectiveness of mathematics in the natural sciences".[9]
Vela Velupillai in economics, "The Unreasonable Ineffectiveness of Mathematics in Economics".[10] -->

# 統計学の危機

## [ASA アメリカ統計学会の声明](https://doi.org/10.1080/00031305.2016.1154108){:target="_blank"}

一方で，心理統計で用いられる母集団に対する信頼性は，しばしば疑問が呈されている。
アメリカ統計学会(ASA) では $p$ 値 を用いることに警告を発する宣言を出している。

出典: [ASA Statement on Statistical Significance and P-values](https://amstat.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108){:target="_blank"},
[その和訳](/2023/2016ASA_state_on_p_values_ja){:target="_blank"}

1. **P 値は，データが指定された統計モデルとどの程度相性が悪いかを示すことができる** P-values can indicate how incompatible the data are with a specified statistical model.
2. **P 値は，研究された仮説が真である確率を測定するものではない。そうではなく，データがランダムな偶然だけから，生成された確率を測定するものである** P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
3. **科学的な結論やビジネスや政策の決定は，p 値が特定の閾値を超えたかどうかだけに基づくべきではない** Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.
4. **適切な推論を行うには，完全な報告と透明性が必要である** Proper inference requires full reporting and transparency.
5. **P 値や統計的有意性は，効果の大きさや結果の重要性を測定するものではない** A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.
6. **それ自体では，p 値はモデルや仮説に関する証拠の良い尺度を提供しない。** By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

    * [基礎と応用社会心理学 (BASP)  編集方針 (2014, 2015)](/2023/2015Basic_and_Applied_Social_Psychology_ban_p_values_ja){:target="_blank"}
    * [アメリカ統計学会の声明 2014, 2015](/2023/2016ASA_state_on_p_values_ja){:target="_blank"}
    * [統計学の誤り : 統計的妥当性の「ゴールドスタンダード」である P 値は多くの科学者が想定しているほど信頼できるものではない](/2023/2014Nuzzo_Statistical_errors_ja){:target="_blank"}
    * [統計的有意性を引退させろ](/2023/2019Amrhein_Retire_statistical_significance_ja){:target="_blank"}

では，どうすればよのでしょうか。
おそらく，その答えの一つが機械学習であると考えることもできるのです。

# ソーカル事件と当世流行馬鹿噺 (ファッショナブル・ナンセンス)

* [ソーカル事件](https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%BC%E3%82%AB%E3%83%AB%E4%BA%8B%E4%BB%B6){:target="_blank"},
イアン・ソーカルとジャック・ブリクモン「境界侵犯 --- 量子重力の変形解釈学に向けて」と題したポストモダンの科学批評のパロディーを書き、（もちろん編集者にはそれがパロディーだとは告げず）カルチュラル・スダディーズの雑誌ソーシャル・テクストに投稿した。
ソーシャル・テクスト誌では「サイエンス・ウォーズ」特集号 (1996) でこのパロディー論文を，まじめな学術論文として掲載した。
その三週間後に、リンガ・フランカ誌に寄せた記事でソーカルはこのいたずらを暴露した。

* この授業は，文化，思想，に関する議論をする科目ではありません。
ましてや，文壇，言論界，などに対するいかなるメッセージも含むものではありません。
* ですが，[ソーカル事件](https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%BC%E3%82%AB%E3%83%AB%E4%BA%8B%E4%BB%B6){:target="_blank"}, や [知の欺瞞](https://www.amazon.co.jp/dp/4006002610){:target="_blank"} のことを考えてください。
- ただ **騙されない** ようにしたいと願うだけです。

  * ソーカル事件とは： 1996 年，ニューヨーク大学物理学教授アラン・ソーカルは「ソーシャル・テキスト」誌に「境界を越える Towards a transformative hermeneutics of quantum gravity（量子重力の変容的解釈学に向けて）」と題する論文を書いた。
この論文は査読を経て受理され，出版された。
ソーカルは即座に，この論文全体がデマであることを告白した。
極端なポストモダニズムの科学批判のスタイルを暴露し，パロディにするために，狡猾な言葉で書かれた論文だった。
この記事は世界中で一面トップニュースとなり，激しく広範な論争を引き起こした。

黒木(「ソーカル事件」1998，大学の物理教育(2), 25-28) より
「科学者は非専門家の無知には筧容であるべきである．
しかし，権威ある知識人がデタラメを述べていて，さらに，それが多くの人達に影響を与えている（もしくはその危険性が高い）場合においては，科学の専門家はそのことをきちんと指摘すべぎだと思う．(27 ページ)」

* 堀 茂樹 (1998) きみはソーカル事件を知っているか？, 平凡社, 月刊百科, 1998 年 2 月号 No.424, p.14−15 および 3 月号 No.425, p.42−43
* 黒木 玄 (1998) ソーカル事件, 大学の物理教育，Vol.2, 23-28.
* ソーカル，アラン & ブリクモン，ジャン, (田崎，大野，堀 訳) (2012)，[__「知」の欺瞞 ---ポストモダン思想における科学の濫用__](https://www.iwanami.co.jp/book/b255892.html){:target="_blank"}, 岩波現代文庫/学術 261, 岩波書店.


## [基盤モデル LxM ウィキペディアより](https://en.wikipedia.org/wiki/Foundation_model)


人工知能（AI）において、基盤モデル（FM）は、大規模Xモデル（LxM）とも呼ばれ、膨大なデータセットで訓練された機械学習または深層学習モデルであり、幅広いユースケースに適用可能である[1]。大規模言語モデル（LLM）のような生成 AI アプリケーションは、基盤モデルの代表的な例である[1]。
<!-- In artificial intelligence (AI), a foundation model (FM), also known as large X model (LxM), is a machine learning or deep learning model trained on vast datasets so that it can be applied across a wide range of use cases.[1] Generative AI applications like large language models (LLM) are common examples of foundation models.[1]-->

基盤モデルの構築には通常、膨大なリソースが必要であり、最先端モデルでは大規模データセットの取得・管理・処理費用に加え、訓練に必要な計算リソースのコストとして数億ドル規模の費用がかかる[2]。 こうしたコストは、高度なインフラ、長期にわたる訓練グ時間、GPU などの先進的なハードウェアの必要性から生じる。これに対し、既存の基盤モデルを特定の課題に適応させるか、直接利用する方法は、事前学習済み能力を活用し、通常は小規模な課題特化データセットでの微調整のみを必要とするため、はるかに低コストである。
<!-- Building foundation models is often highly resource-intensive, with the most advanced models costing hundreds of millions of dollars to cover the expenses of acquiring, curating, and processing massive datasets, as well as the compute power required for training.[2] These costs stem from the need for sophisticated infrastructure, extended training times, and advanced hardware, such as GPUs. In contrast, adapting an existing foundation model for a specific task or using it directly is far less costly, as it leverages pre-trained capabilities and typically requires only fine-tuning on smaller, task-specific datasets. -->

基盤モデルの初期例としては、OpenAI の GPT シリーズや Google の BERT といった言語モデル（LM）が挙げられる[3][4]。テキスト以外にも、画像処理向けの DALL-E や Flamingo[5]、音楽生成向けの MusicGen[6] や LLark[7]、ロボット制御向けの RT-2[8] など、多様なモダリティで基盤モデルが開発されている。基礎モデルは天文学[9]、放射線医学[10]、ゲノミクス[11]、コーディング[12]、時系列予測[13]、数学[14]、化学[15]などの分野でも開発が進められている。
<!--Early examples of foundation models are language models (LMs) like OpenAI's GPT series and Google's BERT.[3][4] Beyond text, foundation models have been developed across a range of modalities—including DALL-E and Flamingo[5] for images, MusicGen[6] and LLark[7] for music, and RT-2[8] for robotic control. Foundation models are also being developed for fields like astronomy,[9] radiology,[10] genomics,[11] coding,[12] times-series forecasting,[13] mathematics,[14] and chemistry.[15] -->


* [1] Competition and Markets Authority (2023). AI Foundation Models: Initial Report. Available at: https://assets.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_.pdf
* [2]  Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, and Raymond Perrault, "The AI Index 2023 Annual Report," AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2023.
* [3] Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). "A Primer in BERTology: What we know about how BERT works". [arXiv:2002.12327 [cs.CL]](https://arxiv.org/abs/2002.12327){:target="_blank"}.
* [4] [Haddad, Mohammed. "How does GPT-4 work and how can you start using it in ChatGPT?"](https://www.aljazeera.com/news/2023/3/15/how-do-ai-models-like-gpt-4-work-and-how-can-you-start-using-it){:target="_blank"}. Al Jazeera. Retrieved 20 October 2024.
* [5] [Tackling multiple tasks with a single visual language model](https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/){:target="_blank"}, 28 April 2022, retrieved 13 June 2022, [Flamingo](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf)
* [6] Copet, Jade; Kreuk, Felix; Gat, Itai; Remez, Tal; Kant, David; Synnaeve, Gabriel; Adi, Yossi; Défossez, Alexandre (7 November 2023). "Simple and Controllable Music Generation". arXiv:2306.05284 [cs.SD].
* [7] Engineering, Spotify (13 October 2023). "LLark: A Multimodal Foundation Model for Music". Spotify Research. Retrieved 11 December 2023.
* [8] "Speaking robot: Our new AI model translates vision and language into robotic actions". Google. 28 July 2023. Retrieved 11 December 2023.
* [9] Nguyen, Tuan Dung; Ting, Yuan-Sen; Ciucă, Ioana; O'Neill, Charlie; Sun, Ze-Chang; Jabłońska, Maja; Kruk, Sandor; Perkowski, Ernest; Miller, Jack (12 September 2023). "AstroLLaMA: Towards Specialized Foundation Models in Astronomy". arXiv:2309.06126 [astro-ph.IM].
* [10] Tu, Tao; Azizi, Shekoofeh; Driess, Danny; Schaekermann, Mike; Amin, Mohamed; Chang, Pi-Chuan; Carroll, Andrew; Lau, Chuck; Tanno, Ryutaro (26 July 2023). "Towards Generalist Biomedical AI". [arXiv:2307.14334 [cs.CL]](https://arxiv.org/abs/2307.14334){:target="_blank"}.
* [11] Zvyagin, Maxim; Brace, Alexander; Hippe, Kyle; Deng, Yuntian; Zhang, Bin; Bohorquez, Cindy Orozco; Clyde, Austin; Kale, Bharat; Perez-Rivera, Danilo (11 October 2022). "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics". bioRxiv 10.1101/2022.10.10.511571.
* [12] Li, Raymond; Allal, Loubna Ben; Zi, Yangtian; Muennighoff, Niklas; Kocetkov, Denis; Mou, Chenghao; Marone, Marc; Akiki, Christopher; Li, Jia (9 May 2023). "StarCoder: may the source be with you!". [arXiv:2305.06161 [cs.CL]](https://arxiv.org/abs/2305.06161){:target="_blank"}.
* [13] Se, Ksenia; Spektor, Ian (5 April 2024). "[Revolutionizing Time Series Forecasting: Interview with TimeGPT's creators](https://www.turingpost.com/p/timegpt){:target="_blank"}". Turing Post. Retrieved 11 April 2024.
* [14] Azerbayev, Zhangir; Schoelkopf, Hailey; Paster, Keiran; Santos, Marco Dos; McAleer, Stephen; Jiang, Albert Q.; Deng, Jia; Biderman, Stella; Welleck, Sean (30 November 2023). "Llemma: An Open Language Model For Mathematics". [arXiv:2310.10631 [cs.CL]](https://arxiv.org/abs/2310.10631){:target="_blank"}.
* [15] "[Orbital](https://www.orbitalmaterials.com/post/technical-blog-introducing-the-orb-ai-based-interatomic-potential){:target="_blank"}".


### 定義<!-- ### Definitions-->

スタンフォード大学人間中心人工知能研究所（HAI）の基盤モデル研究センター（CRFM）は、2021 年 8月に「基盤モデル」という用語を提唱した[16]。これは「広範なデータ（一般的に大規模な自己教師あり学習を用いて）で訓練され、多様な下流課題に適応（例：微調整）可能なモデル」を意味する[17]。 これは既存の用語が重複しつつも不十分であるという観察に基づくもので、「（大規模）言語モデル」は焦点が言語だけに限られない点で狭すぎる、「自己教師ありモデル」は学習目的に特化しすぎている、「事前学習済み pretrained モデル」は注目すべき処理が全て「事前学習」後に発生する印象を与える、と指摘している[18]。「基礎モデル fundational model」ではなく「基盤モデル fundation model」という用語が選ばれた[19]のは、「基礎的 foundational」という語が「基盤的 foundation」とは異なり、これらのモデルが根本原理を提供する意味合いを暗示するためである[20]。
<!--The Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) coined the term "foundation model" in August 2021[16] to mean "any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks".[17] This was based on their observation that preexisting terms, while overlapping, were not adequate, stating that "'(large) language model' was too narrow given [the] focus is not only language; 'self-supervised model' was too specific to the training objective; and 'pretrained model' suggested that the noteworthy action all happened after 'pretraining."[18] The term "foundation model" was chosen over "foundational model"[19] because "foundational" implies that these models provide fundamental principles in a way that "foundation" does not.[20] -->


<!-- As governments regulate foundation models, new legal definitions have emerged.

* In the United States, the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence defines a foundation model as "an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts".[21]
* In the United States, the proposed AI Foundation Model Transparency Act of 2023[22] by House Representatives Don Beyer (D, VA) and Anna Eshoo (D, CA) defines a foundation model as "an artificial intelligence model trained on broad data, generally uses self supervision, generally contains at least 1,000,000,000 parameters, is applicable across a wide range of contexts, and exhibits, or could be easily modified to exhibit, high levels of performance at tasks that could pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters."
* In the European Union, the European Parliament's negotiated position on the E.U. AI Act defines a foundation model as an "AI model that is trained on broad data at scale, is designed for generality of output, and can be adapted to a wide range of distinctive tasks".
* In the United Kingdom, the Competition and Markets Authority's AI Foundation Models: Initial Report [1] defines foundations model as "a type of AI technology that are trained on vast amounts of data that can be adapted to a wide range of tasks and operations." -->

<!-- ### 歴史 -->
<!-- ### History-->

技術的には、基盤モデルは[深層ニューラルネットワーク](https://en.wikipedia.org/wiki/Deep_neural_networks)、[転移学習](https://en.wikipedia.org/wiki/Transfer_learning)、[自己教師あり学習](https://en.wikipedia.org/wiki/Self-supervised_learning) といった確立された機械学習技術を用いて構築される。基盤モデルは、特注の単発タスク特化モデルではなく、再利用可能な基盤として機能する汎用モデルである点で、従来の技術とは異なる。
<!-- Technologically, foundation models are built using established machine learning techniques like deep neural networks, transfer learning, and self-supervised learning. Foundation models differ from previous techniques as they are general purpose models that function as a reusable infrastructure, instead of bespoke and one-off task-specific models. -->

コンピュータ並列処理（例：CUDA GPU）の進歩、ニューラルネットワークアーキテクチャの新展開（例：[Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)){:target="_blank"}）、最小限の教師付きデータを用いた訓練データの増加が、基盤モデルの台頭に寄与した。基盤モデルは 2010 年代後半、深層学習モデルの新たな潮流として具体化し始めた[23]。 従来の深層学習研究の大半と比較して、これらの言語モデルは自己教師あり学習目標（例：大規模テキストコーパスにおける次単語予測）を用いた、Web 由来の大規模データセットでの訓練の可能性を示した。[word2vec](https://en.wikipedia.org/wiki/Word2vec) や [GloVe](https://en.wikipedia.org/wiki/GloVe) といった先行研究を基盤とするこれらの手法は、注釈付きデータ（例：クラウドソーシングによるラベル）を必要とした従来の教師あり学習アプローチから逸脱したものだった。
<!-- Advances in computer parallelism (e.g., CUDA GPUs) and new developments in neural network architecture (e.g., Transformers), and the increased use of training data with minimal supervision all contributed to the rise of foundation models. Foundation models began to materialize as the latest wave of deep learning models in the late 2010s.[23] Relative to most prior work on deep learning, these language models demonstrated the potential of training on much larger web-sourced datasets using self-supervised objectives (e.g. predicting the next word in a large corpus of text). These approaches, which draw upon earlier works like word2vec and GloVe, deviated from prior supervised approaches that required annotated data (e.g. crowd-sourced labels). -->

2022 年にリリースされた [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion) と [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT)（当初は GPT-3.5 モデルで駆動）は、基盤モデルと生成 AI が広く公の議論の対象となる契機となった。さらに 2023 年の [LLaMA](https://en.wikipedia.org/wiki/LLaMA)、Llama 2、[Mistral](https://en.wikipedia.org/wiki/Mistral_AI) のリリースは、基盤モデルの公開方法への注目を高め、オープンな基盤モデルが多くの支持[24]と精査[25]を集める結果をもたらした。
<!--The 2022 releases of Stable Diffusion and ChatGPT (initially powered by the GPT-3.5 model) led to foundation models and generative AI entering widespread public discourse. Further, releases of LLaMA, Llama 2, and Mistral in 2023 contributed to a greater emphasis placed on how foundation models are released with open foundation models garnering a lot of support[24] and scrutiny.[25] -->

* [23] Liang, Percy; Bommasani, Rishi; Lee, Tony; Tsipras, Dimitris; Soylu, Dilara; Yasunaga, Michihiro; Zhang, Yian; Narayanan, Deepak; Wu, Yuhuai (1 October 2023), "Holistic Evaluation of Language Models", Annals of the New York Academy of Sciences, 1525 (1): 140–146, arXiv:2211.09110, Bibcode:2023NYASA1525..140B, doi:10.1111/nyas.15007, PMID 37230490
* [24] "Joint Statement on AI Safety and Openness". Mozilla. 31 October 2023. Retrieved 12 February 2024.
* [25] "Hawley and Blumenthal Demand Answers from Meta, Warn of Misuse After 'Leak' of Meta's AI Model". Senator Josh Hawley. 6 June 2023. Retrieved 12 February 2024.


<!-- ### 世界モデル -->
<!-- ### World models-->

<!-- 2018 年、研究者 David Ha と [Jugen Schmithuber](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber) は[強化学習](https://en.wikipedia.org/wiki/Reinforcement_learning)の文脈で世界モデルを定義した。これは視覚的観測を表現する[変分自己符号化器モデル V](https://en.wikipedia.org/wiki/Variational_autoencoder)、記憶を表現する[再帰型ニューラルネットワークモデル M](https://en.wikipedia.org/wiki/Recurrent_neural_network)、意思決定を行う[線形モデル](https://en.wikipedia.org/wiki/Linear_model) C を備えたエージェントである。彼らは、現実をシミュレートする環境でワールドモデルを用いて訓練されたエージェントが実世界の設定に適用可能であると示唆した[33]。 -->
<!--In 2018, researchers David Ha and Jürgen Schmidhuber defined world models in the context of reinforcement learning: an agent with a variational autoencoder model V for representing visual observations, a recurrent neural network model M for representing memory, and a linear model C for making decisions. They suggested that agents trained on world models in environments that simulate reality could be applied to real world settings.[33] -->

<!-- 2022 年、[Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) は世界モデル（彼が定義するところでは、関連性があると見なされる世界の側面に対するメンタルモデルとして機能する[ニューラルネットワーク](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))）を、より大きな[認知アーキテクチャ](https://en.wikipedia.org/wiki/Cognitive_architecture)システムの一部として捉えた。このシステムは、[脳](https://en.wikipedia.org/wiki/Brain)の異なる領域に類似した他のニューラルネットワークで構成される。彼の見解では、この枠組みは[常識的推論](https://en.wikipedia.org/wiki/Commonsense_reasoning)につながる可能性がある[34][35]。 -->
<!-- In 2022, Yann LeCun saw a world model (defined by him as a neural network that acts as a mental model for aspects of the world that are seen as relevant) as part of a larger system of cognitive architecture – other neural networks that are analogous to different regions of the brain. In his view, this framework could lead to commonsense reasoning.[34][35] -->

<!-- 世界モデルは、テキスト、画像、音声、動画など様々なデータモダリティで学習され、[動画生成](https://en.wikipedia.org/wiki/Text-to-video_model)に応用されている[36]。 -->
<!-- World models are trained on a variety of data modalities, including text, images, audio and video, and have been applied to video generation.[36] -->

<!-- [TechCrunch](https://en.wikipedia.org/wiki/TechCrunch) は、世界モデルは[大規模言語モデル](https://en.wikipedia.org/wiki/Large_language_model)よりも多くのデータを使用し、計算能力が大幅に必要となる（訓練と推論に数千の GPU を使用することを含む）と指摘した[35][36]。 また、[幻覚](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))、[カバレッジバイアス](https://en.wikipedia.org/wiki/Coverage_error)、[アルゴリズムバイアス](https://en.wikipedia.org/wiki/Algorithmic_bias)のリスクにも言及した。[36] -->
<!-- TechCrunch noted that world models could use more data than large language models and would require significantly more computational power (including the use of thousands of GPUs for training and inference).[35][36] It also noted the risk of hallucinations, coverage bias and algorithmic bias.[36] -->

<!-- TechCruch は [Sora](https://en.wikipedia.org/wiki/Sora_(text-to-video_model)) を世界モデルの事例と見なした[36]。一方、2025 年 1 月には NVIDIA が独自の世界モデル群をリリースした[37][38]。 サウスチャイナ・モーニング・ポスト紙は、Manycore Tech が[空間知能](https://en.wikipedia.org/wiki/Location_intelligence)の事例として自社技術を位置付け、世界モデル構築を目指す企業のもう一つの例だと報じた[39]。2025 年 5 月には、モハメド・ビン・ザーイド人工知能大学が [AI エージェント](https://en.wikipedia.org/wiki/Intelligent_agent) のテスト用シミュレーション構築向け世界モデルを発表した[40]。 -->
<!-- TechCrunch saw Sora as an example of a world model,[36] while in January 2025, Nvidia released its own set of world models.[37][38] The South China Morning Post wrote that Manycore Tech was another example of companies aiming to build a world model, viewing their work as an example of spatial intelligence.[39] In May 2025, Mohamed bin Zayed University of Artificial Intelligence released a world model for building simulations to test AI agents.[40] -->

<!-- Google DeepMind はまた、動画データで訓練された二次元空間と三次元空間の 2 つの世界モデルをそれぞれ公開しており、Google は後者が AI エージェントの訓練環境となり得ると主張している[41][42]。 -->
<!-- Google DeepMind has also released two world models in two-dimensional space and three-dimensional space, respectively, that were trained on video data, with Google claiming that the latter can be a training environment for AI agents.[41][42] -->

<!-- Fei-Fei Li は、世界モデルがロボティクスや創造的作業に応用されるとの見解を示している。これらのモデルの複雑さゆえに、彼女はデータ取得、データエンジニアリング、データ処理、データ合成においてより複雑な戦略を提唱している[43]。 彼女は世界モデル構築を目的としたスタートアップを共同設立し、2024 年時点で以下の 3 段階での実現を計画していた：時間と組み合わされた三次元空間の理解の組み込み、拡張現実（AR）のサポート、ロボティクスのサポート[44]。 -->
<!-- Fei-Fei Li views world models as applying to robotics and creative works. Due to the complexity of these models, she advocates for more complex strategies in data acquisition, data engineering, data processing, and synthesizing data.[43] She co-founded a startup on building world models, which, as of 2024, planned to do so in three phases: incorporating an understanding of three-dimensional space along with time; support for augmented reality; and support for robotics.[44] -->

<!-- 世界モデルはインタラクティブメディアや環境シミュレーションでの使用を目的としている。クリエイティブ分野の専門家らは、世界モデルが自業界の雇用を脅かす可能性を懸念している[45]。Wired 誌はワールドモデルをメタバースと比較し[44]、Business Insider誌は軍事応用可能性に言及した[43]。 -->
<!-- World models are intended for use in interactive media and environment simulation. Creative professionals have expressed concern that world models could disrupt jobs in their industries.[45] Wired compared world models to the metaverse,[44] while Business Insider noted possible military applications.[43] -->

* [33] Ha, David; Schmidhuber, Jürgen (3 December 2018). "Recurrent world models facilitate policy evolution". Proceedings of the 32nd International Conference on Neural Information Processing Systems. NIPS'18. Red Hook, NY, USA: Curran Associates Inc.: 2455–2467.
* [34] Heikkilä, Melissa; Douglas Heaven, Will (24 June 2022). "Yann LeCun has a bold new vision for the future of AI". MIT Technology Review. Archived from the original on 24 June 2022. Retrieved 15 June 2025.
* [35] Zeff, Maxwell (17 October 2024). "Meta's AI chief says world models are key to 'human-level AI' — but it might be 10 years out". TechCrunch. Archived from the original on 4 March 2025. Retrieved 15 June 2025.
* [36] Wiggers, Kyle (14 December 2024). "What are AI 'world models,' and why do they matter?". TechCrunch. Archived from the original on 18 March 2025. Retrieved 15 June 2025.
* [37] Wiggers, Kyle (7 January 2025). "Nvidia releases its own brand of world models". TechCrunch. Archived from the original on 5 March 2025. Retrieved 15 June 2025.
* [38] Takahashi, Dean (7 January 2025). "Nvidia launches Cosmos world foundation model platform to accelerate physical AI". VentureBeat. Archived from the original on 8 January 2025. Retrieved 15 June 2025.
* [39] Chen, Wency (5 May 2025). "China's answer to Autodesk is betting on AI to build a 'world model'". South China Morning Post. Archived from the original on 5 May 2025. Retrieved 15 June 2025.
* [40] Knight, Will. "A United Arab Emirates Lab Announces Frontier AI Projects—and a New Outpost in Silicon Valley". Wired. ISSN 1059-1028. Archived from the original on 22 May 2025. Retrieved 15 June 2025.
* [41] Orland, Kyle (5 March 2024). "Google's Genie game maker is what happens when AI watches 30K hrs of video games". Ars Technica. Archived from the original on 8 December 2024. Retrieved 15 June 2025.
* [42] Orland, Kyle (6 December 2024). "Google's Genie 2 "world model" reveal leaves more questions than answers". Ars Technica. Archived from the original on 7 December 2024. Retrieved 15 June 2025.
* [43] Varanasi, Lakshmi. "Top AI researchers say language is limiting. Here's the new kind of model they are building instead". Business Insider. Archived from the original on 15 June 2025. Retrieved 21 June 2025.
* [44] Levy, Steven (13 September 2024). "The Godmother of AI Wants Everyone to Be a World Builder". Wired. ISSN 1059-1028. Archived from the original on 7 February 2025. Retrieved 21 June 2025.
* [45] Wiggers, Kyle (28 May 2025). "Odyssey's new AI model streams 3D interactive worlds". TechCrunch. Archived from the original on 30 May 2025. Retrieved 15 June 2025.


---


<!-- Large X model (LxM) としても知られる基盤モデルは，幅広いユースケースに適用できるように，膨大なデータセットで学習される機械学習または深層学習モデルである[1]。
大規模言語モデルのような生成 AI アプリケーションは，しばしば基盤モデルの例である[1]。 -->
<!-- A foundation model, also known as large X model (LxM), is a machine learning or deep learning model that is trained on vast datasets so it can be applied across a wide range of use cases.[1]
Generative AI applications like Large Language Models are often examples of foundation models.[1]-->

<!-- 基盤モデル (LxM) は，深層学習ネットワーク，転移学習，自己教師あり学習のような確立された機械学習技術を使用して構築される。
基盤モデルは，特注の単一課題に特化したモデルではなく，再利用可能なインフラとして機能する汎用モデルである点が，これまでの技術とは異なる。 -->
<!-- Technologically, foundation models are built using established machine learning techniques like deep neural networks, transfer learning, and self-supervised learning. Foundation models differ from previous techniques as they are general purpose models function as a reusable infrastructure, instead of bespoke and one-off task-specific models.-->

<!-- コンピュータの並列処理 (CUDA GPU など) の進歩や，ニューラルネットワークアーキテクチャ (Transformers など) の新展開，最小限の監視で学習データを使用するようになったことなどが，基盤モデルの台頭に貢献した。
基礎モデルは，2010 年代後半に深層学習モデルの最新 の波として具体化し始めた(23)。
深層学習に関する先行研究のほとんどと比較して，これらの言語モデルは，自己教師ありの目的（例えば，大規模なテキストコーパスの次の単語を予測する）を使用して，はるかに大規模なウェブソースデータセットで学習する可能性を示した。
これらのアプローチは，word2vec や GloVe のような以前の研究に基づくもので，注釈付きデータ（例えばクラウドソースラベル）を必要とする以前の教師ありアプローチから逸脱している。 -->
<!-- Advances in computer parallelism (e.g., CUDA GPUs) and new developments in neural network architecture (e.g., Transformers), and the increased use of training data with minimal supervision all contributed to the rise of foundation models.
Foundation models began to materialize as the latest wave of deep learning models in the late 2010s.[23]
Relative to most prior work on deep learning, these language models demonstrated the potential of training on much larger web-sourced datasets using self-supervised objectives (e.g. predicting the next word in a large corpus of text).
These approaches, which draw upon earlier works like word2vec and GloVe, deviated from prior supervised approaches that required annotated data (e.g. crowd-sourced labels). -->

<!-- 2022 年にリリースされた Stable Diffusion と ChatGPT（当初は GPT-3.5 モデルを搭載）は，基礎モデルと生成 AI が広く一般に普及するきっかけとなった。
さらに，2023 年の LLaMA, Llama2, Mistral のリリースは，オープンな基盤モデルが多くの支持[24] と精査を集めることで，基盤モデルがどのようにリリースされるかがより重視されることに貢献した[25]。 -->
<!--The 2022 releases of Stable Diffusion and ChatGPT (initially powered by the GPT-3.5 model) led to foundation models and generative AI entering widespread public discourse. Further, releases of LLaMA, Llama 2, and Mistral in 2023 contributed to a greater emphasis placed on how foundation models are released with open foundation models garnering a lot of support[24] and scrutiny.[25] -->

<!-- 基盤モデルの構築は，多くの場合，非常にリソース集約的であり，最も高度なモデルでは，膨大なデータセットの取得，キュレーション，処理にかかる費用と，学習に必要な計算能力を賄うために，数億ドルのコストがかかる[2]。
これらのコストは，洗練されたインフラストラクチャ，訓練時間の延長，GPU などの高度なハードウェアの必要性から生じている。
対照的に，既存の基盤モデルを特定のタスクに適合させたり，直接使用したりすることは，事前に訓練された能力を活用するため，はるかにコストがかからず，通常，小規模な課題固有のデータセットで微調整を行うだけで済む。 -->
<!-- Building foundation models is often highly resource-intensive, with the most advanced models costing hundreds of millions of dollars to cover the expenses of acquiring, curating, and processing massive datasets, as well as the compute power required for training.[2]
These costs stem from the need for sophisticated infrastructure, extended training times, and advanced hardware, such as GPUs.
In contrast, adapting an existing foundation model for a specific task or using it directly is far less costly, as it leverages pre-trained capabilities and typically requires only fine-tuning on smaller, task-specific datasets. -->

基盤モデルの初期の例は，OpenAI の GPT シリーズや Google の BERT のような言語モデル（LM）である[3,4]。
テキスト以外にも，画像の DALL-E や Flamingo[5]，音楽の MusicGen[6]，ロボット制御の RT-2[7] など，様々なモダリティの基盤モデルが開発されている。
また，天文学[8]，放射線学[9]，ゲノム学[10]，音楽[11]，コーディング[12]，時系列予測[13]，数学[14]，化学[15] などの分野でも基礎モデルが開発されている。
<!--Early examples of foundation models are language models (LMs) like OpenAI's GPT series and Google's BERT.[3][4]
Beyond text, foundation models have been developed across a range of modalities—including DALL-E and Flamingo[5] for images, MusicGen[6] for music, and RT-2[7] for robotic control.
Foundation models are also being developed for fields like astronomy,[8] radiology,[9] genomics,[10] music,[11] coding,[12] times-series forecasting,[13] mathematics,[14] and chemistry.[15] -->

スタンフォード人間中心人工知能研究所 (HAI) の基盤モデル研究センター (CRFM) は，2021 年 8 月 (16)に「基盤モデル」という用語を作り，「(一般的に大規模な自己教師を使って) 広範なデータで学習され，下流の幅広い課題に適応 (微調整など) 可能なあらゆるモデル」を意味するようになった(17)。
これは，既存の用語が重複するものの，適切ではないという観察に基づいており，「(大規模) 言語モデル」は，(焦点が) 言語だけではないことを考えると狭すぎる。
すなわt 事前学習モデル (pretrained model) は，注目すべき行動がすべて 事前学習後に起こったことを示唆している。
Foundational model ではなく foundation model  という用語が選ばれたのは，fundation model は fundational  (基礎的)  ではない方法で，これらのモデルが fundational principles (基本原理) を提供することを意味するからである。
<!-- The Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) coined the term "foundation model" in August 2021 to mean "any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks".
This was based on their observation that preexisting terms, while overlapping, were not adequate, stating that "'(large) language model' was too narrow given [the] focus is not only language; 'self-supervised model' was too specific to the training objective; and 'pretrained model' suggested that the noteworthy action all happened after 'pretraining.”
The term "foundation model" was chosen over "foundational model” because "foundational" implies that these models provide fundamental principles in a way that "foundation" does not. -->

政府が基盤モデルを規制するにつれ，新たな法的定義が生まれている。
<!-- As governments regulate foundation models, new legal definitions have emerged.-->

* 米国では，「人工知能の安全，確実かつ信頼できる開発および使用に関する大統領令」が，基盤モデルを「広範なデータに基づいて訓練され，一般的に自己監視を使用し，少なくとも数百億のパラメータを含み，広範な文脈にわたって適用可能なAIモデル」と定義している(21)。
* 米国では，Don Beyer 下院議員（バージニア州選出，民主党）と Anbna Eshoo 下院議員（カリフォルニア州選出，民主党）が提案した 2023 年 AI 基盤モデル透明化法(22)  は，基盤モデルを「広範なデータで訓練され，一般的に自己監視を使用し，一般的に少なくとも 1,000,000,000 のパラメータを含み，広範な文脈にわたって適用可能な人工知能モデル」と定義している(21)， また，安全保障，国家経済安全保障，国家公衆衛生もしくは安全，またはそれらの組み合わせに深刻なリスクをもたらす可能性のある課題において，高レベルの性能を示すか，または示すように容易に修正できる。
* 欧州連合 (EU) では，EU に関する欧州議会の交渉見解がある。
AI 法では，基盤モデルを「大規模データで学習され，出力の汎用性を考慮して設計され，幅広い特徴的な課題に適応できる AI モデル」と定義している。
* 英国では，競争市場庁の AI 基盤モデル：
英国では，競争市場庁の AI 基盤モデル：初期報告書[1]は，基盤モデルを 「膨大なデータで訓練され，幅広い課題や業務に適応できる AI 技術の一種」と定義している。

<!--* In the United States, the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence defines a foundation model as "an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts".[21]
* In the United States, the proposed AI Foundation Model Transparency Act of 2023[22] by House Representatives Don Beyer (D, VA) and Anna Eshoo (D, CA) defines a foundation model as "an artificial intelligence model trained on broad data, generally uses self supervision, generally contains at least 1,000,000,000 parameters, is applicable across a wide range of contexts, and exhibits, or could be easily modified to exhibit, high levels of performance at tasks that could pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters."
* In the European Union, the European Parliament's negotiated position on the E.U.
AI Act defines a foundation model as an "AI model that is trained on broad data at scale, is designed for generality of output, and can be adapted to a wide range of distinctive tasks".
* In the United Kingdom, the Competition and Markets Authority's AI Foundation Models:
Initial Report [1] defines foundations model as "a type of AI technology that are trained on vast amounts of data that can be adapted to a wide range of tasks and operations." -->

米国の定義は，基礎モデルの大きさについて言及している唯一のものであり，大きさについては異なっている。
また，Beyer と Eshoo の定義では，基礎モデルは潜在的な危険となるような性能レベルを達成しなければならないと規定している。
対照的に，EU の定義は，モデルが出力の汎用性を考慮して設計されていることを求めている。
どの定義も，基礎モデルは多くの領域で応用される可能性のある広範なデータで訓練されなければならないという点で一致している。
<!-- The United States's definitions are the only ones to make reference to the size of a foundation model, and differ on magnitude.
Beyer and Eshoo's definition also specifies that foundation models must achieve a level of performance as to be a potential danger.
In contrast, the E.U. definition requires the model to be designed for generality of output.
All definitions agree that foundation models must be trained on a broad range of data with potential applications in many domains. -->


<!-- 1. Competition and Markets Authority (2023). AI Foundation Models: Initial Report. Available at: [https://assets.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_.pdf](https://assets.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_.pdf)
2. Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, and Raymond Perrault, "The AI Index 2023 Annual Report," AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2023.
3. Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). "A Primer in BERTology: What we know about how BERT works". [arXiv:2002.12327](https://arxiv.org/abs/2002.12327).
4. Haddad, Mohammed. "[How does GPT-4 work and how can you start using it in ChatGPT?](https://www.aljazeera.com/news/2023/3/15/how-do-ai-models-like-gpt-4-work-and-how-can-you-start-using-it)". Al Jazeera. Retrieved 20 October 2024. -->

<!-- 16.  "[Introducing the Center for Research on Foundation Models (CRFM)](https://hai.stanford.edu/news/introducing-center-research-foundation-models-crfm)". Stanford HAI. 18 August 2021. Retrieved 11 June 2022.
17.  Bommasani, Rishi; et al. (18 August 2021). On the Opportunities and Risks of Foundation Models (Report). [arXiv:2108.07258](https://arxiv.org/abs/2108.07258).
18.   "[Reflections on Foundation Models](https://hai.stanford.edu/news/reflections-foundation-models)". Stanford HAI. 18 October 2021. Retrieved 22 May 2023.
19.   Bommasani, Rishi; Liang, Percy (18 October 2021). "[Reflections on Foundation Models](https://crfm.stanford.edu/2021/10/18/reflections.html)". Stanford CRFM. Retrieved 11 December 2023.
20.   Marcus, Gary (11 September 2021). "[Has AI found a new Foundation?](https://thegradient.pub/has-ai-found-a-new-foundation/)". The Gradient. Retrieved 11 December 2023. -->


### [DALL・E](https://arxiv.org/abs/1605.05396)
<!-- 
<div class="figcenter">
<img src="/2024assets/2025_0110chatGPT_DALLE1.jpg" style="width:33%;">
<Img src="/2024assets/2025_0110chatGPT_DALLE2.jpg" style="width:33%;">
</div> -->

<p align="center">
<img src="/2024assets/stable_diffusion.png" style="width:44%;">
<!-- <img src="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png" style="width:44%;"><br/> -->
<!-- <img src="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png" alt="sd-pipeline" width="500"/> -->
</p>


<div class="figcenter">
<img src="/2024assets/CLIP.png" style="width:77%;">
</div>
<div class="figcaption">
**CLIP の概要**<br/>
標準的な画像モデルが画像特徴抽出器と線形分類器を同時に学習し，何らかのラベルを予測するのに対し，CLIP は画像符号化器とテキスト符号化器を同時に学習し (画像とテキストの) バッチ学習例の正しいペアリングを予測する。
検証時に，学習したテキスト符号化器は，目標データセットのクラスの名前や説明を埋め込むことで，ゼロ撃の線形分類器を合成する。
<!-- Figure 1. Summary of our approach.
While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples.
At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes. -->
</div>

<div class="figcenter">
<img src="/assets/2015Ronneberger_U-Net_Fig1_ja.svg" style="width:55%;">
</div>


<div class="figcenter">
<img src="/2024assets/perturb_vp.gif" style="width:44%;">
<img src="/2024assets/denoise_vp.gif" style="width:44%;">
<div class="figcaption" style="width:44%;">
左: 連続時間確率過程によるデータのノイズへの摂動<!-- Perturbing data to noise with a continuous-time stochastic process. --><br/>
右: 摂動手順を逆にすることで，ノイズからデータを生成
<!-- Generate data from noise by reversing the perturbation procedure. -->
</div></div>

<div class="figcenter">
<img src="/2024assets/sde_schematic.jpg" style="width:55%;">
<!-- ![](../../assets/img/score/sde_schematic.jpg){.img-fluid .rounded .z-depth-1} -->
</div>
<div class="figcaption">
逆 SDE を解くと，スコアベースの生成モデルが得られる。
データを単純なノイズ分布に変換することは，SDE で達成できる。
各中間時間ステップにおける分布のスコアがわかれば，ノイズからサンプルを生成するために逆 SDE を解くことができる
。
<!-- Solving a reverse SDE yields a score-based generative model.
Transforming data to a simple noise distribution can be accomplished with an SDE.
It can be reversed to generate samples from noise if we know the score of the distribution at each intermediat
e time step. -->
</div>


<div class="figcenter">
<img src="/2024assets/teaser.jpg" style="width:77%;">
<!-- ![](../../assets/img/score/teaser.jpg){.img-fluid .rounded .z-depth-1} -->
<div class="figcaption">
SDE を使用してデータをノイズ分布 (事前分布) に写像し，この SDE を逆にして生成モデリングを行うことができる。
関連する確率フロー ODE を逆にすることもできる。
これにより，SDE と同じ分布からサンプリングする決定論的処理が生成される。
逆時間 SDE と確率フロー ODE はどちらも，スコア関数を推定することで取得できる。
<!-- We can map data to a noise distribution (the prior) with an SDE, and reverse this SDE for generative mode
ling.
We can also reverse the associated probability flow ODE, which yields a deterministic process that samples fro
m the same distribution as the SDE.
Both the reverse-time SDE and probability flow ODE can be obtained by estimating score functions. -->
</div></div>
