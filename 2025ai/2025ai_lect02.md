---
title: 2025 年度開講 駒澤大学 人工知能 I および II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">
<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 18/Apr/2025<br/>
Appache 2.0 license<br/><br/>
</div>

# 本日のメニュー

1. 人工知能概論
2. 東京技術計算コンサルタント様 ピッチ
3. 2024 年ノーベル物理学賞，化学賞の解説

   * インターンの募集について
   * 資格試験勉強会について

# 実習

* 前回，自身の Google ドライブに保存した `2024_0411NB9999.ipynb` を開いてファイル名を変更して保存。
* [課題提出用 Google Drive フォルダ](https://drive.google.com/drive/u/5/folders/1i5c4A2M3gOrY0K4d2DnKg9LBU8nDObZx){:target="_blank"}


# 1. 人工知能概論

<div class="figcenter">

<img src='/2025assets/2017Goodfelllow_Fig1_4rev.svg' width="66%"><br/>
Goodfellow et al. (2017) Fig.1 を改変
</div>

本授業の予定
1. 最初に機械学習を 3 回程度
2. ニューラルネットワークによる画像認識 ここまで前期
3. ニューラルネットワークによる自然言語処理
4. 強化学習


### 小説，戯曲の中に現れた AI


- フランケンシュタイン Frankenstein, or The Modern Prometheus [https://www.aozora.gr.jp/cards/001176/files/44904_35865.html](https://www.aozora.gr.jp/cards/001176/files/44904_35865.html){:target="_blank"} マリー・ウォルストンクラフト・シェリー　Mary Wallstoncraft Shelley，
- ロボット (ＲＵＲ ―ロッサム世界ロボット製作所 R.U.R. Rossum's Universal Robots) [https://www.aozora.gr.jp/cards/001236/files/46345_23174.html](https://www.aozora.gr.jp/cards/001236/files/46345_23174.html){:target="_blank"} カレル・チャペック　Karel Capek,
- ロボット三原則 アイザック・アシモフ Issac Asimov, われはロボット I, Robot [https://www.amazon.co.jp/dp/4150105359](https://www.amazon.co.jp/dp/4150105359){:target="_blank"}
  1. ロボットは人間に危害を加えてはならない。また，危険を看過することによって，人間に危害を及ぼしてはならない。
  2. ロボットは人間に与えられた命令に服従しなければならない。ただし与えられた命令が第一条に反する場合は，この限りではない。
  3. ロボットは，前掲第一条および第二条に反するおそれのない限り，自己を守らねばならない。
- HAL 9000, 2001 年宇宙の旅 2001: A Space Odyssey [https://www.amazon.co.jp/dp/415011000X](https://www.amazon.co.jp/dp/415011000X){:target="_blank"}，アーサー・クラーク Arthur C. Clarke,

- クラークの三法則
  1. 高名で年配の科学者が可能であると言った場合、その主張はほぼ間違いない。また不可能であると言った場合には、その主張はまず間違っている。<!--When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong. -->
  2. 可能性の限界を測る唯一の方法は、その限界を少しだけ超越するまで挑戦することである。<!--The only way of discovering the limits of the possible is to venture a little way past them into the impossible. -->
  3. 十分に発達した科学技術は、魔法と見分けがつかない。<!--Any sufficiently advanced technology is indistinguishable from magic. -->

### 人工知能 (AI) とは何か

- 「人工知能の基礎」（小林 一郎）
  - 人の知能，つまり，人が行なう知的作業は，推論，記憶，認識，理解，学習，創造といった現実世界に適応するための能力を指す．人工の「知能」とは，人の「知能」のある部分を機械に行わせることによって創られる．
- デジタル大辞泉 《artificial intelligence》： コンピューターで，記憶・推論・判断・学習など，人間の知的機能を代行できるようにモデル化されたソフトウエア・システム．AI．

シャピロ (Shapiro1992) は次の 3 つの分野だと書いている。

1. 計算論的心理学 Computational Psychology: __人間の知的活動を理解するために人間のように振る舞うコンピュータプログラムを作ること__
2. 計算論的哲学 Computational Philosophy: __人間レベルの知的活動を計算論的に理解すること。計算論的理解=コンピュータ上に実装可能なモデル__
3. 計算機科学 Advanced Computer Science: __コンピュータ科学の拡張，発展。現在のコンピュータはプログラムされたことしか実行できないが，人間はプログラムされていなくても勝手に振る舞う。__

   * Shapiro, Stuart C. (1992), "Artificial Intelligence", in Stuart C. Shapiro (ed.), Encyclopedia of Artificial Intelligence, 2nd edition (New York: John Wiley & Sons)


### 人工知能の 2 潮流と心理学の歴史

ダートマス会議で人工知能 (AI) という用語が提唱された。

|                         |   記号処理系 (非ニューラルネットワーク)      | ニューラルネットワーク|   心理学       |
|:------------------------|:----------------:|:------------------:|:--------------------:|
|第一次 AI ブーム 1950-     |  記号処理         | パーセプトロン        | 認知革命 |
|                         |  オモチャ問題      | ADALINE            |                     |
|                         |                  | ネオコグニトロン      |                      |
|                         |                  | アソシアトロン        |                      |
| AI の 冬|
|第二次 AI ブーム1980-      | エキスパートシステム  | 誤差逆伝播法         | コネクショニスト     |
|                         | Brooks           | リカレントニューラルネットワーク  | 脳画像研究       |
|                         |                  | 強化学習  | 計算論的アプローチ |
| AI の 冬|
|第三次 AI ブーム 2010-     |                  | ディープラーニング    |                      |
|                         |                  | 畳み込みニューラルネットワーク  |                      |
|                         |                  | ベクトル埋め込みモデル         |                      |
|                         |                  | DQN                        |                      |
|                         |                  | Transformer               |                      |
|                         |                  | BERT, GPT, T5            |                      |

### 浅川の妄想 (あるいは暴言)

ダートマス会議 (1955) において，で初めて人工知能 (AI) という言葉が提唱された。
だが，ダートマス会議以前から，知的な振る舞いを自動化 (機械化) しようとする動きは存在していた。

ダートマス会議における AI 研究の始まりが有名だが，人工知能 (AI) という用語を生み出さざるを得なかった背景には **サイバネティクス (Cybernetics)** が影響しているのだろうと推察している。
つまり，ダートマス会議に集った参加者たちは，マーケティング，あるいはポジショントークとして，サイバネティクスとは異なる分野であることを強調する必要があったのだろう。
そうすれば，研究費獲得，就職先確保などの利益が得られると予想されるからである。

サイバネティクスとは，ノーバート・ウィナー (Nobert Wiener) が提唱した自動制御の分野である。
サイバネティクスは，ダートマス会議の時代背景として，学問的，思想的に影響が甚大であった。
現在でもサイバー空間などという用語が用いられることがあるが，サイバネティクスの影響が今でも残っている証拠である。

20 世紀の科学の成果，あるいは特徴として，衆目の認める大きな成果としては以下のものが挙げられるだろう：

1. 相対性理論，量子力学などの物理学の発展と深化。核兵器，核エネルギーの利用，量子論の人文科学への影響。シュレディンガーの殺猫問題。ディラックの海。
2. コンピュータサイエンス，計算機技術，通信理論の発展と一般への普及。電話，ラジオ，映画，からテレビ，インターネット，
3. 分子生物学の始まりとその後の展開。遺伝情報の基本要素あるいはアルファベットとしての 4 つのデオキシリボ核酸の発見，そこから生命現象を説明する試み。誕生と死滅の神秘を解き明かそうとする試み。
4. 電気工学の発展。冷蔵庫，エアコン，電気鉄道，電気通信，電気化学，など日常生活に密着した技術の発展。

21 世紀は，予測不能だが，19 世紀の科学の特徴と比較すると 20 世紀の科学の特徴の中に，AI もサイバネティクスも入ってこないのは，コンピュータサイエンスの中に包含されていると考えても良いのだろう。

ちなみに 19 世紀の科学の 3 つを挙げるとすれば，

1. 熱力学，統計力学による物理学の進歩と応用。蒸気機関の理論的深化，精緻化 (第一次産業革命の科学的理論づけによる応用，発展)。列強によるアフリカの植民地化，アジア進出。
2. 電磁気学の進化による第二次産業革命の準備。エネルギー革命。蒸気機関から内燃機関，電気機関への変換。
3. 進化論の提唱。人間とチンパンジーは隣人であり，人は神と天使から始まる宗教上のピラミッドの 2 段目に位置し，それ以外の生物よりも上の階層に位置するという宗教観を考え直す。

19 世紀の特徴の中に心理学の誕生は含まれない。

### 心理学との関連

心理学の歴史を考えると，
- 1950年代 の認知革命，から認知科学が生まれ，
- 1980年代 の脳画像技術の進歩に伴って，神経科学との接点が強調されるようになりました。
- 2000年代以降の データサイエンスの流行に伴って，データを持って語ることが多くなりました。
- 2014年 ディープラーニングによる画像認識技術が注目を浴びるようになりました。
- このような背景から，心理学とAI，データサイエンスとの融合が数多く試みられるようになっています。
- 2019 年以降，ディープラーニングと神経科学と認知科学との三者関係を論じる風潮がある。

人工知能 は知的機能をコンピュータで作ろうとする **構成論的研究**。
浅川にはむしろ人工知能研究者の方が，人間の心を真摯に向き合っているようにも思える。

* **認知科学** や **神経科学** は 時代ごとに流行があり，その都度変わってきた。
* **行動主義** は 心的機能をブラックボックスとみなし，観察可能な行動のみに注目した。
    <!-- * $\rightarrow$ 心的機能を説明するための明示的な計算モデルを提案できていません。 -->
* **認知心理学** は **行動主義** がブラックボックスとみなしていた心的機能を，情報処理の用語と理論を援用して理解しようした。
    <!-- * $\rightarrow$ 明示的な計算モデルを提案できていません。 -->
* **認知科学** は 情報処理理論をさらに進めることを提案。
    * $\rightarrow$ 神経生理学的なデータによる制約がないため、行動データと一致する複数の代替モデルを判断することが困難。
* **コネクショニズム** は 神経生物学的に妥当な計算の枠組みを提供しました
    * 第２世代までのニューラルネットワークの技術は 現実的な課題に取り組むには十分ではなかった。
    * その結果 AI システムとしての期待に応えることができず、認知科学ではモデル化は **おもちゃの問題** に限定。
* **認知神経科学** では 神経生理学的なデータを用いて 脳の情報処理を研究。
    * $\rightarrow$ 複雑な脳画像データの解析という新たな課題に手を焼いているうちに、理論的な洗練度は認知心理学の段階に後退し、**箱と矢印モデル** を脳領域との対応とを考える。
* **計算論的神経科学** では 生物学的妥当性を持つ **計算モデル** を用いて 神経生理学的データや行動学的データを記述，予測しようとします。
    * $\rightarrow$ 現実世界の複雑な計算課題や，より高度な脳の表現に取り組むことは難しい。
* **深層学習 ディープニューラルネットワーク (ディープラーニング)** は 複雑な認知課題に取り組み 脳と行動の両方の反応を予測するための枠組。

* 行動主義
* 認知心理学
* 認知科学
* 生理学
* 機能的脳画像


### AI の分野

1. 推論，問題解決 Reasoning, problem solving
1. 知識表象 Knowledge representation
1. 計画 Planning
1. **学習 Learning**
1. **自然言語処理 Natural language processing**
1. **認識 Perception**
1. **ロボティクス Motion and manipulation**
1. 社会知能 Social intelligence
1. 創造性 Creativity
1. 一般知能 General intelligence


### AI 進歩の 5 つの要因<!-- Karpathy Deep Reinforcement Learning: Pong from Pixels -->

1. 計算能力の向上 (ムーアの法則，GPU, ASIC)
2. データ爆発 (e.g. ImageNet, AMT),
3. アルゴリズムの改善 (e.g. 誤差逆伝播法, CNN, Transformer, LSTM)
4. 基盤の整備 (Linux, TCP/IP, Github, ROS, AWS, PyTorch, TensorFlow)
5. エコシステム 情報共有 (arXiv, Git, Reddit, Quora, Stackoverflow)

from [Karpathy's blog "Deep Reinforcement Learning: Pong from Pixels"](http://karpathy.github.io/2016/05/31/rl/)


### 情勢

データサイエンス，人工知能系のエンジニアの育成が急務です。
資格，就職にとって武器になり得るでしょう。
実際に，検索すれば，文化系だけどデータサイエンティストになった，という宣伝をする YouTuber が多数見つかります。

* [政府、AI 人材年 25 万人育成へ　全大学生に初級教育](https://www.nikkei.com/article/DGXMZO42932250W9A320C1SHA000/){:target="_blank"}
* [AI人材25万人目標達成へ 政府、統合イノベ戦略を閣議決定](https://www.nikkei.com/article/DGXMZO46386930R20C19A6EAF000/){:target="_blank"}
* [東大など　学生が学ぶべき「AIリテラシー」を定義](https://www.nikkei.com/article/DGXMZO56107550X20C20A2000000/){:target="_blank"}
* [引く手あまたのデータサイエンティスト 学生は及び腰?](https://style.nikkei.com/article/DGXMZO47952800Q9A730C1000000){:target="_blank"}
* [高等学校情報科「情報Ⅱ」教員研修用教材(本編)](https://www.mext.go.jp/a_menu/shotou/zyouhou/detail/mext_00742.html){:target="_blank"}
* [文部科学省 高等学校情報科「情報Ⅰ」教員研修用教材（本編）](https://www.mext.go.jp/a_menu/shotou/zyouhou/detail/1416756.htm){:target="_blnak"}
  * [上サイト第 3 章コンピュータとプログラミング](https://www.mext.go.jp/component/a_menu/education/micro_detail/__icsFiles/afieldfile/2019/10/09/1416758_005.pdf){:target="_blank"} Python でサンプルコードが書かれている
* [高等学校情報科「情報Ⅱ」教員研修用教材(本編) 第3章 情報とデータサイエンス 後半](https://www.mext.go.jp/content/20200609-mxt_jogai01-000007843_007.pdf)  160 ページ以降にニューラルネットワークの記述あり。
  * [上サイト第3章前半 情報とデータサイエンス pdf ファイル](https://www.mext.go.jp/content/20200702-mxt_jogai01-000007843_004.pdf){:target="_blank"}
  * [上サイト第3章後半 pdf ファイル](https://www.mext.go.jp/content/20200609-mxt_jogai01-000007843_007.pdf){:target="_blank"}
  * [上サイト第 4 章情報通信ネットワークとデータの活用](https://www.mext.go.jp/content/20200722-mxt_jogai02-100013300_006.pdf){:target="_blank"}<!--テキストマイニングで R with Mecab -->

* [大学入学共通テストへの『情報Ⅰ』の導入について 資料2-1](https://www.mext.go.jp/content/20211021-mxt_daigakuc02-000018569_3.pdf){:target="_blank"}

### AI 人材不足という社会背景

- AI 人材の不足。
経済産業省の[調査報告書](https://www.meti.go.jp/policy/it_policy/jinzai/houkokusyo.pdf) によれば，2030 年には最大で 79 万人の IT 人材不足との試算がある (同報告書 26 ページ)。
- 自分の強みを活かして，よりよい人生を送るための選択肢として，AI エンジニアという，あるいは AI 関連業界に身を置くという選択肢を考えても良いだろう。
− 本授業の目的の一つは，そのような職業選択へ向けての道標を示すことも含まれる。


## 2. 東京技術計算コンサルタント様 ピッチ
* [東京技術計算コンサルタント株式会社](https://www.tokyo-tcc.co.jp/){:target="_blank"}

## 3.1 2024 年ノーベル物理学賞，化学賞

ノーベル物理学賞 [Hopfield and Hinton, Nobel Prize in Physics 2024 受賞記念](https://www.nobelprize.org/prizes/physics/2024/popular-information/){:target="_blank"}<br/>
* URL: [https://www.nobelprize.org/prizes/physics/2024/popular-information/](https://www.nobelprize.org/prizes/physics/2024/popular-information/){:target="_blank"}

今年の受賞者は，物理学のツールを活用して，今日の強力な機械学習の基礎を築くのに役立つ手法を構築した。
ホップフィールドは，情報を保存し再構築できる構造を考案した。
ヒントンは，データから独自に特性を発見できる手法を発明し，現在使用されている大規模な人工ニューラルネットワークにとって重要なものとなった。
<!-- This year’s laureates used tools from physics to construct methods that helped lay the foundation for today’s powerful machine learning.
John Hopfield created a structure that can store and reconstruct information.
Geoffrey Hinton invented a method that can independently discover properties in data and which has become important for the large artificial neural networks now in use. -->

### 物理学を用いて情報のパターンを見つける<!-- They used physics to find patterns in information-->

多くの人が，コンピュータが言語を翻訳したり，画像を解釈したり，さらにはそれなりの会話を行うことができることを経験している。
しかし，おそらくあまり知られていないのは，この種のテクノロジーが膨大なデータの分類や分析など，研究において長い間重要な役割を果たしてきたということだ。
機械学習は過去15～20年で爆発的に発展し，人工ニューラルネットワークと呼ばれる構造を利用している。
今日，人工知能について語る場合，この技術を指すことが多い。
<!-- Many people have experienced how computers can translate between languages, interpret images and even conduct reasonable conversations.
What is perhaps less well known is that this type of technology has long been important for research, including the sorting and analysis of vast amounts of data.
The development of machine learning has exploded over the past fifteen to twenty years and utilises a structure called an artificial neural network.
Nowadays, when we talk about artificial intelligence, this is often the type of technology we mean. -->

コンピュータは思考することができないが，記憶や学習といった機能は模倣することができる。
今年のノーベル物理学賞受賞者たちは，これを可能にした。
物理学の基本的な概念と手法を用いて，ネットワークの構造を利用して情報を処理する技術を開発した。
<!-- Although computers cannot think, machines can now mimic functions such as memory and learning.
This year’s laureates in physics have helped make this possible. 
Using fundamental concepts and methods from physics, they have developed technologies that use structures in networks to process information. -->

機械学習は，レシピのような従来のソフトウェアとは異なる。
ソフトウェアはデータを受け取り，明確な説明に従って処理を行い，結果を生成する。これは，誰かが材料を集め，レシピに従ってそれらを処理し，ケーキを作るのとよく似ている。
これに対し，機械学習ではコンピュータが例から学習し，段階的な指示では管理できないほど曖昧で複雑な問題にも取り組むことができる。その一例が，画像を解釈してその中に含まれる物体を識別することである。
<!--Machine learning differs from traditional software, which works like a type of recipe.
The software receives data, which is processed according to a clear description and produces the results, much like when someone collects ingredients and processes them by following a recipe, producing a cake.
Instead of this, in machine learning the computer learns by example, enabling it to tackle problems that are too vague and complicated to be managed by step by step instructions. One example is interpreting a picture to identify the objects in it. -->

#### 脳の模倣<!-- #### Mimics the brain-->

人工ニューラルネットワークは，ネットワーク構造全体を使用して情報を処理する。
当初の着想は，脳の仕組みを理解したいという欲求から生まれた。
1940 年代には，研究者たちは脳のニューロンとシナプスのネットワークの基礎となる数学について推論を始めていた。
また，神経科学者ドナルド・ヘッブが，ニューロン間の接続が連携して機能するときに強化されるという学習の発生に関する仮説を立てたことで，心理学からも新たな知見がもたらされた。
<!-- An artificial neural network processes information using the entire network structure.
The inspiration initially came from the desire to understand how the brain works.
In the 1940s, researchers had started to reason around the mathematics that underlies the brain’s network of neurons and synapses.
Another piece of the puzzle came from psychology, thanks to neuroscientist Donald Hebb’s hypothesis about how learning occurs because connections between neurons are reinforced when they work together. -->

その後，これらの考え方は，人工ニューラルネットワークをコンピュータシミュレーションとして構築することで，脳のネットワークの機能を再現しようとする試みに引き継がれた。
この場合，脳のニューロンは異なる値を持つノードで模倣され，シナプスはノード間の接続で表され，その接続は強くなったり弱くなったりする。
ドナルド・ヘッブの仮説は，訓練と呼ばれる処理を通じて人工ネットワークを更新する際の基本的な学習則の一つとして，現在でも使用されている。
<!--Later, these ideas were followed by attempts to recreate how the brain’s network functions by building artificial neural networks as computer simulations.
In these, the brain’s neurons are mimicked by nodes that are given different values, and the synapses are represented by connections between the nodes that can be made stronger or weaker.
Donald Hebb’s hypothesis is still used as one of the basic rules for updating artificial networks through a process called training. -->

<div class="figcenter">
<!-- <img src="https://www.nobelprize.org/uploads/2024/10/popular-physicsprize2024-figure2-2-1024x530.jpg" style="width:77%"> -->
<img src="/2025assets/popular-physicsprize2024-figure2-2.jpg" style="width:66%;">
</div>
<div class="figcaption">

図 2．Image URL: https://www.nobelprize.org/uploads/2024/10/popular-physicsprize2024-figure2-2-1024x530.jpg
</div>

1960 年代の終わりには，いくつかの期待外れの理論的結果により，多くの研究者がこれらのニューラルネットワークが実用化されることはないだろうと疑うようになった。
しかし，人工ニューラルネットワークへの関心は1980年代に再び高まり，その年に受賞した研究者たちの研究を含むいくつかの重要なアイデアが注目を集めた。
<!-- At the end of the 1960s, some discouraging theoretical results caused many researchers to suspect that these neural networks would never be of any real use.
However, interest in artificial neural networks was reawakened in the 1980s, when several important ideas made an impact, including work by this year’s laureates. -->

#### 連想記憶<!-- #### Associative memory-->

映画館や講堂でよく見かける傾斜した床を表す言葉など，めったに使わないかなり変わった単語を思い出そうとしているとしよう。
記憶をたどってみる。
Ramp..., rad...idal? いや rake だ!
<!--ランプのようなもの…ラジアル…？いや，それじゃない。
レーキだ！
Imagine that you are trying to remember a fairly unusual word that you rarely use, such as one for that sloping floor often found in cinemas and lecture halls.
You search your memory.
It’s something like ramp… perhaps rad…ial? No, not that.
Rake, that’s it! -->

適切な単語を見つけるために類似した単語を探索するこの過程は，物理学者ジョン・ホップフィールドが 1982 年に発見した連想記憶を彷彿とさせる。
ホップフィールド・ネットワークはパターンを保存でき，それを再現する方法も備えている。
ネットワークに不完全なパターンやわずかに歪んだパターンが与えられると，その方法によって最も類似した保存パターンを見つけることができる。
<!-- This process of searching through similar words to find the right one is reminiscent of the associative memory that the physicist John Hopfield discovered in 1982.
The Hopfield network can store patterns and has a method for recreating them.
When the network is given an incomplete or slightly distorted pattern, the method can find the stored pattern that is most similar. -->

ホップフィールドは以前，物理学のバックグラウンドを活かして分子生物学の理論的な問題を研究していた。
神経科学に関する会議に招かれた際，彼は脳の構造に関する研究に出会った。
彼はそこで学んだことに魅了され，単純な神経ネットワークの力学について考え始めた。
ニューロンが同時に作用すると，ネットワークの個々の構成要素だけを見ている人には見えない，新しい強力な特性が生み出される可能性がある。
<!-- Hopfield had previously used his background in physics to explore theoretical problems in molecular biology.
When he was invited to a meeting about neuroscience he encountered research into the structure of the brain.
He was fascinated by what he learned and started to think about the dynamics of simple neural networks.
When neurons act together, they can give rise to new and powerful characteristics that are not apparent to someone who only looks at the network’s separate components. -->

1980 年，ホップフィールドはプリンストン大学を辞し，大陸を横断した。
南カリフォルニアのパサデナにあるカリフォルニア工科大学（Caltech）の化学・生物学の教授職のオファーを受けたのだ。
同大学では，ニューラルネットワークに関する自身のアイデアを開発するための無料実験に利用できるコンピューターリソースを利用することができた。
<!-- In 1980, Hopfield left his position at Princeton University, where his research interests had taken him outside the areas in which his colleagues in physics worked, and moved across the continent.
He had accepted the offer of a professorship in chemistry and biology at Caltech (California Institute of Technology) in Pasadena, southern California.
There, he had access to computer resources that he could use for free experimentation and to develop his ideas about neural networks.-->

しかし，彼は物理学の基礎を捨てたわけではなく，そこから，多数の小さな部品が連携して機能するシステムが，新しい興味深い現象を生み出す仕組みについての理解を得た。
特に，原子スピンによって特殊な特性を持つ磁性材料について学んだことは有益であった。
隣接する原子のスピンは互いに影響し合うため，同じ方向のスピンを持つドメインを形成することができる。
彼は，スピンが互いに影響し合うことで物質がどのように発展するかを説明する物理学を利用して，ノードと接続からなるモデルネットワークを作成することができた。
<!--However, he did not abandon his foundation in physics, where he found inspiration for his under­standing of how systems with many small components that work together can give rise to new and interesting phenomena.
He particularly benefitted from having learned about magnetic materials that have special characteristics thanks to their atomic spin – a property that makes each atom a tiny magnet.
The spins of neighbouring atoms affect each other; this can allow domains to form with spin in the same direction.
He was able to make a model network with nodes and connections by using the physics that describes how materials develop when spins influence each other. -->

#### ネットワークは画像の景観を保存<!-- #### The network saves images in a landscape-->

ホップフィールドが構築したネットワークは，強度の異なる接続によって結合されたノードで構成されている。
各ノードは個別の値を保存することができ，ホップフィールドの最初の研究では，白黒写真のピクセルのように，0または1のいずれかを保存することができた。
<!--The network that Hopfield built has nodes that are all joined together via connections of different strengths.
Each node can store an individual value – in Hopfield’s first work this could either be 0 or 1, like the pixels in a black and white picture. -->

ホップフィールドは，物理学におけるスピン系のエネルギーに相当する特性を用いてネットワークの全体的な状態を説明した。エネルギーは，ノードのすべての値と，それらの間の接続のすべての強度を使用する数式を用いて計算される。
ホップフィールドネットワークは，ノードに供給される画像によってプログラムされ，ノードには黒（0）または白（1）の値が与えられる。
次に，保存された画像のエネルギーが低くなるように，エネルギー式を用いてネットワークの接続が調整される。
別のパターンがネットワークに送られると，ノードを一つずつ順に調べ，そのノードの値が変更された場合にネットワークのエネルギーが低くなるかどうかを確認するルールがある。
黒いピクセルが白に置き換えられるとエネルギーが低くなることが判明した場合，色が変更される。
この手順は，これ以上改善が見込めなくなるまで続けられる。この段階に達すると，ネットワークは学習に使用された元の画像を再現していることが多い。
<!-- Hopfield described the overall state of the network with a property that is equivalent to the energy in the spin system found in physics; the energy is calculated using a formula that uses all the values of the nodes and all the strengths of the connections between them.
The Hopfield network is programmed by an image being fed to the nodes, which are given the value of black (0) or white (1).
The network’s connections are then adjusted using the energy formula, so that the saved image gets low energy.
When another pattern is fed into the network, there is a rule for going through the nodes one by one and checking whether the network has lower energy if the value of that node is changed.
If it turns out that energy is reduced if a black pixel is white instead, it changes colour.
This procedure continues until it is impossible to find any further improvements. When this point is reached, the network has often reproduced the original image on which it was trained.-->

これは，1 つのパターンだけを保存する場合はそれほど目立たないかもしれない。
おそらく，画像そのものを保存して，それをテスト中の別の画像と比較すればいいのに，と思うかもしれないが，ホップフィールドの方法は特殊で，同時に複数の画像を保存でき，ネットワークは通常，それらの画像を区別することができる。
<!-- This may not appear so remarkable if you only save one pattern.
Perhaps you are wondering why you don’t just save the image itself and compare it to another image being tested, but Hopfield’s method is special because several pictures can be saved at the same time and the network can usually differentiate between them. -->

ホップフィールドは，ネットワークに保存された状態を検索することを，摩擦により動きが遅くなる山と谷の風景の中をボールが転がっていくことに例えた。
ボールが特定の場所に落とされた場合，ボールは最も近い谷に転がり落ちてそこで止まる。
ネットワークに保存されたパターンのいずれかに近いパターンが与えられた場合，同様に，エネルギー地形の谷底に到達するまで前進し続けるため，最も近いパターンを記憶の中から見つけることができる。
<!-- Hopfield likened searching the network for a saved state to rolling a ball through a landscape of peaks and valleys, with friction that slows its movement.
If the ball is dropped in a particular location, it will roll into the nearest valley and stop there.
If the network is given a pattern that is close to one of the saved patterns it will, in the same way, keep moving forward until it ends up at the bottom of a valley in the energy landscape, thus finding the closest pattern in its memory. -->

ホップフィールド・ネットワークは，ノイズを含むデータや一部が消去されたデータの再作成にも使用できる。
<!--The Hopfield network can be used to recreate data that contains noise or which has been partially erased. -->

<div class="figcenter">
<!-- <img src="https://www.nobelprize.org/uploads/2024/09/popular-physicsprize2024-figure3-1024x657.jpg" style="width:55%"> -->
<img src="/2025assets/popular-physicsprize2024-figure3.jpg" style="width:55%;">
</div>
<div class="figcaption">

図 3.
Image URL: https://www.nobelprize.org/uploads/2024/09/popular-physicsprize2024-figure3-1024x657.jpg
</div>

ホップフィールドやその他の研究者たちは，ホップフィールド・ネットワークの機能の詳細を開発し続けている。
その中には，0 や 1 以外のあらゆる値を保存できるノードも含まれる。
ノードを画像の画素と考えると，黒や白だけでなく，さまざまな色を持つことができる。
改良された方法により，より多くの画像を保存し，それらが非常に似通っている場合でも区別することが可能になった。
多くのデータ点から構築されている限り，あらゆる情報を識別または再構築することが可能である。
<!-- Hopfield and others have continued to develop the details of how the Hopfield network functions, including nodes that can store any value, not just zero or one.
If you think about nodes as pixels in a picture, they can have different colours, not just black or white.
Improved methods have made it possible to save more pictures and to differentiate between them even when they are quite similar.
It is just as possible to identify or reconstruct any information at all, provided it is built from many data points. -->

#### 19 世紀の物理学を用いた分類<!-- #### Classification using nineteenth-century physics-->

画像を想起することはできるが，それが何を表しているかを解釈するには，もう少し必要だ。
<!-- Remembering an image is one thing, but interpreting what it depicts requires a little more. -->

幼い子供でも，さまざまな動物を指さして，それが犬なのか，猫なのか，リスなのかを自信を持って言うことができる。
時には間違えることもあるが，すぐにほとんどの場合正しく言えるようになる。子供は，図や種や哺乳類といった概念の説明を見なくても，これを学ぶことができる。
それぞれの種類の動物の例をいくつか見れば，子供たちはすぐに異なるカテゴリーを理解する。
人は，猫を認識したり，言葉を理解したり，部屋に入って何かが変わったことに気づいたりするが，それは周囲の環境を経験することによってである。
<!-- Even very young children can point at different animals and confidently say whether it is a dog, a cat, or a squirrel.
They might get it wrong occasionally, but fairly soon they are correct almost all the time. A child can learn this even without seeing any diagrams or explanations of concepts such as species or mammal.
After encountering a few examples of each type of animal, the different categories fall into place in the child’s head.
People learn to recognise a cat, or understand a word, or enter a room and notice that something has changed, by experiencing the environment around them. -->

ホップフィールドが連想記憶に関する論文を発表した当時，ジェフリー・ヒントンは米国ピッツバーグのカーネギーメロン大学で働いていた。 彼はそれ以前に，英国とスコットランドで実験心理学と人工知能を研究しており，機械が人間と同様にパターン処理を学習し，情報を分類し解釈するための独自のカテゴリーを見つけ出すことができるかどうかを考えていた。
ヒントンは同僚のテレンス・セジュノフスキーとともに，ホップフィールドのネットワークを出発点として，統計物理学のアイデアを活用しながら，新しいものを構築するためにそれを拡張した。
<!--When Hopfield published his article on associative memory, Geoffrey Hinton was working at Carnegie Mellon University in Pittsburgh, USA. He had previously studied experimental psychology and artificial intelligence in England and Scotland and was wondering whether machines could learn to process patterns in a similar way to humans, finding their own categories for sorting and interpreting information.
Along with his colleague, Terrence Sejnowski, Hinton started from the Hopfield network and expanded it to build something new, using ideas from statistical physics. -->

統計力学は，気体中の分子のように，多くの類似した要素で構成される系を説明する。
気体中の個々の分子をすべて追跡することは困難，あるいは不可能であるが，それらをまとめて考え，圧力や温度といった気体の全体的な性質を決定することは可能である。
気体分子は，それぞれの速度で気体の体積全体に広がる可能性があり，それでも同じ全体的な性質が得られる。
<!-- Statistical physics describes systems that are composed of many similar elements, such as molecules in a gas.
It is difficult, or impossible, to track all the separate molecules in the gas, but it is possible to consider them collectively to determine the gas’ overarching properties like pressure or temperature.
There are many potential ways for gas molecules to spread through its volume at individual speeds and still result in the same collective properties.-->

個々の構成要素が共同して存在できる状態は，統計物理学を用いて分析することができ，その発生確率を計算することができる。
状態によっては，他の状態よりも発生確率が高いものもある。これは利用可能なエネルギーの量に依存するが，この量は 19 世紀の物理学者ルートヴィヒ・ボルツマンの式で表される。
ヒントンのネットワークは，この式を利用しており，その方法は1985年に「ボルツマン・マシン」という印象的な名称で発表された。
<!--The states in which the individual components can jointly exist can be analysed using statistical physics, and the probability of them occurring calculated.
Some states are more probable than others; this depends on the amount of available energy, which is described in an equation by the nineteenth-century physicist Ludwig Boltzmann.
Hinton’s network utilised that equation, and the method was published in 1985 under the striking name of the Boltzmann machine. -->

### 同じタイプの新しい例を認識する<!--#### Recognising new examples of the same type-->

ボルツマン・マシンは通常，2 種類の異なるノードとともに使用される。
情報は，可視ノードと呼ばれる1つのグループに供給される。他のノードは隠れ層を形成する。
隠れノードの値と接続も，ネットワーク全体のエネルギーに寄与する。
<!-- The Boltzmann machine is commonly used with two different types of nodes.
Information is fed to one group, which are called visible nodes. The other nodes form a hidden layer.
The hidden nodes’ values and connections also contribute to the energy of the network as a whole. -->

このマシンは，ノードの値を1つずつ更新するルールを適用することで実行される。
最終的に，ノードのパターンが変化する状態になるが，ネットワーク全体の特性は変わらない。
各パターンには，ボルツマン方程式に従ってネットワークのエネルギーによって決定される特定の確率が存在する。
マシンが停止すると，新しいパターンが作成される。これがボルツマンマシンを生成モデルの初期の例としている。
<!--The machine is run by applying a rule for updating the values of the nodes one at a time.
Eventually the machine will enter a state in which the nodes’ pattern can change, but the properties of the network as a whole remain the same.
Each possible pattern will then have a specific probability that is determined by the network’s energy according to Boltzmann’s equation.
When the machine stops it has created a new pattern, which makes the Boltzmann machine an early example of a generative model.-->

<div class="figcenter">
<!-- <img src="https://www.nobelprize.org/uploads/2024/09/popular-physicsprize2024-figure4-1024x594.jpg" style="width:55%"> -->
<img src="/2025assets/popular-physicsprize2024-figure4.jpg" style="width:66%">
</div>
<div class="figcaption">

図 4．
image URL: https://www.nobelprize.org/uploads/2024/09/popular-physicsprize2024-figure4-1024x594.jpg
</div>

ボルツマンマシンは学習することができる。
ただし，指示からではなく，例を示されることで学習する。
ネットワークの接続部分の値を更新することで訓練を行い，訓練時に可視ノードに与えられた例となるパターンが，マシンが実行された際に発生する可能性が最も高くなるようにする。
この訓練中に同じパターンが複数回繰り返された場合，そのパターンの発生確率はさらに高くなる。
また，訓練は，マシンが訓練された例に類似した新しいパターンを出力する確率にも影響を与える。
<!-- The Boltzmann machine can learn – not from instructions, but from being given examples.
It is trained by updating the values in the network’s connections so that the example patterns, which were fed to the visible nodes when it was trained, have the highest possible probability of occurring when the machine is run. If the same pattern is repeated several times during this training, the probability for this pattern is even higher.
Training also affects the probability of outputting new patterns that resemble the examples on which the machine was trained.-->

訓練されたボルツマンマシンは，以前に見たことのない情報でも，見覚えのある特徴を認識することができる。
友人の兄弟に会ったと想像してみよう。
それらが親戚であることはすぐに分かる。
同様に，ボルツマンマシンは，訓練事例に含まれるカテゴリーに属するものであれば，まったく新しい例を認識し，類似しない資料と区別することができる。
<!-- A trained Boltzmann machine can recognise familiar traits in information it has not previously seen.
Imagine meeting a friend’s sibling, and you can immediately see that they must be related.
In a similar way, the Boltzmann machine can recognise an entirely new example if it belongs to a category found in the training material, and differentiate it from material that is dissimilar. -->

ボルツマンマシンは，その原型ではかなり非効率的で，解を見つけるのに長い時間がかかった。
しかし，ヒントンが研究を続けたように，さまざまな方法で開発が進められると，状況はより興味深いものになっていく。
後続のバージョンでは，いくつかの素子間の接続が削除され，間引きが行われた。
これにより，マシンがより効率的になる可能性があることが判明した。
<!-- In its original form, the Boltzmann machine is fairly inefficient and takes a long time to find solutions.
Things become more interesting when it is developed in various ways, which Hinton has continued to explore.
Later versions have been thinned out, as the connections between some of the units have been removed. It turns out that this may make the machine more efficient. -->

1990 年代には，多くの研究者が人工ニューラルネットワークへの関心を失ったが，Hinton は研究を続けた。
また，彼は2006年に同僚の Simon Osindero, Yee Whye Teh, Ruslan Salakhutdinov とともに，層状に重ねられた一連のボルツマンマシンでネットワークを事前学習する方法を開発し，新たな成果の爆発的な増加に貢献した。
この事前学習により，ネットワーク内の接続に最適な出発点が与えられ，画像内の要素を認識するための学習が最適化された。
<!-- During the 1990s, many researchers lost interest in artificial neural networks, but Hinton was one of those who continued to work in the field.
He also helped start the new explosion of exciting results; in 2006 he and his colleagues Simon Osindero, Yee Whye Teh and Ruslan Salakhutdinov developed a method for pretraining a network with a series of Boltzmann machines in layers, one on top of the other.
This pretraining gave the connections in the network a better starting point, which optimised its training to recognise elements in pictures. -->

ボルツマン・マシンは，より大きなネットワークの一部として使用されることが多い。例えば，視聴者の好みに応じた映画やテレビ番組の推奨に使用できる。
<!-- The Boltzmann machine is often used as part of a larger network. For example, it can be used to recommend films or television series based on the viewer’s preferences. -->

#### 機械学習 - 現在と未来<!-- #### Machine learning – today and tomorrow-->

ジョン・ホップフィールドとジェフリー・ヒントンは，1980 年代以降の研究により，2010 年頃に始まった機械学習革命の基礎を築いた。
<!-- Thanks to their work from the 1980s and onward, John Hopfield and Geoffrey Hinton have helped lay the foundation for the machine learning revolution that started around 2010. -->

現在我々が目撃している発展は，ネットワークの訓練に使用できる膨大な量のデータへのアクセスと，コンピューティング能力の飛躍的な向上により可能となった。
今日の人工ニューラルネットワークは，多くの場合，巨大で多くの層から構成されている。これを深層ニューラルネットワークと呼び，その学習方法を深層学習と呼ぶ。
<!-- The development we are now witnessing has been made possible through access to the vast amounts of data that can be used to train networks, and through the enormous increase in computing power.
Today’s artificial neural networks are often enormous and constructed from many layers. These are called deep neural networks and the way they are trained is called deep learning. -->

1982 年の Hopfield の連想記憶に関する論文をざっと見ると，この発展についてある程度の見通しが得られる。
その論文では，30 個のノードを持つネットワークが使用されていた。
すべてのノードが互いに接続されている場合，435 の接続がある。
ノードには値があり，接続には異なる強度があり，合計すると，追跡すべきパラメータは 500 未満である。
また，彼は 100 ノードのネットワークも試したが，当時使用していたコンピュータでは複雑すぎた。
これを今日の巨大な言語モデルと比較すると，これは 1 兆以上のパラメータ（1 億億の1 億倍）を含むネットワークとして構築されている。
<!-- A quick glance at Hopfield’s article on associative memory, from 1982, provides some perspective on this development.
In it, he used a network with 30 nodes.
If all the nodes are connected to each other, there are 435 connections.
The nodes have their values, the connections have different strengths and, in total, there are fewer than 500 parameters to keep track of.
He also tried a network with 100 nodes, but this was too complicated, given the computer he was using at the time.
We can compare this to the large language models of today, which are built as networks that can contain more than one trillion parameters (one million millions). -->

現在，多くの研究者が機械学習の応用分野の開発に取り組んでいる。
どの分野が最も有望かはまだわからないが，この技術の開発と利用をめぐる倫理的な問題についても幅広い議論が行われている。
<!--Many researchers are now developing machine learning’s areas of application.
Which will be the most viable remains to be seen, while there is also wide-ranging discussion on the ethical issues that surround the development and use of this technology. -->

物理学は機械学習の発展に貢献するツールを提供してきたため，研究分野としての物理学が人工ニューラルネットワークからどのような恩恵を受けているのかを見るのは興味深い。
機械学習は，これまでのノーベル物理学賞受賞者たちの研究分野と関連の深い分野で，以前から利用されてきた。
例えば，ヒッグス粒子の発見に必要な膨大なデータをふるいにかけて処理するために機械学習が利用された。
その他の応用例としては，ブラックホールの衝突による重力波の測定におけるノイズの低減や，太陽系外惑星の探索などがある。
<!-- Because physics has contributed tools for the development of machine learning, it is interesting to see how physics, as a research field, is also benefitting from artificial neural networks.
Machine learning has long been used in areas we may be familiar with from previous Nobel Prizes in Physics.
These include the use of machine learning to sift through and process the vast amounts of data necessary to discover the Higgs particle.
Other applications include reducing noise in measurements of the gravitational waves from colliding black holes, or the search for exoplanets.-->

近年では，分子や物質の特性を計算したり予測したりする際にも，この技術が利用され始めている。例えば，タンパク質分子の構造を計算してその機能を決定したり，より効率的な太陽電池に使用するのに最適な特性を持つ物質の新しいバージョンを特定したりするなどである。
<!--In recent years, this technology has also begun to be used when calculating and predicting the properties of molecules and materials – such as calculating protein molecules’ structure, which determines their function, or working out which new versions of a material may have the best properties for use in more efficient solar cells. -->

#### さらに詳しく<!-- #### Further reading-->

今年の受賞に関する追加情報（英語による科学的背景を含む）は，スウェーデン王立科学アカデミーのウェブサイト（www.kva.se）および www.nobelprize.org で入手できる。
www.nobelprize.org では，記者会見のビデオやノーベル賞受賞講演などを視聴できる。
ノーベル賞および経済学賞に関連する展示会や活動に関する情報は，www.nobelprizemuseum.se で入手できる。
<!--Additional information on this year’s prizes, including a scientific background in English, is available on the website of the Royal Swedish Academy of Sciences, www.kva.se, and at www.nobelprize.org, where you can watch video from the press conferences, the Nobel Lectures and more.
Information on exhibitions and activities related to the Nobel Prizes and the Prize in Economic Sciences is available at www.nobelprizemuseum.se. -->

* [Hopfield1982, Neural networks and physical systems with emergent collective computationalabilitie](https://komazawa-deep-learning.github.io/2024cogpsy/1982Hopfield_Neural_networks_and_physical_systems_with_emergent_collective_computational_abilities.pdf){:target="_blank"}
* [Hopfiled&Tank1985, "Neural" Computation of Decisions in Optimization Problems](https://komazawa-deep-learning.github.io/2024cogpsy/1985Hopfield_Tank_Neural_computation_of_decisions_in_optimization_problems.pdf){:target="_blank"}
* [Rumelhart, Hinton, McClelland 1986, A General Framework for Parallel Distributed Processing, PDP book, chapt. 2](https://komazawa-deep-learning.github.io/2024cogpsy/1986Rumelhart_PDPbook_chapter2.pdf){:target="_blank"}
* [Rumelhart, Hinton, & Williams, 1986, Learning Internal Representations by Error Propagation, PDP book, chapt. 8](https://komazawa-deep-learning.github.io/2024cogpsy/1986Rumelhart_Hinton_Williams_PDP_Chapt8.pdf){:target="_blank"}
* [ジェフェリー・ヒントンのマクセル賞受賞記念講演(2016)](/Hinton_Maxwell_award/){:target="_blank"}
* [浅川, 2013, アトラクタニューラルネットワークモデルの数理解析とその神経心理学への応用](/2024cogpsy/2013Asakawa_JPsychoReview_Published.pdf){:target="_blank"}
* [浅川 2000, ニューラルネットワークの基礎](/2024cogpsy/main-web2010.pdf){:target="_blank"}

<!-- * [ジェフェリー・ヒントンのマクセル賞受賞記念講演(2016)](https://komazawa-deep-learning.github.io/Hinton_Maxwell_award/){:target="_blank"}
* [浅川, 2013, アトラクタニューラルネットワークモデルの数理解析とその神経心理学への応用](https://komazawa-deep-learning.github.io/2024cogpsy/2013Asakawa_JPsychoReview_Published.pdf){:target="_blank"}
* [浅川 2000, ニューラルネットワークの基礎](https://komazawa-deep-learning.github.io/2024cogpsy/main-web2010.pdf){:target="_blank"} -->

<!-- おまけ [学習マッピングー動物の行動から人間の社会文化まで](https://www.amazon.co.jp/dp/4788518600/) -->


## 3.2 化学賞 計算タンパク質設計とタンパク質構造予測<!-- # Computational Protein Design and Protein Structure Prediction -->

The Royal Swedish Academy of Sciences has decided to award David Baker, Demis Hassabis and John Jumper the Nobel Prize in Chemistry 2024, for computational protein design and protein structure prediction.

##### はじめに<!-- ## Introduction-->

約65年前、X線結晶構造解析法によって初めてタンパク質の三次元（3D）構造が決定された。(1,2) 
それ以来、科学者たちはポリペプチド鎖がどのようにして明確な複雑な3Dパターンに折りたたまれるのかということに魅了されてきた。
また、タンパク質の機能は、まさにこれらの特定の構造によってもたらされる。
したがって、タンパク質の 3D 構造を予測する能力があれば、その機能や生化学的特性も予測できることが明らかになった。
1972年 Christian Anfinsen は、タンパク質の3D構造は基本的にポリペプチド鎖のアミノ酸配列によって符号化されているという画期的な発見により、ノーベル化学賞を受賞した。
つまり、タンパク質が可逆的に変性した場合、常に同じ 3D 構造に再折り畳まれることを発見したのである。(3) 
この発見により、一次アミノ酸配列から直接 3D 構造を予測するという、長年の科学的探求が導かれた。
この問題は非常に重要であると考えられており、ノーベル賞受賞者である Venki Ramakrishnan は「生物学における 50 年来の壮大な挑戦」と表現している。(4)
<!--The first three-dimensional (3D) structures of proteins were determined by X-ray crystallography about 65 years ago.(1,2) 
Ever since, scientists have been fascinated by how the polypeptide chains fold themselves up into well-defined and complex 3D patterns. 
It is also precisely these specific structures that confer proteins their function. 
It thus became clear that the ability to predict the 3D structure of a protein would enable prediction also of its function and biochemical properties.
In 1972, Christian Anfinsen was awarded the Nobel Prize in Chemistry for the remarkable finding that protein 3D structures were basically encoded by the sequence of amino acids in the polypeptide chain. 
That is, he found that if a protein is reversibly denatured, it would always refold into the same 3D conformation.(3) 
This finding led to the long scientific quest of predicting 3D structures directly from the primary amino acid sequence. 
The problem is considered so important that Nobel Prize Laureate Venki Ramakrishnan has described it as a “50-year-old grand challenge in biology”.(4) -->

実験的手法によるタンパク質の構造決定には多大な労力を要する。注目すべきは、公共データベースに登録されているDNA配列の数が現在30億に迫り、これまでに生物で特定されたタンパク質の配列の数が2億を超えているにもかかわらず、タンパク質データバンク(5)には、対応するタンパク質の 3D 構造のうち、わずかその一部 (約 20 万) しか登録されていないことだ。
そのため、アミノ酸配列から直接タンパク質の構造を予測できるようになることは、大きな成果となるだろう。
<!-- Determining protein structures by experimental means is labour intensive. It is noteworthy that while the number of DNA sequences in public databases is now close to 3 billion, and the number of protein sequences that have so far been identified in organisms is over 200 million, the Protein Data Bank5 still contains only a small fraction of the corresponding 3D protein structures (~200 000). 
To be able to predict protein structures directly from the amino acid sequence would thus be a major achievement. -->

ここで問題となるのは、理論的に可能なタンパク質のコンフォメーションの数が実に天文学的な数になるということだ。
Cyrus Levinthal は、この数を推定し、いわゆる「Levinthal のパラドックス」と名付けた。(6)
これは、100 アミノ酸残基タンパク質の可能なコンフォメーションの数として、10 の 47 乗のオーダーになるという形でよく言及される。
したがって、必然的な結論は、タンパク質はこれらのすべてのコンフォメーションをランダムに探索して折りたたまれるのではなく、偏りのある折りたたみ経路によって折りたたまれるということである。(6,7)
<!-- The problem here is that the number of theoretically possible conformations of a protein is truly astronomical. 
Cyrus Levinthal estimated this number and gave name to what is called “Levinthal’s paradox”.6 It is often stated in terms of the number of possible conformations for a 100-amino-acid residue protein, which would be on the order of 1047. 
Hence, the inevitable conclusion is that proteins do not fold by means of a random search of all these conformations, but by biased folding pathways.(6,7) -->

実際のタンパク質の折りたたみ過程の研究は、それ自体が非常に大きな科学分野であり、実験と理論計算の両面から長年にわたってかなりの進歩を遂げてきた。
しかし、アミノ酸配列からタンパク質の構造を予測することは、折りたたみ過程の最終的な安定構造が究極の目標であるという点で、異なる問題である。
<!-- Study of the actual protein folding process is a very large scientific subject area in itself and has made considerable progress over the years, both by experimental work and theoretical calculations. 
However, predicting protein structures from sequence is a different problem where the final stable structure of the folding process is the ultimate goal.-->

構造予測の問題は、別の方法でも定式化できる。アミノ酸配列から特定の折りたたみパターンが得られるかを問う方法である。
この問いは、タンパク質設計の核心をなすもので、この分野では、目標とする構造を想定し、その構造を生み出す配列を計算手段によって特定する。
<!-- The structure prediction problem can also be formulated in another way, where one instead asks what amino acid sequences would yield a certain folding pattern. 
This question is at the heart of protein design, a field where a target structure is envisaged and then sequences that would yield this structure are identified by computational means. -->

今年のノーベル化学賞は、この 2 つの問題、すなわち配列からの構造予測と構造からの配列予測の両方を解決する決定的な進歩を称えるものであり、その影響は実に大きい。
ほとんどの単量体タンパク質の構造は、現在では高い精度で予測することができ、何億もの構造からなる大規模なデータベースが作成された。
これにより、生化学および生物学の研究に多大な影響がもたらされている。
同様に、自然界には存在しないまったく新しいタンパク質の構造も、コンピュータ設計によって作成することが可能となり、さまざまなバイオテクノロジーや生物医学の応用に利用されている。
<!--This year’s Nobel Prize in Chemistry recognizes decisive breakthroughs in solving both of these problems – structure prediction from sequence and sequence prediction from structure – and the implications are truly profound. 
Most monomeric protein structures can now be predicted with high fidelity, and large databases of hundreds of millions of structures have thus been created, with huge impact on biochemical and biological research. 
Likewise, completely new protein structures, not found in nature, can now be created by computational design and used in various biotechnological and biomedical applications. -->

#### 背景<!-- ### Background-->

最初のタンパク質の構造が決定された後、タンパク質の三次元（3D）構造には、α-ヘリックスやβ-シートなどの繰り返し現れる規則正しい、いわゆる二次構造要素が含まれていることがすぐに認識された。これらの方向と積み重ね、および接続ループ領域が、実際の三次トポロジーを定義する（図1）。
実際、α-ヘリカルポリペプチドパターンは、1951年に Linus Pauling によって予測されていた。(8) 
そのため、アミノ酸配列からタンパク質の構造を予測しようとする初期の試みでは、三次構造よりも二次構造の予測に重点が置かれていた。
<!--After determination of the first protein structures, it was immediately recognized that protein tertiary (3D) structures contain recurring regular so-called secondary structure elements, such as α-helices and β-sheets, the orientation and packing of which, together with connecting loop regions, define the actual tertiary topology (Figure 1). 
In fact, the α-helical polypeptide pattern was predicted by Linus Pauling as early as 1951.(8) 
The earliest attempts to predict protein structure from amino acid sequence therefore focused on secondary structure prediction, rather than tertiary. -->

<img src="/2025assets/fig1_ke_en_24-5.svg" style="width:77%;">
<div class="figcaption">

**図 1. タンパク質の構造の階層。**<br/>
**一次構造**：DNA の塩基トリプレットの対応する配列によって決定されるアミノ酸配列。
**二次構造**：α-ヘリックスと β-シートの規則正しい幾何学パターンの形成。
**三次構造**：ポリペプチド鎖の詳細な三次元形状。
**四次構造**：複数のポリペプチド鎖またはサブユニットの結合。
<!-- Figure 1. Hierarchy of protein structure. 
Primary: the amino acid sequence that is determined by the corresponding sequence of DNA base triplets. 
Secondary: formation of regular geometric patterns of α-helices and β-sheets. 
Tertiary: the detailed 3D shape of the polypeptide chain. 
Quaternary: the association of several polypeptide chains or subunits. -->
</div>

したがって、1974 年 にChou と Fasman は、立体構造が既知の15種類のタンパク質のデータセットを使用して、タンパク質の α-へリックス または β-シート 領域に存在する 20 種類すべての天然アミノ酸の傾向を決定した。
異なる鎖断片の平均 α 確率と β 確率を計算することで、彼らはポリペプチド鎖の二次構造を予測することができた。(9)
<!-- Hence, in 1974, Chou and Fasman used a dataset of 15 proteins with known conformation to determine propensities for all 20 natural amino acids to be found in either α-helical or β-sheet regions of proteins. 
By calculating the average α- and β-probabilities of different chain fragments, they could thus predict the secondary structure of a polypeptide chain.(9) -->

しかし、このアプローチに基づく予測は、あまり正確ではないことが判明した。
これは主に、アミノ酸の一次構造（一次元配列）だけでなく、三次構造を確立するためには三次の立体相互作用も重要であるという事実によるものである。
当時、Protein Data Bank に実験的に決定された構造の総数は非常に少なく（数百）、1990 年代までこの水準にとどまっていた。
この時期は、タンパク質結晶学におけるいくつかの進歩により、データベースが本格的に成長し始めた時期であった (図 2)。(10)
<!-- The predictions based on this approach, however, turned out not to be very accurate. 
This is mainly due to the fact that 3D tertiary interactions also are important for establishing the secondary structure, and not just the one-dimensional (1D) sequence of amino acids (primary structure). 
At that time, the total number of experimentally determined structures in the Protein Data Bank was also very modest (a few hundred), and it remained at this level until the 1990s, when the database really started to grow due to several advances in protein crystallography (Figure 2).(10) -->

<img src="/2025assets/2024advanced-chemistryprize_fig2.jpeg" style="width:49%;">
<div class="figcaption">

**図 2. タンパク質データバンクに登録された実験的に決定されたタンパク質構造の数の経時変化**
<!-- Figure 2. Time evolution of the number of experimentally determined protein structures deposited in the Protein Data Bank. -->
</div>

#### [AlphaFold2 の動作](/https://www.nobelprize.org/uploads/2024/11/fig2_ke_en_24-5.pdf)

AlphaFold2 の開発の一環として、この AI モデルは既知のアミノ酸配列とタンパク質の構造をすべて学習している。
<!-- As part of AlphaFold2’s development, the AI model has been trained on all the known amino acid sequences and determined protein structures.-->

##### 1. データ入力とデータベース検索<!-- 1. DATA ENTRY AND DATABASE SEARCHES -->

構造が不明なアミノ酸配列が AlphaFold2 に入力されると、同様のアミノ酸配列とタンパク質の構造をデータベースから検索する。
<!-- An amino acid sequence with unknown structure is fed into AlphaFold2, which searches databases for similar amino acid sequences and protein structures. -->

<img src="/2025assets/fig2_ke_en_24-5_part1.png" style="width:33%;">

##### 2. 配列分析<!--2. SEQUENCE ANALYSIS-->

AI モデルは、異なる種から得られた類似のアミノ酸配列をすべて整列させ、進化の過程で保存された部分を調査する。
<!-- The AI model aligns all the similar amino acid sequences – often from different species – and investigates which parts have been preserved during evolution. -->

次のステップでは、AlphaFold2 は、3 次元のタンパク質構造において、どのアミノ酸が互いに相互作用し得るかを探索する。
相互作用するアミノ酸は共進化する。
一方が帯電していれば、もう一方は反対の電荷を持つため、互いに引きつけ合う。
一方が撥水性（疎水性）のアミノ酸に置き換われば、もう一方も疎水性になる。
<!--In the next step, AlphaFold2 explores which amino acids could interact with each other in the three dimensional protein structure. 
Interacting amino acids co-evolve. 
If one is charged, the other has the opposite charge, so they are attracted to each other. 
If one is replaced by a water repellent (hydrophobic) amino acid, the other also becomes hydrophobic. -->

この分析を使用して、AlphaFold2 は構造におけるアミノ酸同士の距離を推定する距離地図を生成する。
<!-- Using this analysis, AlphaFold2 produces a distance map that estimates how close amino acids are to each other in the structure. -->
<img src="/2025assets/fig2_ke_en_24-5_part2.png" style="width:22%;">
<img src="/2025assets/fig2_ke_en_24-5_part3.png" style="width:17%;">


##### 3. AI による分析<!-- 3. AI ANALYSIS-->

反復過程を使用して、AlphaFold2 は配列分析と距離地図を改良する。
AI モデルは、注目すべき重要な要素を特定する能力に優れた Transformer と呼ばれるニューラルネットワークを使用する。ステップ 1 で発見された場合、他のタンパク質の構造に関するデータも利用される。
<!--Using an iterative process, AlphaFold2 refines the sequence analysis and distance map. 
The AI model uses neural networks called transformers, which have a great capacity to identify important elements to focus on. Data about other protein structures – if they were found in step 1 – is also utilised. -->

<img src="/2025assets/fig2_ke_en_24-5_part4.png" style="width:33%;">


##### 4. 仮説構造<!-- 4. HYPOTHETICAL STRUCTURE-->

AlphaFold2 は、すべてのアミノ酸のパズルを組み立て、仮説のタンパク質構造を生成するための経路をテストする。
これをステップ 3 で再実行する。
3サイクル後、AlphaFold2 は特定の構造に到達する。
AI モデルは、この構造の異なる部分が現実に対応する確率を計算する。
<!--AlphaFold2 puts together a puzzle of all the amino acids and tests pathways to produce a hypothetical protein structure. 
This is re-run through step 3. 
After three cycles, AlphaFold2 arrives at a particular structure. 
The AI model calculates the probability that different parts of this structure correspond to reality. -->

